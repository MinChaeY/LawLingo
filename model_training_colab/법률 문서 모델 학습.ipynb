{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9277,"status":"ok","timestamp":1732543543911,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"RXscez6AZ-fo","outputId":"b94f2a52-7c5b-4e9f-99d1-120224ecb71a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1732445123887,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"KXSsIlczh6fB","outputId":"ba92b3e5-affe-43ae-9317-be5d7a8890cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sun Nov 24 10:45:25 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   30C    P0              42W / 400W |      2MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1732543794854,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"BQNw2vzoaad4","outputId":"c8d71e2c-3243-494a-872b-2311296c1cf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/training/30-finlaw-src\n"]}],"source":["cd /content/drive/MyDrive/training/30-finlaw-src"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4798,"status":"ok","timestamp":1732543554373,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"NJIxKv7luixH","outputId":"84e2f520-7ca0-4958-8bb4-88d984ab9670"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-docx\n","  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n","Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-docx\n","Successfully installed python-docx-1.1.2\n"]}],"source":["!pip install python-docx"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":240483,"status":"ok","timestamp":1732543794854,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"blmEXLOjegQL","outputId":"4c77e599-ba07-49b8-a995-a8f866525208"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting attrdict==2.0.1 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 1))\n","  Downloading attrdict-2.0.1-py2.py3-none-any.whl.metadata (6.7 kB)\n","Collecting charset-normalizer==2.1.0 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 2))\n","  Downloading charset_normalizer-2.1.0-py3-none-any.whl.metadata (11 kB)\n","Collecting cycler==0.11.0 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 3))\n","  Downloading cycler-0.11.0-py3-none-any.whl.metadata (785 bytes)\n","Collecting et-xmlfile==1.1.0 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 4))\n","  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: fastprogress==1.0.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 5)) (1.0.3)\n","Collecting filelock==3.7.1 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 6))\n","  Downloading filelock-3.7.1-py3-none-any.whl.metadata (2.5 kB)\n","Collecting fonttools==4.34.4 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 7))\n","  Downloading fonttools-4.34.4-py3-none-any.whl.metadata (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna==3.3 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 8))\n","  Downloading idna-3.3-py3-none-any.whl.metadata (9.8 kB)\n","Collecting kiwisolver==1.4.4 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 9))\n","  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n","Collecting matplotlib==3.5.2 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 10))\n","  Downloading matplotlib-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting numpy==1.23.1 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 11))\n","  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n","Collecting packaging==21.3 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 12))\n","  Downloading packaging-21.3-py3-none-any.whl.metadata (15 kB)\n","Collecting pyparsing==3.0.9 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 13))\n","  Downloading pyparsing-3.0.9-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 14)) (2.8.2)\n","Collecting pytz==2022.2 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 15))\n","  Downloading pytz-2022.2-py2.py3-none-any.whl.metadata (21 kB)\n","Collecting PyYAML==6.0 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 16))\n","  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.0 kB)\n","Collecting regex==2022.7.25 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 17))\n","  Downloading regex-2022.7.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests==2.28.1 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 18))\n","  Downloading requests-2.28.1-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 19)) (1.16.0)\n","Collecting tokenizers==0.12.1 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 20))\n","  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21)) (2.5.1+cu121)\n","Collecting tqdm==4.64.0 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 22))\n","  Downloading tqdm-4.64.0-py2.py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers==4.20.1 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 23))\n","  Downloading transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.3/77.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing_extensions==4.3.0 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 24))\n","  Downloading typing_extensions-4.3.0-py3-none-any.whl.metadata (6.3 kB)\n","Collecting urllib3==1.26.11 (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 25))\n","  Downloading urllib3-1.26.11-py2.py3-none-any.whl.metadata (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 10)) (11.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.1->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 18)) (2024.8.30)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 23)) (0.26.2)\n","INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n","Collecting torch (from -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n","  Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n","  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","INFO: pip is still looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n","  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n","  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21)) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21)) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.18.1 (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.1.0 (from torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21))\n","  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21)) (12.6.77)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21)) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt (line 21)) (1.3.0)\n","Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n","Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n","Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n","Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n","Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n","Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m944.1/944.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-3.3-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading packaging-21.3-py3-none-any.whl (40 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytz-2022.2-py2.py3-none-any.whl (504 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.7/504.7 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading regex-2022.7.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (765 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m765.7/765.7 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.28.1-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n","Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m855.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, pytz, urllib3, typing_extensions, tqdm, regex, PyYAML, pyparsing, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, kiwisolver, idna, fonttools, filelock, et-xmlfile, cycler, charset-normalizer, attrdict, triton, requests, packaging, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, matplotlib, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.20.3\n","    Uninstalling tokenizers-0.20.3:\n","      Successfully uninstalled tokenizers-0.20.3\n","  Attempting uninstall: pytz\n","    Found existing installation: pytz 2024.2\n","    Uninstalling pytz-2024.2:\n","      Successfully uninstalled pytz-2024.2\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.2.3\n","    Uninstalling urllib3-2.2.3:\n","      Successfully uninstalled urllib3-2.2.3\n","  Attempting uninstall: typing_extensions\n","    Found existing installation: typing_extensions 4.12.2\n","    Uninstalling typing_extensions-4.12.2:\n","      Successfully uninstalled typing_extensions-4.12.2\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.66.6\n","    Uninstalling tqdm-4.66.6:\n","      Successfully uninstalled tqdm-4.66.6\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2024.9.11\n","    Uninstalling regex-2024.9.11:\n","      Successfully uninstalled regex-2024.9.11\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 6.0.2\n","    Uninstalling PyYAML-6.0.2:\n","      Successfully uninstalled PyYAML-6.0.2\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.2.0\n","    Uninstalling pyparsing-3.2.0:\n","      Successfully uninstalled pyparsing-3.2.0\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n","    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","  Attempting uninstall: kiwisolver\n","    Found existing installation: kiwisolver 1.4.7\n","    Uninstalling kiwisolver-1.4.7:\n","      Successfully uninstalled kiwisolver-1.4.7\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.10\n","    Uninstalling idna-3.10:\n","      Successfully uninstalled idna-3.10\n","  Attempting uninstall: fonttools\n","    Found existing installation: fonttools 4.55.0\n","    Uninstalling fonttools-4.55.0:\n","      Successfully uninstalled fonttools-4.55.0\n","  Attempting uninstall: filelock\n","    Found existing installation: filelock 3.16.1\n","    Uninstalling filelock-3.16.1:\n","      Successfully uninstalled filelock-3.16.1\n","  Attempting uninstall: et-xmlfile\n","    Found existing installation: et_xmlfile 2.0.0\n","    Uninstalling et_xmlfile-2.0.0:\n","      Successfully uninstalled et_xmlfile-2.0.0\n","  Attempting uninstall: cycler\n","    Found existing installation: cycler 0.12.1\n","    Uninstalling cycler-0.12.1:\n","      Successfully uninstalled cycler-0.12.1\n","  Attempting uninstall: charset-normalizer\n","    Found existing installation: charset-normalizer 3.4.0\n","    Uninstalling charset-normalizer-3.4.0:\n","      Successfully uninstalled charset-normalizer-3.4.0\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.32.3\n","    Uninstalling requests-2.32.3:\n","      Successfully uninstalled requests-2.32.3\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 24.2\n","    Uninstalling packaging-24.2:\n","      Successfully uninstalled packaging-24.2\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n","    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.5.1+cu121\n","    Uninstalling torch-2.5.1+cu121:\n","      Successfully uninstalled torch-2.5.1+cu121\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.8.0\n","    Uninstalling matplotlib-3.8.0:\n","      Successfully uninstalled matplotlib-3.8.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.46.2\n","    Uninstalling transformers-4.46.2:\n","      Successfully uninstalled transformers-4.46.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sqlalchemy 2.0.36 requires typing-extensions>=4.6.0, but you have typing-extensions 4.3.0 which is incompatible.\n","albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.1 which is incompatible.\n","albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.1 which is incompatible.\n","bigframes 1.27.0 requires matplotlib>=3.7.1, but you have matplotlib 3.5.2 which is incompatible.\n","bigframes 1.27.0 requires numpy>=1.24.0, but you have numpy 1.23.1 which is incompatible.\n","chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.23.1 which is incompatible.\n","dopamine-rl 4.0.9 requires tqdm>=4.64.1, but you have tqdm 4.64.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.28.1 which is incompatible.\n","ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.23.1 which is incompatible.\n","ibis-framework 9.2.0 requires pytz>=2022.7, but you have pytz 2022.2 which is incompatible.\n","jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\n","jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\n","langchain-core 0.3.19 requires packaging<25,>=23.2, but you have packaging 21.3 which is incompatible.\n","langchain-core 0.3.19 requires typing-extensions>=4.7, but you have typing-extensions 4.3.0 which is incompatible.\n","mizani 0.13.0 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n","nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.3.0 which is incompatible.\n","openai 1.54.4 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.3.0 which is incompatible.\n","optree 0.13.1 requires typing-extensions>=4.5.0, but you have typing-extensions 4.3.0 which is incompatible.\n","pandas-gbq 0.24.0 requires packaging>=22.0.0, but you have packaging 21.3 which is incompatible.\n","pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n","plotnine 0.14.1 requires matplotlib>=3.8.0, but you have matplotlib 3.5.2 which is incompatible.\n","plotnine 0.14.1 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n","pydantic 2.9.2 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.3.0 which is incompatible.\n","pydantic-core 2.23.4 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.3.0 which is incompatible.\n","pytensor 2.26.3 requires filelock>=3.15, but you have filelock 3.7.1 which is incompatible.\n","python-docx 1.1.2 requires typing-extensions>=4.9.0, but you have typing-extensions 4.3.0 which is incompatible.\n","sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.20.1 which is incompatible.\n","simple-parsing 0.1.6 requires typing-extensions>=4.5.0, but you have typing-extensions 4.3.0 which is incompatible.\n","sphinx 8.1.3 requires packaging>=23.0, but you have packaging 21.3 which is incompatible.\n","sphinx 8.1.3 requires requests>=2.30.0, but you have requests 2.28.1 which is incompatible.\n","tables 3.10.1 requires typing-extensions>=4.4.0, but you have typing-extensions 4.3.0 which is incompatible.\n","tensorflow 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.23.1 which is incompatible.\n","torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.1.2 which is incompatible.\n","torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.1.2 which is incompatible.\n","typeguard 4.4.1 requires typing-extensions>=4.10.0, but you have typing-extensions 4.3.0 which is incompatible.\n","wandb 0.18.7 requires typing-extensions<5,>=4.4; python_version < \"3.12\", but you have typing-extensions 4.3.0 which is incompatible.\n","xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\n","xarray 2024.10.0 requires packaging>=23.1, but you have packaging 21.3 which is incompatible.\n","yfinance 0.2.49 requires pytz>=2022.5, but you have pytz 2022.2 which is incompatible.\n","yfinance 0.2.49 requires requests>=2.31, but you have requests 2.28.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed PyYAML-6.0 attrdict-2.0.1 charset-normalizer-2.1.0 cycler-0.11.0 et-xmlfile-1.1.0 filelock-3.7.1 fonttools-4.34.4 idna-3.3 kiwisolver-1.4.4 matplotlib-3.5.2 numpy-1.23.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 packaging-21.3 pyparsing-3.0.9 pytz-2022.2 regex-2022.7.25 requests-2.28.1 tokenizers-0.12.1 torch-2.1.2 tqdm-4.64.0 transformers-4.20.1 triton-2.1.0 typing_extensions-4.3.0 urllib3-1.26.11\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["cycler","kiwisolver","matplotlib","mpl_toolkits","numpy","packaging"]},"id":"b7e5e768ac684471a7e07c831d0f2fb1"}},"metadata":{}}],"source":["!pip install -r /content/drive/MyDrive/training/30-finlaw-src/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113338,"status":"ok","timestamp":1732467735622,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"jb0HbEBCe6T9","outputId":"129d3fe5-d4d0-47e0-b51f-18b876d2e45e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torch==2.0.0\n","  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.16.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n","  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n","  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n","  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n","  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n","  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n","  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.0.0 (from torch==2.0.0)\n","  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (75.1.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.45.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (3.30.5)\n","Collecting lit (from triton==2.0.0->torch==2.0.0)\n","  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n","Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.5.1+cu121\n","    Uninstalling torch-2.5.1+cu121:\n","      Successfully uninstalled torch-2.5.1+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\n","torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 triton-2.0.0\n"]}],"source":["!pip install torch==2.0.0\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5430,"status":"ok","timestamp":1732543825069,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"EVQ0_BCnfwlL","outputId":"d904bc21-1121-4edd-b85d-e5aeead9d939"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: python-box in /usr/local/lib/python3.10/dist-packages (7.2.0)\n"]}],"source":["!pip install python-box\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1732543794854,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"AVbOf8CgaeuY","outputId":"0966d010-66e4-4e20-fddc-467ffb9e9711"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/training/30-finlaw-src/data_prepare\n"]}],"source":["cd data_prepare"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1732467738105,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"2MTA6UQfcVvI","outputId":"d6da957d-6af5-405f-fb1e-d2a0a528daa4"},"outputs":[{"name":"stdout","output_type":"stream","text":["data_prepare.json  koelectra_base.json\n"]}],"source":["ls ../config"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":255,"status":"ok","timestamp":1732467738352,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"pAZA7-iQcZ7n","outputId":"2bfb135b-eaef-49dd-928c-a51ce4440f80"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\r\n","  \"datatype_keywords\": {\r\n","    \"정답경계추출형\": \"answer_span\",\r\n","    \"YesNo 단문형\": \"yesno\",\r\n","    \"Table 정답 추출형\": \"table\",\r\n","    \"다지선다형(추론형)\": \"multi_choice\",\r\n","    \"절차(방법)\": \"list\"\r\n","  },\r\n","  \"data_dir\": \"../data\",\r\n","  \"save_dir\": \"../data\",\r\n","  \"data_filename_list\": {\r\n","    \"answer_span\": [\r\n","      \"sampled_answer_span_train.json\",\r\n","      \"sampled_answer_span_test.json\",\r\n","      \"answer_span_family.json\"\r\n","    ],\r\n","    \"yesno\": [\r\n","      \"sampled_yesno_train.json\",\r\n","      \"sampled_yesno_test.json\"\r\n","    ],\r\n","    \"table\": [\r\n","      \"sampled_table_train.json\",\r\n","      \"sampled_table_test.json\"\r\n","    ],\r\n","    \"multi_choice\": [\r\n","      \"shuffled_sampled_multi_choice_train.json\",\r\n","      \"shuffled_sampled_multi_choice_test.json\"\r\n","    ],\r\n","    \"list\": [\r\n","      \"sampled_list_train.json\",\r\n","      \"sampled_list_test.json\"\r\n","    ]\r\n","  }\r\n","}"]}],"source":[" cat ../config/data_prepare.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1176,"status":"ok","timestamp":1732467772236,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"3AV-z5e6csGv","outputId":"9df65e3c-6813-4481-ebc9-c165a0f8ee31"},"outputs":[{"name":"stdout","output_type":"stream","text":["usage: data_prepare.py [-h] --config_file CONFIG_FILE --task\n","                       {merge_answer_span_family,shuffle_multi_choice_option,show_data_info}\n","data_prepare.py: error: argument --task: invalid choice: '{TARGET_TASK}' (choose from 'merge_answer_span_family', 'shuffle_multi_choice_option', 'show_data_info')\n"]}],"source":["!python data_prepare.py --config_file {CONFIG_FILE_NAME} --task {TARGET_TASK}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7070,"status":"ok","timestamp":1732467780234,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"07ns_zZHhvMV","outputId":"26ff5f11-8607-4c65-ab3a-8658b1a273d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["saving answer_span_family.json ... Done!!\n"]}],"source":[" !python data_prepare.py --config_file data_prepare.json --task merge_answer_span_family"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261,"status":"ok","timestamp":1732467781953,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"kTEQZlbohzvs","outputId":"ed9f0afa-b71e-4d9e-f154-3877196866c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["answer_span_family.json         sampled_multi_choice_train.json\n","\u001b[0m\u001b[01;34mdata_cache\u001b[0m/                     sampled_table_test.json\n","sampled_answer_span_test.json   sampled_table_train.json\n","sampled_answer_span_train.json  sampled_yesno_test.json\n","sampled_list_test.json          sampled_yesno_train.json\n","sampled_list_train.json         shuffled_sampled_multi_choice_test.json\n","sampled_multi_choice_test.json  shuffled_sampled_multi_choice_train.json\n"]}],"source":["ls ../data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2034,"status":"ok","timestamp":1732467785690,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"2enQ0_X3iEUe","outputId":"6fa227c4-f0a9-4b41-eaa1-b0c09ffc7fec"},"outputs":[{"name":"stdout","output_type":"stream","text":["saving shuffled_sampled_multi_choice_test.json ... Done!!\n","saving shuffled_sampled_multi_choice_train.json ... Done!!\n"]}],"source":["!python data_prepare.py --config_file data_prepare.json --task shuffle_multi_choice_option"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":257,"status":"ok","timestamp":1732467787069,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"Dn2Y5PtNiUN-","outputId":"ee7cc458-5690-44f8-abe6-670bba6d5b9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["answer_span_family.json         sampled_multi_choice_train.json\n","\u001b[0m\u001b[01;34mdata_cache\u001b[0m/                     sampled_table_test.json\n","sampled_answer_span_test.json   sampled_table_train.json\n","sampled_answer_span_train.json  sampled_yesno_test.json\n","sampled_list_test.json          sampled_yesno_train.json\n","sampled_list_train.json         shuffled_sampled_multi_choice_test.json\n","sampled_multi_choice_test.json  shuffled_sampled_multi_choice_train.json\n"]}],"source":["ls ../data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1732467788904,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"9P0-QeHaiXbM","outputId":"3a477b8b-00cf-4272-8b2f-e9ceb5749b91"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\r\n","  \"datatype_keywords\": {\r\n","    \"정답경계추출형\": \"answer_span\",\r\n","    \"YesNo 단문형\": \"yesno\",\r\n","    \"Table 정답 추출형\": \"table\",\r\n","    \"다지선다형(추론형)\": \"multi_choice\",\r\n","    \"절차(방법)\": \"list\"\r\n","  },\r\n","  \"data_dir\": \"../data\",\r\n","  \"save_dir\": \"../data\",\r\n","  \"data_filename_list\": {\r\n","    \"answer_span\": [\r\n","      \"sampled_answer_span_train.json\",\r\n","      \"sampled_answer_span_test.json\",\r\n","      \"answer_span_family.json\"\r\n","    ],\r\n","    \"yesno\": [\r\n","      \"sampled_yesno_train.json\",\r\n","      \"sampled_yesno_test.json\"\r\n","    ],\r\n","    \"table\": [\r\n","      \"sampled_table_train.json\",\r\n","      \"sampled_table_test.json\"\r\n","    ],\r\n","    \"multi_choice\": [\r\n","      \"shuffled_sampled_multi_choice_train.json\",\r\n","      \"shuffled_sampled_multi_choice_test.json\"\r\n","    ],\r\n","    \"list\": [\r\n","      \"sampled_list_train.json\",\r\n","      \"sampled_list_test.json\"\r\n","    ]\r\n","  }\r\n","}"]}],"source":["#데이터 정보를 확인\n","!cat ../config/data_prepare.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4384,"status":"ok","timestamp":1732467795665,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"G38p4X_dibhk","outputId":"f0c3bd27-92e3-4d7f-91fc-a664c3e503a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," =============== answer_span ===============\n","sampled_answer_span_train.json\n","\t# of total_samples: 9624\n","\t# of context_num: 9039\n","\t# of answer_span_samples: 9624\n","\taverage context length: 615.8762031198141\n","\taverage question length: 43.19690357439734\n","\taverage answer length: 6.746882793017456\n","------------------------------------------------------------\n","sampled_answer_span_test.json\n","\t# of total_samples: 1203\n","\t# of context_num: 1121\n","\t# of answer_span_samples: 1203\n","\taverage context length: 611.7020517395183\n","\taverage question length: 43.33000831255195\n","\taverage answer length: 6.645054031587698\n","------------------------------------------------------------\n","answer_span_family.json\n","\t# of total_samples: 25644\n","\t# of context_num: 21173\n","\t# of answer_span_samples: 25644\n","\taverage context length: 544.8031927454778\n","\taverage question length: 41.18561846825768\n","\taverage answer length: 9.42661051318047\n","------------------------------------------------------------\n","\n"," =============== yesno ===============\n","sampled_yesno_train.json\n","\t# of total_samples: 3211\n","\t# of context_num: 2983\n","\t# of Yes_samples: 2083\n","\t# of No_samples: 1128\n","\taverage context length: 581.9416694602749\n","\taverage question length: 41.658673310495175\n","------------------------------------------------------------\n","sampled_yesno_test.json\n","\t# of total_samples: 401\n","\t# of context_num: 366\n","\t# of Yes_samples: 264\n","\t# of No_samples: 137\n","\taverage context length: 567.6174863387978\n","\taverage question length: 41.905236907730675\n","------------------------------------------------------------\n","\n"," =============== table ===============\n","sampled_table_train.json\n","\t# of total_samples: 9600\n","\t# of context_num: 6190\n","\t# of answer_span_samples: 9600\n","\taverage context length: 303.8080775444265\n","\taverage question length: 39.415729166666665\n","\taverage answer length: 4.561875\n","------------------------------------------------------------\n","sampled_table_test.json\n","\t# of total_samples: 1200\n","\t# of context_num: 761\n","\t# of answer_span_samples: 1200\n","\taverage context length: 315.557161629435\n","\taverage question length: 39.09166666666667\n","\taverage answer length: 4.525\n","------------------------------------------------------------\n","\n"," =============== multi_choice ===============\n","shuffled_sampled_multi_choice_train.json\n","\t# of total_samples: 3232\n","\t# of context_num: 3000\n","\t# of answer_1_samples: 787\n","\t# of answer_2_samples: 811\n","\t# of answer_3_samples: 801\n","\t# of answer_4_samples: 833\n","\taverage context length: 568.829\n","\taverage question length: 39.11293316831683\n","------------------------------------------------------------\n","shuffled_sampled_multi_choice_test.json\n","\t# of total_samples: 404\n","\t# of context_num: 374\n","\t# of answer_1_samples: 111\n","\t# of answer_2_samples: 102\n","\t# of answer_3_samples: 102\n","\t# of answer_4_samples: 89\n","\taverage context length: 551.5454545454545\n","\taverage question length: 39.039603960396036\n","------------------------------------------------------------\n","\n"," =============== list ===============\n","sampled_list_train.json\n","\t# of total_samples: 6420\n","\t# of context_num: 5944\n","\t# of answer_span_samples: 6420\n","\taverage context length: 687.6919582772543\n","\taverage question length: 40.81713395638629\n","\taverage answer length: 20.718068535825545\n","------------------------------------------------------------\n","sampled_list_test.json\n","\t# of total_samples: 803\n","\t# of context_num: 751\n","\t# of answer_span_samples: 803\n","\taverage context length: 697.977363515313\n","\taverage question length: 40.742216687422165\n","\taverage answer length: 20.450809464508094\n","------------------------------------------------------------\n"]}],"source":[" !python data_prepare.py --config_file data_prepare.json --task show_data_info"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1732467796721,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"LvJUP7DxijQF","outputId":"26d01b26-a5a9-4312-f4bf-946262f7c10a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/training/30-finlaw-src\n"]}],"source":["cd /content/drive/MyDrive/training/30-finlaw-src"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1009,"status":"ok","timestamp":1732441197621,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"B2vJ-Yibigg7","outputId":"097ebefd-30cd-4384-ae06-41fa02e1ff8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/training/30-finlaw-src/src\n"]}],"source":["cd src"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1732467798768,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"ipbT0ygOittM","outputId":"b745997c-1f81-488b-d78d-a618b7f251bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\r\n","  \"data_dir_answer_span_family\": \"data\",\r\n","  \"data_dir_yesno\": \"data\",\r\n","  \"data_dir_multi_choice\": \"data\",\r\n","  \"data_cache_dir\": \"data_cache\",\r\n","  \"ckpt_dir\": \"ckpt\",\r\n","  \"model_dir\": \"30_integrated_model_test\",\r\n","  \"train_file_answer_span_family\": [\"answer_span_family.json\"],\r\n","  \"predict_file_answer_span_family\": [\r\n","    \"sampled_answer_span_test.json\",\r\n","    \"sampled_table_test.json\",\r\n","    \"sampled_list_test.json\"\r\n","  ],\r\n","  \"train_file_yesno\": [\"sampled_yesno_train.json\"],\r\n","  \"predict_file_yesno\": [\"sampled_yesno_test.json\"],\r\n","  \"train_file_multi_choice\": [\"shuffled_sampled_multi_choice_train.json\"],\r\n","  \"predict_file_multi_choice\": [\"shuffled_sampled_multi_choice_test.json\"],\r\n","  \"threads\": 8,\r\n","  \"score_method_with_white_space\": false,\r\n","  \"null_score_diff_threshold\": 20.0,\r\n","  \"max_seq_length\": 512,\r\n","  \"doc_stride\": 128,\r\n","  \"max_query_length\": 64,\r\n","  \"max_answer_length\": 64,\r\n","  \"max_option_length\": 64,\r\n","  \"n_best_size\": 20,\r\n","  \"verbose_logging\": true,\r\n","  \"save_optimizer\": false,\r\n","  \"do_lower_case\": false,\r\n","  \"do_train\": true,\r\n","  \"do_eval\": true,\r\n","  \"num_train_epochs\": 2,\r\n","  \"weight_decay\": 0.0,\r\n","  \"gradient_accumulation_steps\": 1,\r\n","  \"loss_weight\": {\r\n","    \"answer_span\": 0.334,\r\n","    \"yesno\": 0.333,\r\n","    \"multi_choice\": 0.333\r\n","  },\r\n","  \"adam_epsilon\": 1e-6,\r\n","  \"warmup_proportion\": 0,\r\n","  \"max_steps\": -1,\r\n","  \"max_grad_norm\": 1.0,\r\n","  \"no_cuda\": false,\r\n","  \"model_type\": \"koelectra-base-v3\",\r\n","  \"model_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\r\n","  \"seed\": 42,\r\n","  \"train_batch_size\": 48,\r\n","  \"eval_batch_size\": 128,\r\n","  \"target_epoch\": -1,\r\n","  \"logging_steps\": 2000,\r\n","  \"save_steps\": 0,\r\n","  \"learning_rate\": 5e-5,\r\n","  \"result_dir_name\": \"result/30_integrated_model_test\",\r\n","  \"cuda_visible_devices\": \"0,1,2,3\"\r\n","}\r\n"]}],"source":["cat /content/drive/MyDrive/training/30-finlaw-src/config/koelectra_base.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132999,"status":"ok","timestamp":1732512922891,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"HY6Sduswkgne","outputId":"151ea5a6-c28c-4b2f-c218-01e37a4874dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.7.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.3.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.77)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.1)\n","Collecting torch\n","  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n","Collecting typing-extensions>=4.8.0 (from torch)\n","  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==8.9.2.26->torch)\n","  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.1.0 (from torch)\n","  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n","Installing collected packages: typing-extensions, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.3.0\n","    Uninstalling typing_extensions-4.3.0:\n","      Successfully uninstalled typing_extensions-4.3.0\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.1.0\n","    Uninstalling triton-2.1.0:\n","      Successfully uninstalled triton-2.1.0\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.1.105\n","    Uninstalling nvidia-nvtx-cu12-12.1.105:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.6.77\n","    Uninstalling nvidia-nvjitlink-cu12-12.6.77:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.77\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.18.1\n","    Uninstalling nvidia-nccl-cu12-2.18.1:\n","      Successfully uninstalled nvidia-nccl-cu12-2.18.1\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.2.106\n","    Uninstalling nvidia-curand-cu12-10.3.2.106:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n","    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n","      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n","    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n","    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n","    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n","    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n","    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n","      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n","    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.2\n","    Uninstalling torch-2.1.2:\n","      Successfully uninstalled torch-2.1.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 1.27.0 requires matplotlib>=3.7.1, but you have matplotlib 3.5.2 which is incompatible.\n","bigframes 1.27.0 requires numpy>=1.24.0, but you have numpy 1.23.1 which is incompatible.\n","chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.23.1 which is incompatible.\n","ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.23.1 which is incompatible.\n","ibis-framework 9.2.0 requires pytz>=2022.7, but you have pytz 2022.2 which is incompatible.\n","langchain-core 0.3.19 requires packaging<25,>=23.2, but you have packaging 21.3 which is incompatible.\n","sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.20.1 which is incompatible.\n","tensorflow 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.23.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.5.1 triton-3.1.0 typing-extensions-4.12.2\n"]}],"source":["!pip install torch torchvision"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4830,"status":"ok","timestamp":1732543831999,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"3j_PDlg7rN9I","outputId":"0fe5b132-fbf6-4289-c747-84b113600d39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.64.0)\n"]}],"source":["!pip install tqdm\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"2XxF0Ne1uylK","executionInfo":{"status":"ok","timestamp":1732543831999,"user_tz":-540,"elapsed":2,"user":{"displayName":"Min","userId":"00121443274832754975"}}},"outputs":[],"source":["rm -rf __pycache__\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"kbgxHP9WjdeZ","outputId":"a927b305-869f-4e69-f216-09bb30df986b"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-11-24 17:05:55.877805: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-11-24 17:05:55.894732: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-24 17:05:55.915933: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-24 17:05:55.922430: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-24 17:05:55.937543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-24 17:05:57.115440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","11/24/2024 17:06:03 - INFO - __main__ -   Training/evaluation parameters {'data_dir_answer_span_family': 'data', 'data_dir_yesno': 'data', 'data_dir_multi_choice': 'data', 'data_cache_dir': 'data_cache', 'ckpt_dir': 'ckpt', 'model_dir': 'ckpt/30_integrated_model_test', 'train_file_answer_span_family': ['answer_span_family.json'], 'predict_file_answer_span_family': ['sampled_answer_span_test.json', 'sampled_table_test.json', 'sampled_list_test.json'], 'train_file_yesno': ['sampled_yesno_train.json'], 'predict_file_yesno': ['sampled_yesno_test.json'], 'train_file_multi_choice': ['shuffled_sampled_multi_choice_train.json'], 'predict_file_multi_choice': ['shuffled_sampled_multi_choice_test.json'], 'threads': 8, 'score_method_with_white_space': False, 'null_score_diff_threshold': 20.0, 'max_seq_length': 512, 'doc_stride': 128, 'max_query_length': 64, 'max_answer_length': 64, 'max_option_length': 64, 'n_best_size': 20, 'verbose_logging': True, 'save_optimizer': False, 'do_lower_case': False, 'do_train': True, 'do_eval': True, 'num_train_epochs': 2, 'weight_decay': 0.0, 'gradient_accumulation_steps': 1, 'loss_weight': {'answer_span': 0.334, 'yesno': 0.333, 'multi_choice': 0.333}, 'adam_epsilon': 1e-06, 'warmup_proportion': 0, 'max_steps': -1, 'max_grad_norm': 1.0, 'no_cuda': False, 'model_type': 'koelectra-base-v3', 'model_name_or_path': 'monologg/koelectra-base-v3-discriminator', 'seed': 42, 'train_batch_size': 48, 'eval_batch_size': 128, 'target_epoch': -1, 'logging_steps': 2000, 'save_steps': 0, 'learning_rate': 5e-05, 'result_dir_name': 'result/30_integrated_model_test', 'cuda_visible_devices': '0,1,2,3', 'device': 'cuda'}\n","tokenizer_config.json: 100% 61.0/61.0 [00:00<00:00, 497kB/s]\n","vocab.txt: 100% 263k/263k [00:00<00:00, 5.20MB/s]\n","config.json: 100% 467/467 [00:00<00:00, 4.69MB/s]\n","11/24/2024 17:06:04 - INFO - utils.utils -    model_name_or_path = monologg/koelectra-base-v3-discriminator\n","pytorch_model.bin: 100% 452M/452M [00:01<00:00, 233MB/s]\n","Some weights of QuestionAnsweringForIntegratedElectra were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['electra.classifier_multi_choice.dense.bias', 'electra.classifier_multi_choice.dense.weight', 'electra.classifier_multi_choice.out_proj.bias', 'electra.classifier_multi_choice.out_proj.weight', 'electra.classifier_yesno.dense.bias', 'electra.classifier_yesno.dense.weight', 'electra.classifier_yesno.out_proj.bias', 'electra.classifier_yesno.out_proj.weight', 'electra.embedding_answer_span.LayerNorm.bias', 'electra.embedding_answer_span.LayerNorm.weight', 'electra.embedding_answer_span.position_embeddings.weight', 'electra.embedding_answer_span.token_type_embeddings.weight', 'electra.embedding_answer_span.word_embeddings.weight', 'electra.embedding_multi_choice.LayerNorm.bias', 'electra.embedding_multi_choice.LayerNorm.weight', 'electra.embedding_multi_choice.position_embeddings.weight', 'electra.embedding_multi_choice.token_type_embeddings.weight', 'electra.embedding_multi_choice.word_embeddings.weight', 'electra.embedding_yesno.LayerNorm.bias', 'electra.embedding_yesno.LayerNorm.weight', 'electra.embedding_yesno.position_embeddings.weight', 'electra.embedding_yesno.token_type_embeddings.weight', 'electra.embedding_yesno.word_embeddings.weight', 'electra.qa_outputs.bias', 'electra.qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\n","\n","\ttarget is - answer_span_family.json \n","\n","11/24/2024 17:06:08 - INFO - utils.utils -   Loading features from cached file data/data_cache/cached_answer_span_family_512_inte\n","/content/drive/MyDrive/training/30-finlaw-src/utils/utils.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dataset = torch.load(cached_features_file_path)['dataset']\n","\n","\n","\ttarget is - sampled_yesno_train.json \n","\n","11/24/2024 17:06:18 - INFO - utils.utils -   Loading features from cached file data/data_cache/cached_sampled_yesno_train_512_inte\n","/content/drive/MyDrive/training/30-finlaw-src/utils/utils.py:199: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dataset = torch.load(cached_features_file_path)['dataset']\n","\n","\n","\ttarget is - shuffled_sampled_multi_choice_train.json \n","\n","11/24/2024 17:06:18 - INFO - utils.utils -   Loading features from cached file data/data_cache/cached_shuffled_sampled_multi_choice_train_512_inte\n","/content/drive/MyDrive/training/30-finlaw-src/utils/utils.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dataset = torch.load(cached_features_file_path)['dataset']\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","11/24/2024 17:06:19 - INFO - train_eval -   ***** Running training *****\n","11/24/2024 17:06:19 - INFO - train_eval -     Num examples = 100413\n","11/24/2024 17:06:19 - INFO - train_eval -     Num Epochs = 2\n","11/24/2024 17:06:19 - INFO - train_eval -     Train batch size per GPU = 48\n","11/24/2024 17:06:19 - INFO - train_eval -     Total train batch size (w. parallel, distributed & accumulation) = 48\n","11/24/2024 17:06:19 - INFO - train_eval -     Gradient Accumulation steps = 1\n","11/24/2024 17:06:19 - INFO - train_eval -     Total optimization steps = 4184\n","Epochs:   0% 0/2 [00:00<?, ?it/s]\n","Steps:   0% 0/2092 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","11/24/2024 17:06:20 - INFO - train_eval -   *****0 - 0: loss: 2.5896830558776855 *****\n","\n","Steps:   0% 1/2092 [00:01<1:01:48,  1.77s/it]\u001b[A\n","Steps:   0% 2/2092 [00:03<51:07,  1.47s/it]  \u001b[A\n","Steps:   0% 3/2092 [00:04<44:02,  1.26s/it]\u001b[A\n","Steps:   0% 4/2092 [00:05<45:25,  1.31s/it]\u001b[A\n","Steps:   0% 5/2092 [00:06<41:43,  1.20s/it]\u001b[A\n","Steps:   0% 6/2092 [00:07<39:34,  1.14s/it]\u001b[A\n","Steps:   0% 7/2092 [00:08<38:06,  1.10s/it]\u001b[A\n","Steps:   0% 8/2092 [00:09<37:11,  1.07s/it]\u001b[A\n","Steps:   0% 9/2092 [00:10<36:40,  1.06s/it]\u001b[A\n","Steps:   0% 10/2092 [00:11<36:18,  1.05s/it]\u001b[A\n","Steps:   1% 11/2092 [00:12<36:09,  1.04s/it]\u001b[A\n","Steps:   1% 12/2092 [00:13<35:52,  1.04s/it]\u001b[A\n","Steps:   1% 13/2092 [00:14<35:49,  1.03s/it]\u001b[A\n","Steps:   1% 14/2092 [00:15<35:46,  1.03s/it]\u001b[A\n","Steps:   1% 15/2092 [00:16<35:43,  1.03s/it]\u001b[A\n","Steps:   1% 16/2092 [00:17<35:31,  1.03s/it]\u001b[A\n","Steps:   1% 17/2092 [00:18<35:24,  1.02s/it]\u001b[A\n","Steps:   1% 18/2092 [00:19<35:27,  1.03s/it]\u001b[A\n","Steps:   1% 19/2092 [00:20<35:22,  1.02s/it]\u001b[A\n","Steps:   1% 20/2092 [00:21<35:15,  1.02s/it]\u001b[A\n","Steps:   1% 21/2092 [00:22<35:08,  1.02s/it]\u001b[A\n","Steps:   1% 22/2092 [00:24<40:53,  1.19s/it]\u001b[A\n","Steps:   1% 23/2092 [00:25<39:02,  1.13s/it]\u001b[A\n","Steps:   1% 24/2092 [00:26<37:57,  1.10s/it]\u001b[A\n","Steps:   1% 25/2092 [00:27<37:12,  1.08s/it]\u001b[A\n","Steps:   1% 26/2092 [00:28<36:32,  1.06s/it]\u001b[A\n","Steps:   1% 27/2092 [00:29<36:06,  1.05s/it]\u001b[A\n","Steps:   1% 28/2092 [00:30<35:42,  1.04s/it]\u001b[A\n","Steps:   1% 29/2092 [00:31<35:34,  1.03s/it]\u001b[A\n","Steps:   1% 30/2092 [00:32<35:24,  1.03s/it]\u001b[A\n","Steps:   1% 31/2092 [00:33<35:11,  1.02s/it]\u001b[A\n","Steps:   2% 32/2092 [00:34<35:13,  1.03s/it]\u001b[A\n","Steps:   2% 33/2092 [00:35<35:12,  1.03s/it]\u001b[A\n","Steps:   2% 34/2092 [00:36<35:13,  1.03s/it]\u001b[A\n","Steps:   2% 35/2092 [00:37<35:03,  1.02s/it]\u001b[A\n","Steps:   2% 36/2092 [00:39<41:18,  1.21s/it]\u001b[A\n","Steps:   2% 37/2092 [00:40<39:27,  1.15s/it]\u001b[A\n","Steps:   2% 38/2092 [00:41<38:05,  1.11s/it]\u001b[A\n","Steps:   2% 39/2092 [00:42<37:02,  1.08s/it]\u001b[A\n","Steps:   2% 40/2092 [00:43<36:17,  1.06s/it]\u001b[A\n","Steps:   2% 41/2092 [00:44<35:57,  1.05s/it]\u001b[A\n","Steps:   2% 42/2092 [00:45<35:42,  1.04s/it]\u001b[A\n","Steps:   2% 43/2092 [00:46<35:28,  1.04s/it]\u001b[A\n","Steps:   2% 44/2092 [00:47<35:12,  1.03s/it]\u001b[A\n","Steps:   2% 45/2092 [00:48<35:03,  1.03s/it]\u001b[A\n","Steps:   2% 46/2092 [00:49<35:01,  1.03s/it]\u001b[A\n","Steps:   2% 47/2092 [00:50<34:50,  1.02s/it]\u001b[A\n","Steps:   2% 48/2092 [00:51<34:39,  1.02s/it]\u001b[A\n","Steps:   2% 49/2092 [00:52<34:38,  1.02s/it]\u001b[A\n","Steps:   2% 50/2092 [00:53<34:30,  1.01s/it]\u001b[A\n","Steps:   2% 51/2092 [00:54<34:33,  1.02s/it]\u001b[A\n","Steps:   2% 52/2092 [00:55<34:39,  1.02s/it]\u001b[A\n","Steps:   3% 53/2092 [00:56<34:39,  1.02s/it]\u001b[A\n","Steps:   3% 54/2092 [00:57<34:37,  1.02s/it]\u001b[A\n","Steps:   3% 55/2092 [00:58<34:35,  1.02s/it]\u001b[A\n","Steps:   3% 56/2092 [00:59<34:38,  1.02s/it]\u001b[A\n","Steps:   3% 57/2092 [01:00<34:37,  1.02s/it]\u001b[A\n","Steps:   3% 58/2092 [01:01<34:39,  1.02s/it]\u001b[A\n","Steps:   3% 59/2092 [01:02<34:31,  1.02s/it]\u001b[A\n","Steps:   3% 60/2092 [01:03<34:25,  1.02s/it]\u001b[A\n","Steps:   3% 61/2092 [01:04<34:27,  1.02s/it]\u001b[A\n","Steps:   3% 62/2092 [01:05<34:27,  1.02s/it]\u001b[A\n","Steps:   3% 63/2092 [01:06<34:28,  1.02s/it]\u001b[A\n","Steps:   3% 64/2092 [01:08<40:03,  1.19s/it]\u001b[A\n","Steps:   3% 65/2092 [01:09<38:27,  1.14s/it]\u001b[A\n","Steps:   3% 66/2092 [01:10<37:09,  1.10s/it]\u001b[A\n","Steps:   3% 67/2092 [01:11<36:25,  1.08s/it]\u001b[A\n","Steps:   3% 68/2092 [01:12<35:43,  1.06s/it]\u001b[A\n","Steps:   3% 69/2092 [01:13<35:12,  1.04s/it]\u001b[A\n","Steps:   3% 70/2092 [01:14<34:56,  1.04s/it]\u001b[A\n","Steps:   3% 71/2092 [01:15<34:46,  1.03s/it]\u001b[A\n","Steps:   3% 72/2092 [01:16<34:30,  1.03s/it]\u001b[A\n","Steps:   3% 73/2092 [01:17<34:25,  1.02s/it]\u001b[A\n","Steps:   4% 74/2092 [01:18<34:22,  1.02s/it]\u001b[A\n","Steps:   4% 75/2092 [01:19<34:14,  1.02s/it]\u001b[A\n","Steps:   4% 76/2092 [01:20<34:08,  1.02s/it]\u001b[A\n","Steps:   4% 77/2092 [01:21<34:12,  1.02s/it]\u001b[A\n","Steps:   4% 78/2092 [01:22<34:17,  1.02s/it]\u001b[A\n","Steps:   4% 79/2092 [01:23<34:22,  1.02s/it]\u001b[A\n","Steps:   4% 80/2092 [01:24<34:24,  1.03s/it]\u001b[A\n","Steps:   4% 81/2092 [01:25<34:14,  1.02s/it]\u001b[A\n","Steps:   4% 82/2092 [01:26<34:13,  1.02s/it]\u001b[A\n","Steps:   4% 83/2092 [01:27<34:14,  1.02s/it]\u001b[A\n","Steps:   4% 84/2092 [01:28<34:05,  1.02s/it]\u001b[A\n","Steps:   4% 85/2092 [01:29<33:56,  1.01s/it]\u001b[A\n","Steps:   4% 86/2092 [01:30<34:01,  1.02s/it]\u001b[A\n","Steps:   4% 87/2092 [01:31<34:06,  1.02s/it]\u001b[A\n","Steps:   4% 88/2092 [01:32<34:05,  1.02s/it]\u001b[A\n","Steps:   4% 89/2092 [01:33<34:09,  1.02s/it]\u001b[A\n","Steps:   4% 90/2092 [01:34<34:05,  1.02s/it]\u001b[A\n","Steps:   4% 91/2092 [01:35<34:02,  1.02s/it]\u001b[A\n","Steps:   4% 92/2092 [01:36<34:05,  1.02s/it]\u001b[A\n","Steps:   4% 93/2092 [01:37<34:07,  1.02s/it]\u001b[A\n","Steps:   4% 94/2092 [01:38<34:08,  1.03s/it]\u001b[A\n","Steps:   5% 95/2092 [01:39<34:08,  1.03s/it]\u001b[A\n","Steps:   5% 96/2092 [01:41<34:09,  1.03s/it]\u001b[A\n","Steps:   5% 97/2092 [01:42<34:04,  1.02s/it]\u001b[A\n","Steps:   5% 98/2092 [01:43<34:07,  1.03s/it]\u001b[A\n","Steps:   5% 99/2092 [01:44<34:08,  1.03s/it]\u001b[A\n","Steps:   5% 100/2092 [01:45<34:01,  1.02s/it]\u001b[A\n","Steps:   5% 101/2092 [01:46<33:50,  1.02s/it]\u001b[A\n","Steps:   5% 102/2092 [01:47<33:50,  1.02s/it]\u001b[A\n","Steps:   5% 103/2092 [01:48<33:52,  1.02s/it]\u001b[A\n","Steps:   5% 104/2092 [01:49<33:44,  1.02s/it]\u001b[A\n","Steps:   5% 105/2092 [01:50<33:48,  1.02s/it]\u001b[A\n","Steps:   5% 106/2092 [01:51<33:52,  1.02s/it]\u001b[A\n","Steps:   5% 107/2092 [01:52<33:51,  1.02s/it]\u001b[A\n","Steps:   5% 108/2092 [01:53<33:48,  1.02s/it]\u001b[A\n","Steps:   5% 109/2092 [01:54<33:49,  1.02s/it]\u001b[A\n","Steps:   5% 110/2092 [01:55<33:45,  1.02s/it]\u001b[A\n","Steps:   5% 111/2092 [01:56<33:42,  1.02s/it]\u001b[A\n","Steps:   5% 112/2092 [01:57<33:36,  1.02s/it]\u001b[A\n","Steps:   5% 113/2092 [01:58<33:41,  1.02s/it]\u001b[A\n","Steps:   5% 114/2092 [01:59<33:38,  1.02s/it]\u001b[A\n","Steps:   5% 115/2092 [02:00<33:43,  1.02s/it]\u001b[A\n","Steps:   6% 116/2092 [02:01<33:46,  1.03s/it]\u001b[A\n","Steps:   6% 117/2092 [02:02<33:47,  1.03s/it]\u001b[A\n","Steps:   6% 118/2092 [02:03<33:33,  1.02s/it]\u001b[A\n","Steps:   6% 119/2092 [02:04<33:38,  1.02s/it]\u001b[A\n","Steps:   6% 120/2092 [02:05<33:35,  1.02s/it]\u001b[A\n","Steps:   6% 121/2092 [02:06<33:33,  1.02s/it]\u001b[A\n","Steps:   6% 122/2092 [02:07<33:37,  1.02s/it]\u001b[A\n","Steps:   6% 123/2092 [02:08<33:38,  1.03s/it]\u001b[A\n","Steps:   6% 124/2092 [02:09<33:35,  1.02s/it]\u001b[A\n","Steps:   6% 125/2092 [02:10<33:31,  1.02s/it]\u001b[A\n","Steps:   6% 126/2092 [02:11<33:23,  1.02s/it]\u001b[A\n","Steps:   6% 127/2092 [02:12<33:23,  1.02s/it]\u001b[A\n","Steps:   6% 128/2092 [02:13<33:17,  1.02s/it]\u001b[A\n","Steps:   6% 129/2092 [02:14<33:22,  1.02s/it]\u001b[A\n","Steps:   6% 130/2092 [02:15<33:16,  1.02s/it]\u001b[A\n","Steps:   6% 131/2092 [02:16<33:20,  1.02s/it]\u001b[A\n","Steps:   6% 132/2092 [02:17<33:22,  1.02s/it]\u001b[A\n","Steps:   6% 133/2092 [02:18<33:19,  1.02s/it]\u001b[A\n","Steps:   6% 134/2092 [02:19<33:22,  1.02s/it]\u001b[A\n","Steps:   6% 135/2092 [02:20<33:24,  1.02s/it]\u001b[A\n","Steps:   7% 136/2092 [02:22<39:18,  1.21s/it]\u001b[A\n","Steps:   7% 137/2092 [02:23<37:33,  1.15s/it]\u001b[A\n","Steps:   7% 138/2092 [02:24<36:13,  1.11s/it]\u001b[A\n","Steps:   7% 139/2092 [02:25<35:12,  1.08s/it]\u001b[A\n","Steps:   7% 140/2092 [02:26<34:30,  1.06s/it]\u001b[A\n","Steps:   7% 141/2092 [02:27<34:12,  1.05s/it]\u001b[A\n","Steps:   7% 142/2092 [02:28<33:51,  1.04s/it]\u001b[A\n","Steps:   7% 143/2092 [02:29<33:38,  1.04s/it]\u001b[A\n","Steps:   7% 144/2092 [02:30<33:32,  1.03s/it]\u001b[A\n","Steps:   7% 145/2092 [02:31<33:27,  1.03s/it]\u001b[A\n","Steps:   7% 146/2092 [02:32<33:19,  1.03s/it]\u001b[A\n","Steps:   7% 147/2092 [02:33<33:12,  1.02s/it]\u001b[A\n","Steps:   7% 148/2092 [02:34<33:08,  1.02s/it]\u001b[A\n","Steps:   7% 149/2092 [02:35<33:05,  1.02s/it]\u001b[A\n","Steps:   7% 150/2092 [02:36<33:03,  1.02s/it]\u001b[A\n","Steps:   7% 151/2092 [02:37<33:07,  1.02s/it]\u001b[A\n","Steps:   7% 152/2092 [02:38<33:07,  1.02s/it]\u001b[A\n","Steps:   7% 153/2092 [02:39<33:04,  1.02s/it]\u001b[A\n","Steps:   7% 154/2092 [02:40<33:05,  1.02s/it]\u001b[A\n","Steps:   7% 155/2092 [02:41<33:01,  1.02s/it]\u001b[A\n","Steps:   7% 156/2092 [02:42<33:03,  1.02s/it]\u001b[A\n","Steps:   8% 157/2092 [02:43<33:03,  1.02s/it]\u001b[A\n","Steps:   8% 158/2092 [02:44<32:57,  1.02s/it]\u001b[A\n","Steps:   8% 159/2092 [02:45<32:49,  1.02s/it]\u001b[A\n","Steps:   8% 160/2092 [02:47<32:54,  1.02s/it]\u001b[A\n","Steps:   8% 161/2092 [02:48<32:55,  1.02s/it]\u001b[A\n","Steps:   8% 162/2092 [02:49<32:57,  1.02s/it]\u001b[A\n","Steps:   8% 163/2092 [02:50<32:59,  1.03s/it]\u001b[A\n","Steps:   8% 164/2092 [02:51<33:00,  1.03s/it]\u001b[A\n","Steps:   8% 165/2092 [02:52<32:49,  1.02s/it]\u001b[A\n","Steps:   8% 166/2092 [02:53<32:52,  1.02s/it]\u001b[A\n","Steps:   8% 167/2092 [02:54<32:49,  1.02s/it]\u001b[A\n","Steps:   8% 168/2092 [02:55<32:46,  1.02s/it]\u001b[A\n","Steps:   8% 169/2092 [02:56<32:35,  1.02s/it]\u001b[A\n","Steps:   8% 170/2092 [02:57<32:36,  1.02s/it]\u001b[A\n","Steps:   8% 171/2092 [02:58<32:35,  1.02s/it]\u001b[A\n","Steps:   8% 172/2092 [02:59<32:41,  1.02s/it]\u001b[A\n","Steps:   8% 173/2092 [03:00<32:39,  1.02s/it]\u001b[A\n","Steps:   8% 174/2092 [03:01<32:40,  1.02s/it]\u001b[A\n","Steps:   8% 175/2092 [03:02<32:37,  1.02s/it]\u001b[A\n","Steps:   8% 176/2092 [03:03<32:41,  1.02s/it]\u001b[A\n","Steps:   8% 177/2092 [03:04<32:44,  1.03s/it]\u001b[A\n","Steps:   9% 178/2092 [03:05<32:33,  1.02s/it]\u001b[A\n","Steps:   9% 179/2092 [03:06<32:25,  1.02s/it]\u001b[A\n","Steps:   9% 180/2092 [03:07<32:25,  1.02s/it]\u001b[A\n","Steps:   9% 181/2092 [03:08<32:28,  1.02s/it]\u001b[A\n","Steps:   9% 182/2092 [03:09<32:21,  1.02s/it]\u001b[A\n","Steps:   9% 183/2092 [03:10<32:21,  1.02s/it]\u001b[A\n","Steps:   9% 184/2092 [03:11<32:26,  1.02s/it]\u001b[A\n","Steps:   9% 185/2092 [03:12<32:30,  1.02s/it]\u001b[A\n","Steps:   9% 186/2092 [03:13<32:29,  1.02s/it]\u001b[A\n","Steps:   9% 187/2092 [03:14<32:32,  1.02s/it]\u001b[A\n","Steps:   9% 188/2092 [03:15<32:19,  1.02s/it]\u001b[A\n","Steps:   9% 189/2092 [03:16<32:22,  1.02s/it]\u001b[A\n","Steps:   9% 190/2092 [03:17<32:20,  1.02s/it]\u001b[A\n","Steps:   9% 191/2092 [03:18<32:19,  1.02s/it]\u001b[A\n","Steps:   9% 192/2092 [03:19<32:23,  1.02s/it]\u001b[A\n","Steps:   9% 193/2092 [03:20<32:26,  1.02s/it]\u001b[A\n","Steps:   9% 194/2092 [03:21<32:23,  1.02s/it]\u001b[A\n","Steps:   9% 195/2092 [03:22<32:19,  1.02s/it]\u001b[A\n","Steps:   9% 196/2092 [03:23<32:21,  1.02s/it]\u001b[A\n","Steps:   9% 197/2092 [03:24<32:18,  1.02s/it]\u001b[A\n","Steps:   9% 198/2092 [03:25<32:15,  1.02s/it]\u001b[A\n","Steps:  10% 199/2092 [03:26<32:19,  1.02s/it]\u001b[A\n","Steps:  10% 200/2092 [03:27<32:20,  1.03s/it]\u001b[A\n","Steps:  10% 201/2092 [03:28<32:15,  1.02s/it]\u001b[A\n","Steps:  10% 202/2092 [03:29<32:11,  1.02s/it]\u001b[A\n","Steps:  10% 203/2092 [03:30<32:04,  1.02s/it]\u001b[A\n","Steps:  10% 204/2092 [03:31<32:07,  1.02s/it]\u001b[A\n","Steps:  10% 205/2092 [03:33<32:09,  1.02s/it]\u001b[A\n","Steps:  10% 206/2092 [03:34<32:01,  1.02s/it]\u001b[A\n","Steps:  10% 207/2092 [03:35<32:05,  1.02s/it]\u001b[A\n","Steps:  10% 208/2092 [03:36<32:06,  1.02s/it]\u001b[A\n","Steps:  10% 209/2092 [03:37<32:02,  1.02s/it]\u001b[A\n","Steps:  10% 210/2092 [03:38<32:00,  1.02s/it]\u001b[A\n","Steps:  10% 211/2092 [03:39<31:53,  1.02s/it]\u001b[A\n","Steps:  10% 212/2092 [03:40<31:54,  1.02s/it]\u001b[A\n","Steps:  10% 213/2092 [03:41<31:53,  1.02s/it]\u001b[A\n","Steps:  10% 214/2092 [03:42<31:57,  1.02s/it]\u001b[A\n","Steps:  10% 215/2092 [03:43<31:50,  1.02s/it]\u001b[A\n","Steps:  10% 216/2092 [03:44<31:51,  1.02s/it]\u001b[A\n","Steps:  10% 217/2092 [03:45<31:51,  1.02s/it]\u001b[A\n","Steps:  10% 218/2092 [03:46<31:46,  1.02s/it]\u001b[A\n","Steps:  10% 219/2092 [03:47<31:45,  1.02s/it]\u001b[A\n","Steps:  11% 220/2092 [03:48<31:38,  1.01s/it]\u001b[A\n","Steps:  11% 221/2092 [03:49<31:39,  1.02s/it]\u001b[A\n","Steps:  11% 222/2092 [03:50<31:39,  1.02s/it]\u001b[A\n","Steps:  11% 223/2092 [03:51<31:35,  1.01s/it]\u001b[A\n","Steps:  11% 224/2092 [03:52<31:38,  1.02s/it]\u001b[A\n","Steps:  11% 225/2092 [03:53<31:38,  1.02s/it]\u001b[A\n","Steps:  11% 226/2092 [03:54<31:38,  1.02s/it]\u001b[A\n","Steps:  11% 227/2092 [03:55<31:43,  1.02s/it]\u001b[A\n","Steps:  11% 228/2092 [03:56<31:47,  1.02s/it]\u001b[A\n","Steps:  11% 229/2092 [03:57<31:39,  1.02s/it]\u001b[A\n","Steps:  11% 230/2092 [03:58<31:38,  1.02s/it]\u001b[A\n","Steps:  11% 231/2092 [03:59<31:40,  1.02s/it]\u001b[A\n","Steps:  11% 232/2092 [04:00<31:39,  1.02s/it]\u001b[A\n","Steps:  11% 233/2092 [04:01<31:41,  1.02s/it]\u001b[A\n","Steps:  11% 234/2092 [04:02<31:38,  1.02s/it]\u001b[A\n","Steps:  11% 235/2092 [04:03<31:36,  1.02s/it]\u001b[A\n","Steps:  11% 236/2092 [04:04<31:28,  1.02s/it]\u001b[A\n","Steps:  11% 237/2092 [04:05<31:33,  1.02s/it]\u001b[A\n","Steps:  11% 238/2092 [04:06<31:36,  1.02s/it]\u001b[A\n","Steps:  11% 239/2092 [04:07<31:28,  1.02s/it]\u001b[A\n","Steps:  11% 240/2092 [04:08<31:23,  1.02s/it]\u001b[A\n","Steps:  12% 241/2092 [04:09<31:26,  1.02s/it]\u001b[A\n","Steps:  12% 242/2092 [04:10<31:25,  1.02s/it]\u001b[A\n","Steps:  12% 243/2092 [04:11<31:28,  1.02s/it]\u001b[A\n","Steps:  12% 244/2092 [04:12<31:21,  1.02s/it]\u001b[A\n","Steps:  12% 245/2092 [04:13<31:26,  1.02s/it]\u001b[A\n","Steps:  12% 246/2092 [04:14<31:21,  1.02s/it]\u001b[A\n","Steps:  12% 247/2092 [04:15<31:22,  1.02s/it]\u001b[A\n","Steps:  12% 248/2092 [04:16<31:24,  1.02s/it]\u001b[A\n","Steps:  12% 249/2092 [04:17<31:26,  1.02s/it]\u001b[A\n","Steps:  12% 250/2092 [04:18<31:28,  1.03s/it]\u001b[A\n","Steps:  12% 251/2092 [04:19<31:30,  1.03s/it]\u001b[A\n","Steps:  12% 252/2092 [04:20<31:26,  1.03s/it]\u001b[A\n","Steps:  12% 253/2092 [04:21<31:25,  1.03s/it]\u001b[A\n","Steps:  12% 254/2092 [04:22<31:20,  1.02s/it]\u001b[A\n","Steps:  12% 255/2092 [04:24<31:18,  1.02s/it]\u001b[A\n","Steps:  12% 256/2092 [04:25<31:14,  1.02s/it]\u001b[A\n","Steps:  12% 257/2092 [04:26<31:04,  1.02s/it]\u001b[A\n","Steps:  12% 258/2092 [04:27<31:09,  1.02s/it]\u001b[A\n","Steps:  12% 259/2092 [04:28<31:06,  1.02s/it]\u001b[A\n","Steps:  12% 260/2092 [04:29<31:17,  1.02s/it]\u001b[A\n","Steps:  12% 261/2092 [04:30<31:14,  1.02s/it]\u001b[A\n","Steps:  13% 262/2092 [04:31<31:07,  1.02s/it]\u001b[A\n","Steps:  13% 263/2092 [04:32<31:06,  1.02s/it]\u001b[A\n","Steps:  13% 264/2092 [04:33<31:03,  1.02s/it]\u001b[A\n","Steps:  13% 265/2092 [04:34<30:56,  1.02s/it]\u001b[A\n","Steps:  13% 266/2092 [04:35<31:01,  1.02s/it]\u001b[A\n","Steps:  13% 267/2092 [04:36<31:02,  1.02s/it]\u001b[A\n","Steps:  13% 268/2092 [04:37<31:04,  1.02s/it]\u001b[A\n","Steps:  13% 269/2092 [04:38<30:56,  1.02s/it]\u001b[A\n","Steps:  13% 270/2092 [04:39<30:50,  1.02s/it]\u001b[A\n","Steps:  13% 271/2092 [04:40<30:55,  1.02s/it]\u001b[A\n","Steps:  13% 272/2092 [04:41<30:54,  1.02s/it]\u001b[A\n","Steps:  13% 273/2092 [04:42<30:58,  1.02s/it]\u001b[A\n","Steps:  13% 274/2092 [04:43<30:50,  1.02s/it]\u001b[A\n","Steps:  13% 275/2092 [04:44<30:47,  1.02s/it]\u001b[A\n","Steps:  13% 276/2092 [04:45<30:53,  1.02s/it]\u001b[A\n","Steps:  13% 277/2092 [04:46<30:53,  1.02s/it]\u001b[A\n","Steps:  13% 278/2092 [04:47<30:53,  1.02s/it]\u001b[A\n","Steps:  13% 279/2092 [04:48<30:51,  1.02s/it]\u001b[A\n","Steps:  13% 280/2092 [04:49<30:45,  1.02s/it]\u001b[A\n","Steps:  13% 281/2092 [04:50<30:44,  1.02s/it]\u001b[A\n","Steps:  13% 282/2092 [04:51<30:49,  1.02s/it]\u001b[A\n","Steps:  14% 283/2092 [04:52<30:51,  1.02s/it]\u001b[A\n","Steps:  14% 284/2092 [04:53<30:52,  1.02s/it]\u001b[A\n","Steps:  14% 285/2092 [04:54<30:51,  1.02s/it]\u001b[A\n","Steps:  14% 286/2092 [04:55<30:53,  1.03s/it]\u001b[A\n","Steps:  14% 287/2092 [04:56<30:48,  1.02s/it]\u001b[A\n","Steps:  14% 288/2092 [04:57<30:43,  1.02s/it]\u001b[A\n","Steps:  14% 289/2092 [04:58<30:35,  1.02s/it]\u001b[A\n","Steps:  14% 290/2092 [04:59<30:36,  1.02s/it]\u001b[A\n","Steps:  14% 291/2092 [05:00<30:30,  1.02s/it]\u001b[A\n","Steps:  14% 292/2092 [05:01<30:30,  1.02s/it]\u001b[A\n","Steps:  14% 293/2092 [05:02<30:29,  1.02s/it]\u001b[A\n","Steps:  14% 294/2092 [05:03<30:30,  1.02s/it]\u001b[A\n","Steps:  14% 295/2092 [05:04<30:35,  1.02s/it]\u001b[A\n","Steps:  14% 296/2092 [05:05<30:34,  1.02s/it]\u001b[A\n","Steps:  14% 297/2092 [05:06<30:31,  1.02s/it]\u001b[A\n","Steps:  14% 298/2092 [05:07<30:35,  1.02s/it]\u001b[A\n","Steps:  14% 299/2092 [05:08<30:31,  1.02s/it]\u001b[A\n","Steps:  14% 300/2092 [05:09<30:23,  1.02s/it]\u001b[A\n","Steps:  14% 301/2092 [05:10<30:27,  1.02s/it]\u001b[A\n","Steps:  14% 302/2092 [05:11<30:31,  1.02s/it]\u001b[A\n","Steps:  14% 303/2092 [05:12<30:23,  1.02s/it]\u001b[A\n","Steps:  15% 304/2092 [05:13<30:22,  1.02s/it]\u001b[A\n","Steps:  15% 305/2092 [05:15<30:15,  1.02s/it]\u001b[A\n","Steps:  15% 306/2092 [05:16<30:18,  1.02s/it]\u001b[A\n","Steps:  15% 307/2092 [05:17<30:12,  1.02s/it]\u001b[A\n","Steps:  15% 308/2092 [05:18<30:16,  1.02s/it]\u001b[A\n","Steps:  15% 309/2092 [05:19<30:22,  1.02s/it]\u001b[A\n","Steps:  15% 310/2092 [05:20<30:15,  1.02s/it]\u001b[A\n","Steps:  15% 311/2092 [05:21<30:14,  1.02s/it]\u001b[A\n","Steps:  15% 312/2092 [05:22<30:13,  1.02s/it]\u001b[A\n","Steps:  15% 313/2092 [05:23<30:08,  1.02s/it]\u001b[A\n","Steps:  15% 314/2092 [05:24<30:07,  1.02s/it]\u001b[A\n","Steps:  15% 315/2092 [05:25<30:02,  1.01s/it]\u001b[A\n","Steps:  15% 316/2092 [05:26<30:08,  1.02s/it]\u001b[A\n","Steps:  15% 317/2092 [05:27<30:08,  1.02s/it]\u001b[A\n","Steps:  15% 318/2092 [05:28<30:11,  1.02s/it]\u001b[A\n","Steps:  15% 319/2092 [05:29<30:04,  1.02s/it]\u001b[A\n","Steps:  15% 320/2092 [05:30<30:04,  1.02s/it]\u001b[A\n","Steps:  15% 321/2092 [05:31<29:59,  1.02s/it]\u001b[A\n","Steps:  15% 322/2092 [05:32<29:59,  1.02s/it]\u001b[A\n","Steps:  15% 323/2092 [05:33<30:04,  1.02s/it]\u001b[A\n","Steps:  15% 324/2092 [05:34<30:03,  1.02s/it]\u001b[A\n","Steps:  16% 325/2092 [05:35<29:57,  1.02s/it]\u001b[A\n","Steps:  16% 326/2092 [05:36<29:58,  1.02s/it]\u001b[A\n","Steps:  16% 327/2092 [05:37<30:03,  1.02s/it]\u001b[A\n","Steps:  16% 328/2092 [05:38<29:56,  1.02s/it]\u001b[A\n","Steps:  16% 329/2092 [05:39<29:50,  1.02s/it]\u001b[A\n","Steps:  16% 330/2092 [05:40<29:56,  1.02s/it]\u001b[A\n","Steps:  16% 331/2092 [05:41<29:54,  1.02s/it]\u001b[A\n","Steps:  16% 332/2092 [05:42<29:49,  1.02s/it]\u001b[A\n","Steps:  16% 333/2092 [05:43<29:50,  1.02s/it]\u001b[A\n","Steps:  16% 334/2092 [05:44<29:54,  1.02s/it]\u001b[A\n","Steps:  16% 335/2092 [05:45<29:47,  1.02s/it]\u001b[A\n","Steps:  16% 336/2092 [05:46<29:50,  1.02s/it]\u001b[A\n","Steps:  16% 337/2092 [05:47<29:52,  1.02s/it]\u001b[A\n","Steps:  16% 338/2092 [05:48<29:50,  1.02s/it]\u001b[A\n","Steps:  16% 339/2092 [05:49<29:49,  1.02s/it]\u001b[A\n","Steps:  16% 340/2092 [05:50<29:47,  1.02s/it]\u001b[A\n","Steps:  16% 341/2092 [05:51<29:46,  1.02s/it]\u001b[A\n","Steps:  16% 342/2092 [05:52<29:44,  1.02s/it]\u001b[A\n","Steps:  16% 343/2092 [05:53<29:43,  1.02s/it]\u001b[A\n","Steps:  16% 344/2092 [05:54<29:43,  1.02s/it]\u001b[A\n","Steps:  16% 345/2092 [05:55<29:36,  1.02s/it]\u001b[A\n","Steps:  17% 346/2092 [05:56<29:35,  1.02s/it]\u001b[A\n","Steps:  17% 347/2092 [05:57<29:34,  1.02s/it]\u001b[A\n","Steps:  17% 348/2092 [05:58<29:29,  1.01s/it]\u001b[A\n","Steps:  17% 349/2092 [05:59<29:27,  1.01s/it]\u001b[A\n","Steps:  17% 350/2092 [06:00<29:34,  1.02s/it]\u001b[A\n","Steps:  17% 351/2092 [06:01<29:33,  1.02s/it]\u001b[A\n","Steps:  17% 352/2092 [06:02<29:31,  1.02s/it]\u001b[A\n","Steps:  17% 353/2092 [06:03<29:24,  1.01s/it]\u001b[A\n","Steps:  17% 354/2092 [06:04<29:31,  1.02s/it]\u001b[A\n","Steps:  17% 355/2092 [06:05<29:31,  1.02s/it]\u001b[A\n","Steps:  17% 356/2092 [06:06<29:28,  1.02s/it]\u001b[A\n","Steps:  17% 357/2092 [06:07<29:27,  1.02s/it]\u001b[A\n","Steps:  17% 358/2092 [06:08<29:29,  1.02s/it]\u001b[A\n","Steps:  17% 359/2092 [06:10<29:26,  1.02s/it]\u001b[A\n","Steps:  17% 360/2092 [06:11<29:29,  1.02s/it]\u001b[A\n","Steps:  17% 361/2092 [06:12<29:22,  1.02s/it]\u001b[A\n","Steps:  17% 362/2092 [06:13<29:25,  1.02s/it]\u001b[A\n","Steps:  17% 363/2092 [06:14<29:23,  1.02s/it]\u001b[A\n","Steps:  17% 364/2092 [06:15<29:17,  1.02s/it]\u001b[A\n","Steps:  17% 365/2092 [06:16<29:15,  1.02s/it]\u001b[A\n","Steps:  17% 366/2092 [06:17<29:20,  1.02s/it]\u001b[A\n","Steps:  18% 367/2092 [06:18<29:21,  1.02s/it]\u001b[A\n","Steps:  18% 368/2092 [06:19<29:22,  1.02s/it]\u001b[A\n","Steps:  18% 369/2092 [06:20<29:19,  1.02s/it]\u001b[A\n","Steps:  18% 370/2092 [06:21<29:23,  1.02s/it]\u001b[A\n","Steps:  18% 371/2092 [06:22<29:25,  1.03s/it]\u001b[A\n","Steps:  18% 372/2092 [06:23<29:20,  1.02s/it]\u001b[A\n","Steps:  18% 373/2092 [06:24<29:20,  1.02s/it]\u001b[A\n","Steps:  18% 374/2092 [06:25<29:20,  1.02s/it]\u001b[A\n","Steps:  18% 375/2092 [06:26<29:20,  1.03s/it]\u001b[A\n","Steps:  18% 376/2092 [06:27<29:20,  1.03s/it]\u001b[A\n","Steps:  18% 377/2092 [06:28<29:21,  1.03s/it]\u001b[A\n","Steps:  18% 378/2092 [06:29<29:17,  1.03s/it]\u001b[A\n","Steps:  18% 379/2092 [06:30<29:17,  1.03s/it]\u001b[A\n","Steps:  18% 380/2092 [06:31<29:15,  1.03s/it]\u001b[A\n","Steps:  18% 381/2092 [06:32<29:17,  1.03s/it]\u001b[A\n","Steps:  18% 382/2092 [06:33<29:07,  1.02s/it]\u001b[A\n","Steps:  18% 383/2092 [06:34<29:10,  1.02s/it]\u001b[A\n","Steps:  18% 384/2092 [06:35<29:11,  1.03s/it]\u001b[A\n","Steps:  18% 385/2092 [06:36<29:07,  1.02s/it]\u001b[A\n","Steps:  18% 386/2092 [06:37<29:04,  1.02s/it]\u001b[A\n","Steps:  18% 387/2092 [06:38<29:07,  1.02s/it]\u001b[A\n","Steps:  19% 388/2092 [06:39<29:08,  1.03s/it]\u001b[A\n","Steps:  19% 389/2092 [06:40<28:58,  1.02s/it]\u001b[A\n","Steps:  19% 390/2092 [06:41<28:57,  1.02s/it]\u001b[A\n","Steps:  19% 391/2092 [06:42<28:54,  1.02s/it]\u001b[A\n","Steps:  19% 392/2092 [06:43<28:48,  1.02s/it]\u001b[A\n","Steps:  19% 393/2092 [06:44<28:53,  1.02s/it]\u001b[A\n","Steps:  19% 394/2092 [06:45<28:57,  1.02s/it]\u001b[A\n","Steps:  19% 395/2092 [06:46<28:54,  1.02s/it]\u001b[A\n","Steps:  19% 396/2092 [06:47<28:54,  1.02s/it]\u001b[A\n","Steps:  19% 397/2092 [06:48<28:56,  1.02s/it]\u001b[A\n","Steps:  19% 398/2092 [06:49<28:54,  1.02s/it]\u001b[A\n","Steps:  19% 399/2092 [06:50<28:50,  1.02s/it]\u001b[A\n","Steps:  19% 400/2092 [06:51<28:53,  1.02s/it]\u001b[A\n","Steps:  19% 401/2092 [06:52<28:43,  1.02s/it]\u001b[A\n","Steps:  19% 402/2092 [06:53<28:47,  1.02s/it]\u001b[A\n","Steps:  19% 403/2092 [06:54<28:41,  1.02s/it]\u001b[A\n","Steps:  19% 404/2092 [06:56<28:35,  1.02s/it]\u001b[A\n","Steps:  19% 405/2092 [06:57<28:34,  1.02s/it]\u001b[A\n","Steps:  19% 406/2092 [06:58<28:39,  1.02s/it]\u001b[A\n","Steps:  19% 407/2092 [06:59<28:38,  1.02s/it]\u001b[A\n","Steps:  20% 408/2092 [07:00<28:37,  1.02s/it]\u001b[A\n","Steps:  20% 409/2092 [07:01<28:35,  1.02s/it]\u001b[A\n","Steps:  20% 410/2092 [07:02<28:35,  1.02s/it]\u001b[A\n","Steps:  20% 411/2092 [07:03<28:33,  1.02s/it]\u001b[A\n","Steps:  20% 412/2092 [07:04<28:31,  1.02s/it]\u001b[A\n","Steps:  20% 413/2092 [07:05<28:34,  1.02s/it]\u001b[A\n","Steps:  20% 414/2092 [07:06<28:32,  1.02s/it]\u001b[A\n","Steps:  20% 415/2092 [07:07<28:35,  1.02s/it]\u001b[A\n","Steps:  20% 416/2092 [07:08<28:27,  1.02s/it]\u001b[A\n","Steps:  20% 417/2092 [07:09<28:25,  1.02s/it]\u001b[A\n","Steps:  20% 418/2092 [07:10<28:19,  1.01s/it]\u001b[A\n","Steps:  20% 419/2092 [07:11<28:24,  1.02s/it]\u001b[A\n","Steps:  20% 420/2092 [07:12<28:19,  1.02s/it]\u001b[A\n","Steps:  20% 421/2092 [07:13<28:15,  1.01s/it]\u001b[A\n","Steps:  20% 422/2092 [07:14<28:12,  1.01s/it]\u001b[A\n","Steps:  20% 423/2092 [07:15<28:18,  1.02s/it]\u001b[A\n","Steps:  20% 424/2092 [07:16<28:18,  1.02s/it]\u001b[A\n","Steps:  20% 425/2092 [07:17<28:18,  1.02s/it]\u001b[A\n","Steps:  20% 426/2092 [07:18<28:20,  1.02s/it]\u001b[A\n","Steps:  20% 427/2092 [07:19<28:23,  1.02s/it]\u001b[A\n","Steps:  20% 428/2092 [07:20<28:24,  1.02s/it]\u001b[A\n","Steps:  21% 429/2092 [07:21<28:22,  1.02s/it]\u001b[A\n","Steps:  21% 430/2092 [07:22<28:19,  1.02s/it]\u001b[A\n","Steps:  21% 431/2092 [07:23<28:15,  1.02s/it]\u001b[A\n","Steps:  21% 432/2092 [07:24<28:15,  1.02s/it]\u001b[A\n","Steps:  21% 433/2092 [07:25<28:15,  1.02s/it]\u001b[A\n","Steps:  21% 434/2092 [07:26<28:13,  1.02s/it]\u001b[A\n","Steps:  21% 435/2092 [07:27<28:13,  1.02s/it]\u001b[A\n","Steps:  21% 436/2092 [07:28<28:13,  1.02s/it]\u001b[A\n","Steps:  21% 437/2092 [07:29<28:15,  1.02s/it]\u001b[A\n","Steps:  21% 438/2092 [07:30<28:12,  1.02s/it]\u001b[A\n","Steps:  21% 439/2092 [07:31<28:08,  1.02s/it]\u001b[A\n","Steps:  21% 440/2092 [07:32<28:04,  1.02s/it]\u001b[A\n","Steps:  21% 441/2092 [07:33<28:07,  1.02s/it]\u001b[A\n","Steps:  21% 442/2092 [07:34<28:03,  1.02s/it]\u001b[A\n","Steps:  21% 443/2092 [07:35<28:05,  1.02s/it]\u001b[A\n","Steps:  21% 444/2092 [07:36<28:02,  1.02s/it]\u001b[A\n","Steps:  21% 445/2092 [07:37<27:56,  1.02s/it]\u001b[A\n","Steps:  21% 446/2092 [07:38<27:54,  1.02s/it]\u001b[A\n","Steps:  21% 447/2092 [07:39<27:51,  1.02s/it]\u001b[A\n","Steps:  21% 448/2092 [07:40<27:51,  1.02s/it]\u001b[A\n","Steps:  21% 449/2092 [07:41<27:48,  1.02s/it]\u001b[A\n","Steps:  22% 450/2092 [07:42<27:44,  1.01s/it]\u001b[A\n","Steps:  22% 451/2092 [07:43<27:46,  1.02s/it]\u001b[A\n","Steps:  22% 452/2092 [07:44<27:52,  1.02s/it]\u001b[A\n","Steps:  22% 453/2092 [07:45<27:46,  1.02s/it]\u001b[A\n","Steps:  22% 454/2092 [07:46<27:40,  1.01s/it]\u001b[A\n","Steps:  22% 455/2092 [07:47<27:37,  1.01s/it]\u001b[A\n","Steps:  22% 456/2092 [07:49<27:40,  1.01s/it]\u001b[A\n","Steps:  22% 457/2092 [07:50<27:46,  1.02s/it]\u001b[A\n","Steps:  22% 458/2092 [07:51<27:45,  1.02s/it]\u001b[A\n","Steps:  22% 459/2092 [07:52<27:41,  1.02s/it]\u001b[A\n","Steps:  22% 460/2092 [07:53<27:39,  1.02s/it]\u001b[A\n","Steps:  22% 461/2092 [07:54<27:44,  1.02s/it]\u001b[A\n","Steps:  22% 462/2092 [07:55<27:45,  1.02s/it]\u001b[A\n","Steps:  22% 463/2092 [07:56<27:48,  1.02s/it]\u001b[A\n","Steps:  22% 464/2092 [07:57<27:47,  1.02s/it]\u001b[A\n","Steps:  22% 465/2092 [07:58<27:48,  1.03s/it]\u001b[A\n","Steps:  22% 466/2092 [07:59<27:49,  1.03s/it]\u001b[A\n","Steps:  22% 467/2092 [08:00<27:49,  1.03s/it]\u001b[A\n","Steps:  22% 468/2092 [08:01<27:48,  1.03s/it]\u001b[A\n","Steps:  22% 469/2092 [08:02<27:39,  1.02s/it]\u001b[A\n","Steps:  22% 470/2092 [08:03<27:42,  1.02s/it]\u001b[A\n","Steps:  23% 471/2092 [08:04<27:33,  1.02s/it]\u001b[A\n","Steps:  23% 472/2092 [08:05<27:35,  1.02s/it]\u001b[A\n","Steps:  23% 473/2092 [08:06<27:34,  1.02s/it]\u001b[A\n","Steps:  23% 474/2092 [08:07<27:34,  1.02s/it]\u001b[A\n","Steps:  23% 475/2092 [08:08<27:28,  1.02s/it]\u001b[A\n","Steps:  23% 476/2092 [08:09<27:20,  1.01s/it]\u001b[A\n","Steps:  23% 477/2092 [08:10<27:24,  1.02s/it]\u001b[A\n","Steps:  23% 478/2092 [08:11<27:22,  1.02s/it]\u001b[A\n","Steps:  23% 479/2092 [08:12<27:23,  1.02s/it]\u001b[A\n","Steps:  23% 480/2092 [08:13<27:16,  1.02s/it]\u001b[A\n","Steps:  23% 481/2092 [08:14<27:21,  1.02s/it]\u001b[A\n","Steps:  23% 482/2092 [08:15<27:21,  1.02s/it]\u001b[A\n","Steps:  23% 483/2092 [08:16<27:19,  1.02s/it]\u001b[A\n","Steps:  23% 484/2092 [08:17<27:23,  1.02s/it]\u001b[A\n","Steps:  23% 485/2092 [08:18<27:26,  1.02s/it]\u001b[A\n","Steps:  23% 486/2092 [08:19<27:27,  1.03s/it]\u001b[A\n","Steps:  23% 487/2092 [08:20<27:26,  1.03s/it]\u001b[A\n","Steps:  23% 488/2092 [08:21<27:19,  1.02s/it]\u001b[A\n","Steps:  23% 489/2092 [08:22<27:19,  1.02s/it]\u001b[A\n","Steps:  23% 490/2092 [08:23<27:12,  1.02s/it]\u001b[A\n","Steps:  23% 491/2092 [08:24<27:12,  1.02s/it]\u001b[A\n","Steps:  24% 492/2092 [08:25<27:14,  1.02s/it]\u001b[A\n","Steps:  24% 493/2092 [08:26<27:12,  1.02s/it]\u001b[A\n","Steps:  24% 494/2092 [08:27<27:10,  1.02s/it]\u001b[A\n","Steps:  24% 495/2092 [08:28<27:08,  1.02s/it]\u001b[A\n","Steps:  24% 496/2092 [08:29<27:01,  1.02s/it]\u001b[A\n","Steps:  24% 497/2092 [08:30<27:00,  1.02s/it]\u001b[A\n","Steps:  24% 498/2092 [08:31<27:01,  1.02s/it]\u001b[A\n","Steps:  24% 499/2092 [08:32<27:01,  1.02s/it]\u001b[A\n","Steps:  24% 500/2092 [08:33<27:01,  1.02s/it]\u001b[A\n","Steps:  24% 501/2092 [08:34<27:02,  1.02s/it]\u001b[A\n","Steps:  24% 502/2092 [08:35<26:59,  1.02s/it]\u001b[A\n","Steps:  24% 503/2092 [08:36<27:02,  1.02s/it]\u001b[A\n","Steps:  24% 504/2092 [08:38<27:05,  1.02s/it]\u001b[A\n","Steps:  24% 505/2092 [08:39<26:54,  1.02s/it]\u001b[A\n","Steps:  24% 506/2092 [08:40<26:58,  1.02s/it]\u001b[A\n","Steps:  24% 507/2092 [08:41<26:57,  1.02s/it]\u001b[A\n","Steps:  24% 508/2092 [08:42<27:00,  1.02s/it]\u001b[A\n","Steps:  24% 509/2092 [08:43<27:00,  1.02s/it]\u001b[A\n","Steps:  24% 510/2092 [08:44<26:58,  1.02s/it]\u001b[A\n","Steps:  24% 511/2092 [08:45<26:50,  1.02s/it]\u001b[A\n","Steps:  24% 512/2092 [08:46<26:50,  1.02s/it]\u001b[A\n","Steps:  25% 513/2092 [08:47<26:50,  1.02s/it]\u001b[A\n","Steps:  25% 514/2092 [08:48<26:45,  1.02s/it]\u001b[A\n","Steps:  25% 515/2092 [08:49<26:46,  1.02s/it]\u001b[A\n","Steps:  25% 516/2092 [08:50<26:50,  1.02s/it]\u001b[A\n","Steps:  25% 517/2092 [08:51<26:50,  1.02s/it]\u001b[A\n","Steps:  25% 518/2092 [08:52<26:52,  1.02s/it]\u001b[A\n","Steps:  25% 519/2092 [08:53<26:44,  1.02s/it]\u001b[A\n","Steps:  25% 520/2092 [08:54<26:45,  1.02s/it]\u001b[A\n","Steps:  25% 521/2092 [08:55<26:48,  1.02s/it]\u001b[A\n","Steps:  25% 522/2092 [08:56<26:49,  1.03s/it]\u001b[A\n","Steps:  25% 523/2092 [08:57<26:46,  1.02s/it]\u001b[A\n","Steps:  25% 524/2092 [08:58<26:39,  1.02s/it]\u001b[A\n","Steps:  25% 525/2092 [08:59<26:36,  1.02s/it]\u001b[A\n","Steps:  25% 526/2092 [09:00<26:31,  1.02s/it]\u001b[A\n","Steps:  25% 527/2092 [09:01<26:27,  1.01s/it]\u001b[A\n","Steps:  25% 528/2092 [09:02<26:24,  1.01s/it]\u001b[A\n","Steps:  25% 529/2092 [09:03<26:30,  1.02s/it]\u001b[A\n","Steps:  25% 530/2092 [09:04<26:35,  1.02s/it]\u001b[A\n","Steps:  25% 531/2092 [09:05<26:29,  1.02s/it]\u001b[A\n","Steps:  25% 532/2092 [09:06<26:31,  1.02s/it]\u001b[A\n","Steps:  25% 533/2092 [09:07<26:26,  1.02s/it]\u001b[A\n","Steps:  26% 534/2092 [09:08<26:27,  1.02s/it]\u001b[A\n","Steps:  26% 535/2092 [09:09<26:22,  1.02s/it]\u001b[A\n","Steps:  26% 536/2092 [09:10<26:25,  1.02s/it]\u001b[A\n","Steps:  26% 537/2092 [09:11<26:26,  1.02s/it]\u001b[A\n","Steps:  26% 538/2092 [09:12<26:29,  1.02s/it]\u001b[A\n","Steps:  26% 539/2092 [09:13<26:21,  1.02s/it]\u001b[A\n","Steps:  26% 540/2092 [09:14<26:25,  1.02s/it]\u001b[A\n","Steps:  26% 541/2092 [09:15<26:19,  1.02s/it]\u001b[A\n","Steps:  26% 542/2092 [09:16<26:18,  1.02s/it]\u001b[A\n","Steps:  26% 543/2092 [09:17<26:20,  1.02s/it]\u001b[A\n","Steps:  26% 544/2092 [09:18<26:21,  1.02s/it]\u001b[A\n","Steps:  26% 545/2092 [09:19<26:14,  1.02s/it]\u001b[A\n","Steps:  26% 546/2092 [09:20<26:16,  1.02s/it]\u001b[A\n","Steps:  26% 547/2092 [09:21<26:17,  1.02s/it]\u001b[A\n","Steps:  26% 548/2092 [09:22<26:11,  1.02s/it]\u001b[A\n","Steps:  26% 549/2092 [09:23<26:07,  1.02s/it]\u001b[A\n","Steps:  26% 550/2092 [09:24<26:07,  1.02s/it]\u001b[A\n","Steps:  26% 551/2092 [09:25<26:12,  1.02s/it]\u001b[A\n","Steps:  26% 552/2092 [09:26<26:12,  1.02s/it]\u001b[A\n","Steps:  26% 553/2092 [09:27<26:10,  1.02s/it]\u001b[A\n","Steps:  26% 554/2092 [09:29<26:09,  1.02s/it]\u001b[A\n","Steps:  27% 555/2092 [09:30<26:11,  1.02s/it]\u001b[A\n","Steps:  27% 556/2092 [09:31<26:13,  1.02s/it]\u001b[A\n","Steps:  27% 557/2092 [09:32<26:14,  1.03s/it]\u001b[A\n","Steps:  27% 558/2092 [09:33<26:06,  1.02s/it]\u001b[A\n","Steps:  27% 559/2092 [09:34<26:08,  1.02s/it]\u001b[A\n","Steps:  27% 560/2092 [09:35<26:05,  1.02s/it]\u001b[A\n","Steps:  27% 561/2092 [09:36<26:02,  1.02s/it]\u001b[A\n","Steps:  27% 562/2092 [09:37<26:00,  1.02s/it]\u001b[A\n","Steps:  27% 563/2092 [09:38<25:58,  1.02s/it]\u001b[A\n","Steps:  27% 564/2092 [09:39<25:59,  1.02s/it]\u001b[A\n","Steps:  27% 565/2092 [09:40<25:58,  1.02s/it]\u001b[A\n","Steps:  27% 566/2092 [09:41<25:57,  1.02s/it]\u001b[A\n","Steps:  27% 567/2092 [09:42<25:55,  1.02s/it]\u001b[A\n","Steps:  27% 568/2092 [09:43<25:55,  1.02s/it]\u001b[A\n","Steps:  27% 569/2092 [09:44<25:52,  1.02s/it]\u001b[A\n","Steps:  27% 570/2092 [09:45<25:50,  1.02s/it]\u001b[A\n","Steps:  27% 571/2092 [09:46<25:50,  1.02s/it]\u001b[A\n","Steps:  27% 572/2092 [09:47<25:50,  1.02s/it]\u001b[A\n","Steps:  27% 573/2092 [09:48<25:49,  1.02s/it]\u001b[A\n","Steps:  27% 574/2092 [09:49<25:49,  1.02s/it]\u001b[A\n","Steps:  27% 575/2092 [09:50<25:48,  1.02s/it]\u001b[A\n","Steps:  28% 576/2092 [09:51<25:47,  1.02s/it]\u001b[A\n","Steps:  28% 577/2092 [09:52<25:46,  1.02s/it]\u001b[A\n","Steps:  28% 578/2092 [09:53<25:38,  1.02s/it]\u001b[A\n","Steps:  28% 579/2092 [09:54<25:38,  1.02s/it]\u001b[A\n","Steps:  28% 580/2092 [09:55<25:37,  1.02s/it]\u001b[A\n","Steps:  28% 581/2092 [09:56<25:29,  1.01s/it]\u001b[A\n","Steps:  28% 582/2092 [09:57<25:36,  1.02s/it]\u001b[A\n","Steps:  28% 583/2092 [09:58<25:38,  1.02s/it]\u001b[A\n","Steps:  28% 584/2092 [09:59<25:40,  1.02s/it]\u001b[A\n","Steps:  28% 585/2092 [10:00<25:39,  1.02s/it]\u001b[A\n","Steps:  28% 586/2092 [10:01<25:39,  1.02s/it]\u001b[A\n","Steps:  28% 587/2092 [10:02<25:40,  1.02s/it]\u001b[A\n","Steps:  28% 588/2092 [10:03<25:34,  1.02s/it]\u001b[A\n","Steps:  28% 589/2092 [10:04<25:33,  1.02s/it]\u001b[A\n","Steps:  28% 590/2092 [10:05<25:24,  1.01s/it]\u001b[A\n","Steps:  28% 591/2092 [10:06<25:28,  1.02s/it]\u001b[A\n","Steps:  28% 592/2092 [10:07<25:24,  1.02s/it]\u001b[A\n","Steps:  28% 593/2092 [10:08<25:29,  1.02s/it]\u001b[A\n","Steps:  28% 594/2092 [10:09<25:28,  1.02s/it]\u001b[A\n","Steps:  28% 595/2092 [10:10<25:31,  1.02s/it]\u001b[A\n","Steps:  28% 596/2092 [10:11<25:36,  1.03s/it]\u001b[A\n","Steps:  29% 597/2092 [10:12<25:30,  1.02s/it]\u001b[A\n","Steps:  29% 598/2092 [10:13<25:32,  1.03s/it]\u001b[A\n","Steps:  29% 599/2092 [10:14<25:31,  1.03s/it]\u001b[A\n","Steps:  29% 600/2092 [10:15<25:26,  1.02s/it]\u001b[A\n","Steps:  29% 601/2092 [10:16<25:26,  1.02s/it]\u001b[A\n","Steps:  29% 602/2092 [10:18<25:24,  1.02s/it]\u001b[A\n","Steps:  29% 603/2092 [10:19<25:21,  1.02s/it]\u001b[A\n","Steps:  29% 604/2092 [10:20<25:18,  1.02s/it]\u001b[A\n","Steps:  29% 605/2092 [10:21<25:17,  1.02s/it]\u001b[A\n","Steps:  29% 606/2092 [10:22<25:18,  1.02s/it]\u001b[A\n","Steps:  29% 607/2092 [10:23<25:21,  1.02s/it]\u001b[A\n","Steps:  29% 608/2092 [10:24<25:20,  1.02s/it]\u001b[A\n","Steps:  29% 609/2092 [10:25<25:17,  1.02s/it]\u001b[A\n","Steps:  29% 610/2092 [10:26<25:19,  1.02s/it]\u001b[A\n","Steps:  29% 611/2092 [10:27<25:19,  1.03s/it]\u001b[A\n","Steps:  29% 612/2092 [10:28<25:16,  1.02s/it]\u001b[A\n","Steps:  29% 613/2092 [10:29<25:14,  1.02s/it]\u001b[A\n","Steps:  29% 614/2092 [10:30<25:16,  1.03s/it]\u001b[A\n","Steps:  29% 615/2092 [10:31<25:07,  1.02s/it]\u001b[A\n","Steps:  29% 616/2092 [10:32<25:09,  1.02s/it]\u001b[A\n","Steps:  29% 617/2092 [10:33<25:09,  1.02s/it]\u001b[A\n","Steps:  30% 618/2092 [10:34<25:10,  1.02s/it]\u001b[A\n","Steps:  30% 619/2092 [10:35<25:04,  1.02s/it]\u001b[A\n","Steps:  30% 620/2092 [10:36<24:58,  1.02s/it]\u001b[A\n","Steps:  30% 621/2092 [10:37<24:57,  1.02s/it]\u001b[A\n","Steps:  30% 622/2092 [10:38<24:52,  1.02s/it]\u001b[A\n","Steps:  30% 623/2092 [10:39<24:52,  1.02s/it]\u001b[A\n","Steps:  30% 624/2092 [10:40<24:52,  1.02s/it]\u001b[A\n","Steps:  30% 625/2092 [10:41<24:57,  1.02s/it]\u001b[A\n","Steps:  30% 626/2092 [10:42<24:59,  1.02s/it]\u001b[A\n","Steps:  30% 627/2092 [10:43<24:53,  1.02s/it]\u001b[A\n","Steps:  30% 628/2092 [10:44<24:56,  1.02s/it]\u001b[A\n","Steps:  30% 629/2092 [10:45<24:54,  1.02s/it]\u001b[A\n","Steps:  30% 630/2092 [10:46<24:55,  1.02s/it]\u001b[A\n","Steps:  30% 631/2092 [10:47<24:56,  1.02s/it]\u001b[A\n","Steps:  30% 632/2092 [10:48<24:48,  1.02s/it]\u001b[A\n","Steps:  30% 633/2092 [10:49<24:50,  1.02s/it]\u001b[A\n","Steps:  30% 634/2092 [10:50<24:51,  1.02s/it]\u001b[A\n","Steps:  30% 635/2092 [10:51<24:49,  1.02s/it]\u001b[A\n","Steps:  30% 636/2092 [10:52<24:46,  1.02s/it]\u001b[A\n","Steps:  30% 637/2092 [10:53<24:48,  1.02s/it]\u001b[A\n","Steps:  30% 638/2092 [10:54<24:48,  1.02s/it]\u001b[A\n","Steps:  31% 639/2092 [10:55<24:46,  1.02s/it]\u001b[A\n","Steps:  31% 640/2092 [10:56<24:40,  1.02s/it]\u001b[A\n","Steps:  31% 641/2092 [10:57<24:40,  1.02s/it]\u001b[A\n","Steps:  31% 642/2092 [10:58<24:38,  1.02s/it]\u001b[A\n","Steps:  31% 643/2092 [10:59<24:32,  1.02s/it]\u001b[A\n","Steps:  31% 644/2092 [11:00<24:37,  1.02s/it]\u001b[A\n","Steps:  31% 645/2092 [11:01<24:39,  1.02s/it]\u001b[A\n","Steps:  31% 646/2092 [11:02<24:40,  1.02s/it]\u001b[A\n","Steps:  31% 647/2092 [11:03<24:37,  1.02s/it]\u001b[A\n","Steps:  31% 648/2092 [11:04<24:30,  1.02s/it]\u001b[A\n","Steps:  31% 649/2092 [11:06<24:26,  1.02s/it]\u001b[A\n","Steps:  31% 650/2092 [11:07<24:29,  1.02s/it]\u001b[A\n","Steps:  31% 651/2092 [11:08<24:33,  1.02s/it]\u001b[A\n","Steps:  31% 652/2092 [11:09<24:34,  1.02s/it]\u001b[A\n","Steps:  31% 653/2092 [11:10<24:27,  1.02s/it]\u001b[A\n","Steps:  31% 654/2092 [11:11<24:27,  1.02s/it]\u001b[A\n","Steps:  31% 655/2092 [11:12<24:26,  1.02s/it]\u001b[A\n","Steps:  31% 656/2092 [11:13<24:27,  1.02s/it]\u001b[A\n","Steps:  31% 657/2092 [11:14<24:26,  1.02s/it]\u001b[A\n","Steps:  31% 658/2092 [11:15<24:23,  1.02s/it]\u001b[A\n","Steps:  32% 659/2092 [11:16<24:21,  1.02s/it]\u001b[A\n","Steps:  32% 660/2092 [11:17<24:22,  1.02s/it]\u001b[A\n","Steps:  32% 661/2092 [11:18<24:20,  1.02s/it]\u001b[A\n","Steps:  32% 662/2092 [11:19<24:15,  1.02s/it]\u001b[A\n","Steps:  32% 663/2092 [11:20<24:14,  1.02s/it]\u001b[A\n","Steps:  32% 664/2092 [11:21<24:17,  1.02s/it]\u001b[A\n","Steps:  32% 665/2092 [11:22<24:15,  1.02s/it]\u001b[A\n","Steps:  32% 666/2092 [11:23<24:16,  1.02s/it]\u001b[A\n","Steps:  32% 667/2092 [11:24<24:18,  1.02s/it]\u001b[A\n","Steps:  32% 668/2092 [11:25<24:20,  1.03s/it]\u001b[A\n","Steps:  32% 669/2092 [11:26<24:16,  1.02s/it]\u001b[A\n","Steps:  32% 670/2092 [11:27<24:12,  1.02s/it]\u001b[A\n","Steps:  32% 671/2092 [11:28<24:12,  1.02s/it]\u001b[A\n","Steps:  32% 672/2092 [11:29<24:06,  1.02s/it]\u001b[A\n","Steps:  32% 673/2092 [11:30<24:09,  1.02s/it]\u001b[A\n","Steps:  32% 674/2092 [11:31<24:06,  1.02s/it]\u001b[A\n","Steps:  32% 675/2092 [11:32<24:05,  1.02s/it]\u001b[A\n","Steps:  32% 676/2092 [11:33<24:07,  1.02s/it]\u001b[A\n","Steps:  32% 677/2092 [11:34<24:01,  1.02s/it]\u001b[A\n","Steps:  32% 678/2092 [11:35<23:59,  1.02s/it]\u001b[A\n","Steps:  32% 679/2092 [11:36<24:02,  1.02s/it]\u001b[A\n","Steps:  33% 680/2092 [11:37<23:58,  1.02s/it]\u001b[A\n","Steps:  33% 681/2092 [11:38<23:57,  1.02s/it]\u001b[A\n","Steps:  33% 682/2092 [11:39<23:52,  1.02s/it]\u001b[A\n","Steps:  33% 683/2092 [11:40<23:52,  1.02s/it]\u001b[A\n","Steps:  33% 684/2092 [11:41<23:54,  1.02s/it]\u001b[A\n","Steps:  33% 685/2092 [11:42<23:53,  1.02s/it]\u001b[A\n","Steps:  33% 686/2092 [11:43<23:52,  1.02s/it]\u001b[A\n","Steps:  33% 687/2092 [11:44<23:51,  1.02s/it]\u001b[A\n","Steps:  33% 688/2092 [11:45<23:49,  1.02s/it]\u001b[A\n","Steps:  33% 689/2092 [11:46<23:45,  1.02s/it]\u001b[A\n","Steps:  33% 690/2092 [11:47<23:49,  1.02s/it]\u001b[A\n","Steps:  33% 691/2092 [11:48<23:52,  1.02s/it]\u001b[A\n","Steps:  33% 692/2092 [11:49<23:51,  1.02s/it]\u001b[A\n","Steps:  33% 693/2092 [11:50<23:45,  1.02s/it]\u001b[A\n","Steps:  33% 694/2092 [11:51<23:48,  1.02s/it]\u001b[A\n","Steps:  33% 695/2092 [11:52<23:49,  1.02s/it]\u001b[A\n","Steps:  33% 696/2092 [11:53<23:50,  1.02s/it]\u001b[A\n","Steps:  33% 697/2092 [11:55<23:47,  1.02s/it]\u001b[A\n","Steps:  33% 698/2092 [11:56<23:45,  1.02s/it]\u001b[A\n","Steps:  33% 699/2092 [11:57<23:43,  1.02s/it]\u001b[A\n","Steps:  33% 700/2092 [11:58<23:43,  1.02s/it]\u001b[A\n","Steps:  34% 701/2092 [11:59<23:44,  1.02s/it]\u001b[A\n","Steps:  34% 702/2092 [12:00<23:37,  1.02s/it]\u001b[A\n","Steps:  34% 703/2092 [12:01<23:35,  1.02s/it]\u001b[A\n","Steps:  34% 704/2092 [12:02<23:38,  1.02s/it]\u001b[A\n","Steps:  34% 705/2092 [12:03<23:35,  1.02s/it]\u001b[A\n","Steps:  34% 706/2092 [12:04<23:27,  1.02s/it]\u001b[A\n","Steps:  34% 707/2092 [12:05<23:30,  1.02s/it]\u001b[A\n","Steps:  34% 708/2092 [12:06<23:33,  1.02s/it]\u001b[A\n","Steps:  34% 709/2092 [12:07<23:36,  1.02s/it]\u001b[A\n","Steps:  34% 710/2092 [12:08<23:36,  1.03s/it]\u001b[A\n","Steps:  34% 711/2092 [12:09<23:29,  1.02s/it]\u001b[A\n","Steps:  34% 712/2092 [12:10<23:27,  1.02s/it]\u001b[A\n","Steps:  34% 713/2092 [12:11<23:26,  1.02s/it]\u001b[A\n","Steps:  34% 714/2092 [12:12<23:22,  1.02s/it]\u001b[A\n","Steps:  34% 715/2092 [12:13<23:21,  1.02s/it]\u001b[A\n","Steps:  34% 716/2092 [12:14<23:15,  1.01s/it]\u001b[A\n","Steps:  34% 717/2092 [12:15<23:12,  1.01s/it]\u001b[A\n","Steps:  34% 718/2092 [12:16<23:17,  1.02s/it]\u001b[A\n","Steps:  34% 719/2092 [12:17<23:13,  1.01s/it]\u001b[A\n","Steps:  34% 720/2092 [12:18<23:14,  1.02s/it]\u001b[A\n","Steps:  34% 721/2092 [12:19<23:11,  1.02s/it]\u001b[A\n","Steps:  35% 722/2092 [12:20<23:16,  1.02s/it]\u001b[A\n","Steps:  35% 723/2092 [12:21<23:15,  1.02s/it]\u001b[A\n","Steps:  35% 724/2092 [12:22<23:13,  1.02s/it]\u001b[A\n","Steps:  35% 725/2092 [12:23<23:16,  1.02s/it]\u001b[A\n","Steps:  35% 726/2092 [12:24<23:15,  1.02s/it]\u001b[A\n","Steps:  35% 727/2092 [12:25<23:17,  1.02s/it]\u001b[A\n","Steps:  35% 728/2092 [12:26<23:15,  1.02s/it]\u001b[A\n","Steps:  35% 729/2092 [12:27<23:13,  1.02s/it]\u001b[A\n","Steps:  35% 730/2092 [12:28<23:15,  1.02s/it]\u001b[A\n","Steps:  35% 731/2092 [12:29<23:16,  1.03s/it]\u001b[A\n","Steps:  35% 732/2092 [12:30<23:08,  1.02s/it]\u001b[A\n","Steps:  35% 733/2092 [12:31<23:11,  1.02s/it]\u001b[A\n","Steps:  35% 734/2092 [12:32<23:11,  1.02s/it]\u001b[A\n","Steps:  35% 735/2092 [12:33<23:12,  1.03s/it]\u001b[A\n","Steps:  35% 736/2092 [12:34<23:10,  1.03s/it]\u001b[A\n","Steps:  35% 737/2092 [12:35<23:03,  1.02s/it]\u001b[A\n","Steps:  35% 738/2092 [12:36<23:01,  1.02s/it]\u001b[A\n","Steps:  35% 739/2092 [12:37<22:59,  1.02s/it]\u001b[A\n","Steps:  35% 740/2092 [12:38<22:53,  1.02s/it]\u001b[A\n","Steps:  35% 741/2092 [12:39<22:50,  1.01s/it]\u001b[A\n","Steps:  35% 742/2092 [12:40<22:55,  1.02s/it]\u001b[A\n","Steps:  36% 743/2092 [12:41<22:53,  1.02s/it]\u001b[A\n","Steps:  36% 744/2092 [12:42<22:52,  1.02s/it]\u001b[A\n","Steps:  36% 745/2092 [12:43<22:49,  1.02s/it]\u001b[A\n","Steps:  36% 746/2092 [12:44<22:52,  1.02s/it]\u001b[A\n","Steps:  36% 747/2092 [12:46<22:51,  1.02s/it]\u001b[A\n","Steps:  36% 748/2092 [12:47<22:54,  1.02s/it]\u001b[A\n","Steps:  36% 749/2092 [12:48<22:56,  1.02s/it]\u001b[A\n","Steps:  36% 750/2092 [12:49<22:52,  1.02s/it]\u001b[A\n","Steps:  36% 751/2092 [12:50<22:46,  1.02s/it]\u001b[A\n","Steps:  36% 752/2092 [12:51<22:40,  1.02s/it]\u001b[A\n","Steps:  36% 753/2092 [12:52<22:44,  1.02s/it]\u001b[A\n","Steps:  36% 754/2092 [12:53<22:43,  1.02s/it]\u001b[A\n","Steps:  36% 755/2092 [12:54<22:39,  1.02s/it]\u001b[A\n","Steps:  36% 756/2092 [12:55<22:35,  1.01s/it]\u001b[A\n","Steps:  36% 757/2092 [12:56<22:39,  1.02s/it]\u001b[A\n","Steps:  36% 758/2092 [12:57<22:43,  1.02s/it]\u001b[A\n","Steps:  36% 759/2092 [12:58<22:35,  1.02s/it]\u001b[A\n","Steps:  36% 760/2092 [12:59<22:34,  1.02s/it]\u001b[A\n","Steps:  36% 761/2092 [13:00<22:34,  1.02s/it]\u001b[A\n","Steps:  36% 762/2092 [13:01<22:38,  1.02s/it]\u001b[A\n","Steps:  36% 763/2092 [13:02<22:37,  1.02s/it]\u001b[A\n","Steps:  37% 764/2092 [13:03<22:39,  1.02s/it]\u001b[A\n","Steps:  37% 765/2092 [13:04<22:40,  1.03s/it]\u001b[A\n","Steps:  37% 766/2092 [13:05<22:39,  1.03s/it]\u001b[A\n","Steps:  37% 767/2092 [13:06<22:35,  1.02s/it]\u001b[A\n","Steps:  37% 768/2092 [13:07<22:37,  1.03s/it]\u001b[A\n","Steps:  37% 769/2092 [13:08<22:33,  1.02s/it]\u001b[A\n","Steps:  37% 770/2092 [13:09<22:25,  1.02s/it]\u001b[A\n","Steps:  37% 771/2092 [13:10<22:27,  1.02s/it]\u001b[A\n","Steps:  37% 772/2092 [13:11<22:28,  1.02s/it]\u001b[A\n","Steps:  37% 773/2092 [13:12<22:27,  1.02s/it]\u001b[A\n","Steps:  37% 774/2092 [13:13<22:24,  1.02s/it]\u001b[A\n","Steps:  37% 775/2092 [13:14<22:27,  1.02s/it]\u001b[A\n","Steps:  37% 776/2092 [13:15<22:26,  1.02s/it]\u001b[A\n","Steps:  37% 777/2092 [13:16<22:27,  1.02s/it]\u001b[A\n","Steps:  37% 778/2092 [13:17<22:21,  1.02s/it]\u001b[A\n","Steps:  37% 779/2092 [13:18<22:19,  1.02s/it]\u001b[A\n","Steps:  37% 780/2092 [13:19<22:17,  1.02s/it]\u001b[A\n","Steps:  37% 781/2092 [13:20<22:20,  1.02s/it]\u001b[A\n","Steps:  37% 782/2092 [13:21<22:13,  1.02s/it]\u001b[A\n","Steps:  37% 783/2092 [13:22<22:17,  1.02s/it]\u001b[A\n","Steps:  37% 784/2092 [13:23<22:11,  1.02s/it]\u001b[A\n","Steps:  38% 785/2092 [13:24<22:06,  1.01s/it]\u001b[A\n","Steps:  38% 786/2092 [13:25<22:10,  1.02s/it]\u001b[A\n","Steps:  38% 787/2092 [13:26<22:09,  1.02s/it]\u001b[A\n","Steps:  38% 788/2092 [13:27<22:08,  1.02s/it]\u001b[A\n","Steps:  38% 789/2092 [13:28<22:10,  1.02s/it]\u001b[A\n","Steps:  38% 790/2092 [13:29<22:07,  1.02s/it]\u001b[A\n","Steps:  38% 791/2092 [13:30<22:05,  1.02s/it]\u001b[A\n","Steps:  38% 792/2092 [13:31<22:08,  1.02s/it]\u001b[A\n","Steps:  38% 793/2092 [13:32<22:06,  1.02s/it]\u001b[A\n","Steps:  38% 794/2092 [13:33<22:06,  1.02s/it]\u001b[A\n","Steps:  38% 795/2092 [13:34<22:06,  1.02s/it]\u001b[A\n","Steps:  38% 796/2092 [13:36<22:06,  1.02s/it]\u001b[A\n","Steps:  38% 797/2092 [13:37<22:00,  1.02s/it]\u001b[A\n","Steps:  38% 798/2092 [13:38<21:55,  1.02s/it]\u001b[A\n","Steps:  38% 799/2092 [13:39<21:52,  1.02s/it]\u001b[A\n","Steps:  38% 800/2092 [13:40<21:52,  1.02s/it]\u001b[A\n","Steps:  38% 801/2092 [13:41<21:48,  1.01s/it]\u001b[A\n","Steps:  38% 802/2092 [13:42<21:49,  1.02s/it]\u001b[A\n","Steps:  38% 803/2092 [13:43<21:54,  1.02s/it]\u001b[A\n","Steps:  38% 804/2092 [13:44<21:49,  1.02s/it]\u001b[A\n","Steps:  38% 805/2092 [13:45<21:46,  1.02s/it]\u001b[A\n","Steps:  39% 806/2092 [13:46<21:41,  1.01s/it]\u001b[A\n","Steps:  39% 807/2092 [13:47<21:47,  1.02s/it]\u001b[A\n","Steps:  39% 808/2092 [13:48<21:43,  1.02s/it]\u001b[A\n","Steps:  39% 809/2092 [13:49<21:39,  1.01s/it]\u001b[A\n","Steps:  39% 810/2092 [13:50<21:44,  1.02s/it]\u001b[A\n","Steps:  39% 811/2092 [13:51<21:44,  1.02s/it]\u001b[A\n","Steps:  39% 812/2092 [13:52<21:47,  1.02s/it]\u001b[A\n","Steps:  39% 813/2092 [13:53<21:44,  1.02s/it]\u001b[A\n","Steps:  39% 814/2092 [13:54<21:42,  1.02s/it]\u001b[A\n","Steps:  39% 815/2092 [13:55<21:45,  1.02s/it]\u001b[A\n","Steps:  39% 816/2092 [13:56<21:46,  1.02s/it]\u001b[A\n","Steps:  39% 817/2092 [13:57<21:47,  1.03s/it]\u001b[A\n","Steps:  39% 818/2092 [13:58<21:48,  1.03s/it]\u001b[A\n","Steps:  39% 819/2092 [13:59<21:48,  1.03s/it]\u001b[A\n","Steps:  39% 820/2092 [14:00<21:44,  1.03s/it]\u001b[A\n","Steps:  39% 821/2092 [14:01<21:43,  1.03s/it]\u001b[A\n","Steps:  39% 822/2092 [14:02<21:39,  1.02s/it]\u001b[A\n","Steps:  39% 823/2092 [14:03<21:33,  1.02s/it]\u001b[A\n","Steps:  39% 824/2092 [14:04<21:33,  1.02s/it]\u001b[A\n","Steps:  39% 825/2092 [14:05<21:34,  1.02s/it]\u001b[A\n","Steps:  39% 826/2092 [14:06<21:33,  1.02s/it]\u001b[A\n","Steps:  40% 827/2092 [14:07<21:32,  1.02s/it]\u001b[A\n","Steps:  40% 828/2092 [14:08<21:34,  1.02s/it]\u001b[A\n","Steps:  40% 829/2092 [14:09<21:32,  1.02s/it]\u001b[A\n","Steps:  40% 830/2092 [14:10<21:27,  1.02s/it]\u001b[A\n","Steps:  40% 831/2092 [14:11<21:26,  1.02s/it]\u001b[A\n","Steps:  40% 832/2092 [14:12<21:26,  1.02s/it]\u001b[A\n","Steps:  40% 833/2092 [14:13<21:25,  1.02s/it]\u001b[A\n","Steps:  40% 834/2092 [14:14<21:24,  1.02s/it]\u001b[A\n","Steps:  40% 835/2092 [14:15<21:18,  1.02s/it]\u001b[A\n","Steps:  40% 836/2092 [14:16<21:17,  1.02s/it]\u001b[A\n","Steps:  40% 837/2092 [14:17<21:17,  1.02s/it]\u001b[A\n","Steps:  40% 838/2092 [14:18<21:16,  1.02s/it]\u001b[A\n","Steps:  40% 839/2092 [14:19<21:18,  1.02s/it]\u001b[A\n","Steps:  40% 840/2092 [14:20<21:15,  1.02s/it]\u001b[A\n","Steps:  40% 841/2092 [14:21<21:19,  1.02s/it]\u001b[A\n","Steps:  40% 842/2092 [14:22<21:17,  1.02s/it]\u001b[A\n","Steps:  40% 843/2092 [14:23<21:19,  1.02s/it]\u001b[A\n","Steps:  40% 844/2092 [14:24<21:15,  1.02s/it]\u001b[A\n","Steps:  40% 845/2092 [14:25<21:14,  1.02s/it]\u001b[A\n","Steps:  40% 846/2092 [14:27<21:08,  1.02s/it]\u001b[A\n","Steps:  40% 847/2092 [14:28<21:06,  1.02s/it]\u001b[A\n","Steps:  41% 848/2092 [14:29<21:06,  1.02s/it]\u001b[A\n","Steps:  41% 849/2092 [14:30<21:10,  1.02s/it]\u001b[A\n","Steps:  41% 850/2092 [14:31<21:11,  1.02s/it]\u001b[A\n","Steps:  41% 851/2092 [14:32<21:06,  1.02s/it]\u001b[A\n","Steps:  41% 852/2092 [14:33<21:09,  1.02s/it]\u001b[A\n","Steps:  41% 853/2092 [14:34<21:07,  1.02s/it]\u001b[A\n","Steps:  41% 854/2092 [14:35<21:07,  1.02s/it]\u001b[A\n","Steps:  41% 855/2092 [14:36<21:01,  1.02s/it]\u001b[A\n","Steps:  41% 856/2092 [14:37<21:04,  1.02s/it]\u001b[A\n","Steps:  41% 857/2092 [14:38<20:55,  1.02s/it]\u001b[A\n","Steps:  41% 858/2092 [14:39<20:57,  1.02s/it]\u001b[A\n","Steps:  41% 859/2092 [14:40<20:56,  1.02s/it]\u001b[A\n","Steps:  41% 860/2092 [14:41<20:58,  1.02s/it]\u001b[A\n","Steps:  41% 861/2092 [14:42<20:53,  1.02s/it]\u001b[A\n","Steps:  41% 862/2092 [14:43<20:49,  1.02s/it]\u001b[A\n","Steps:  41% 863/2092 [14:44<20:49,  1.02s/it]\u001b[A\n","Steps:  41% 864/2092 [14:45<20:51,  1.02s/it]\u001b[A\n","Steps:  41% 865/2092 [14:46<20:50,  1.02s/it]\u001b[A\n","Steps:  41% 866/2092 [14:47<20:53,  1.02s/it]\u001b[A\n","Steps:  41% 867/2092 [14:48<20:50,  1.02s/it]\u001b[A\n","Steps:  41% 868/2092 [14:49<20:51,  1.02s/it]\u001b[A\n","Steps:  42% 869/2092 [14:50<20:49,  1.02s/it]\u001b[A\n","Steps:  42% 870/2092 [14:51<20:44,  1.02s/it]\u001b[A\n","Steps:  42% 871/2092 [14:52<20:46,  1.02s/it]\u001b[A\n","Steps:  42% 872/2092 [14:53<20:46,  1.02s/it]\u001b[A\n","Steps:  42% 873/2092 [14:54<20:47,  1.02s/it]\u001b[A\n","Steps:  42% 874/2092 [14:55<20:41,  1.02s/it]\u001b[A\n","Steps:  42% 875/2092 [14:56<20:40,  1.02s/it]\u001b[A\n","Steps:  42% 876/2092 [14:57<20:38,  1.02s/it]\u001b[A\n","Steps:  42% 877/2092 [14:58<20:38,  1.02s/it]\u001b[A\n","Steps:  42% 878/2092 [14:59<20:40,  1.02s/it]\u001b[A\n","Steps:  42% 879/2092 [15:00<20:38,  1.02s/it]\u001b[A\n","Steps:  42% 880/2092 [15:01<20:33,  1.02s/it]\u001b[A\n","Steps:  42% 881/2092 [15:02<20:36,  1.02s/it]\u001b[A\n","Steps:  42% 882/2092 [15:03<20:30,  1.02s/it]\u001b[A\n","Steps:  42% 883/2092 [15:04<20:30,  1.02s/it]\u001b[A\n","Steps:  42% 884/2092 [15:05<20:32,  1.02s/it]\u001b[A\n","Steps:  42% 885/2092 [15:06<20:27,  1.02s/it]\u001b[A\n","Steps:  42% 886/2092 [15:07<20:27,  1.02s/it]\u001b[A\n","Steps:  42% 887/2092 [15:08<20:30,  1.02s/it]\u001b[A\n","Steps:  42% 888/2092 [15:09<20:30,  1.02s/it]\u001b[A\n","Steps:  42% 889/2092 [15:10<20:27,  1.02s/it]\u001b[A\n","Steps:  43% 890/2092 [15:11<20:25,  1.02s/it]\u001b[A\n","Steps:  43% 891/2092 [15:12<20:28,  1.02s/it]\u001b[A\n","Steps:  43% 892/2092 [15:13<20:26,  1.02s/it]\u001b[A\n","Steps:  43% 893/2092 [15:14<20:28,  1.02s/it]\u001b[A\n","Steps:  43% 894/2092 [15:16<20:27,  1.02s/it]\u001b[A\n","Steps:  43% 895/2092 [15:17<20:28,  1.03s/it]\u001b[A\n","Steps:  43% 896/2092 [15:18<20:24,  1.02s/it]\u001b[A\n","Steps:  43% 897/2092 [15:19<20:22,  1.02s/it]\u001b[A\n","Steps:  43% 898/2092 [15:20<20:20,  1.02s/it]\u001b[A\n","Steps:  43% 899/2092 [15:21<20:14,  1.02s/it]\u001b[A\n","Steps:  43% 900/2092 [15:22<20:15,  1.02s/it]\u001b[A\n","Steps:  43% 901/2092 [15:23<20:17,  1.02s/it]\u001b[A\n","Steps:  43% 902/2092 [15:24<20:15,  1.02s/it]\u001b[A\n","Steps:  43% 903/2092 [15:25<20:15,  1.02s/it]\u001b[A\n","Steps:  43% 904/2092 [15:26<20:16,  1.02s/it]\u001b[A\n","Steps:  43% 905/2092 [15:27<20:16,  1.02s/it]\u001b[A\n","Steps:  43% 906/2092 [15:28<20:12,  1.02s/it]\u001b[A\n","Steps:  43% 907/2092 [15:29<20:07,  1.02s/it]\u001b[A\n","Steps:  43% 908/2092 [15:30<20:09,  1.02s/it]\u001b[A\n","Steps:  43% 909/2092 [15:31<20:04,  1.02s/it]\u001b[A\n","Steps:  43% 910/2092 [15:32<20:00,  1.02s/it]\u001b[A\n","Steps:  44% 911/2092 [15:33<20:00,  1.02s/it]\u001b[A\n","Steps:  44% 912/2092 [15:34<20:03,  1.02s/it]\u001b[A\n","Steps:  44% 913/2092 [15:35<20:05,  1.02s/it]\u001b[A\n","Steps:  44% 914/2092 [15:36<20:01,  1.02s/it]\u001b[A\n","Steps:  44% 915/2092 [15:37<20:02,  1.02s/it]\u001b[A\n","Steps:  44% 916/2092 [15:38<20:03,  1.02s/it]\u001b[A\n","Steps:  44% 917/2092 [15:39<20:04,  1.03s/it]\u001b[A\n","Steps:  44% 918/2092 [15:40<20:05,  1.03s/it]\u001b[A\n","Steps:  44% 919/2092 [15:41<20:05,  1.03s/it]\u001b[A\n","Steps:  44% 920/2092 [15:42<19:57,  1.02s/it]\u001b[A\n","Steps:  44% 921/2092 [15:43<19:54,  1.02s/it]\u001b[A\n","Steps:  44% 922/2092 [15:44<19:54,  1.02s/it]\u001b[A\n","Steps:  44% 923/2092 [15:45<19:56,  1.02s/it]\u001b[A\n","Steps:  44% 924/2092 [15:46<19:52,  1.02s/it]\u001b[A\n","Steps:  44% 925/2092 [15:47<19:54,  1.02s/it]\u001b[A\n","Steps:  44% 926/2092 [15:48<19:51,  1.02s/it]\u001b[A\n","Steps:  44% 927/2092 [15:49<19:48,  1.02s/it]\u001b[A\n","Steps:  44% 928/2092 [15:50<19:50,  1.02s/it]\u001b[A\n","Steps:  44% 929/2092 [15:51<19:48,  1.02s/it]\u001b[A\n","Steps:  44% 930/2092 [15:52<19:49,  1.02s/it]\u001b[A\n","Steps:  45% 931/2092 [15:53<19:46,  1.02s/it]\u001b[A\n","Steps:  45% 932/2092 [15:54<19:43,  1.02s/it]\u001b[A\n","Steps:  45% 933/2092 [15:55<19:38,  1.02s/it]\u001b[A\n","Steps:  45% 934/2092 [15:56<19:41,  1.02s/it]\u001b[A\n","Steps:  45% 935/2092 [15:57<19:39,  1.02s/it]\u001b[A\n","Steps:  45% 936/2092 [15:58<19:41,  1.02s/it]\u001b[A\n","Steps:  45% 937/2092 [15:59<19:42,  1.02s/it]\u001b[A\n","Steps:  45% 938/2092 [16:00<19:36,  1.02s/it]\u001b[A\n","Steps:  45% 939/2092 [16:01<19:38,  1.02s/it]\u001b[A\n","Steps:  45% 940/2092 [16:03<19:39,  1.02s/it]\u001b[A\n","Steps:  45% 941/2092 [16:04<19:37,  1.02s/it]\u001b[A\n","Steps:  45% 942/2092 [16:05<19:34,  1.02s/it]\u001b[A\n","Steps:  45% 943/2092 [16:06<19:35,  1.02s/it]\u001b[A\n","Steps:  45% 944/2092 [16:07<19:37,  1.03s/it]\u001b[A\n","Steps:  45% 945/2092 [16:08<19:31,  1.02s/it]\u001b[A\n","Steps:  45% 946/2092 [16:09<19:29,  1.02s/it]\u001b[A\n","Steps:  45% 947/2092 [16:10<19:31,  1.02s/it]\u001b[A\n","Steps:  45% 948/2092 [16:11<19:28,  1.02s/it]\u001b[A\n","Steps:  45% 949/2092 [16:12<19:23,  1.02s/it]\u001b[A\n","Steps:  45% 950/2092 [16:13<19:20,  1.02s/it]\u001b[A\n","Steps:  45% 951/2092 [16:14<19:21,  1.02s/it]\u001b[A\n","Steps:  46% 952/2092 [16:15<19:23,  1.02s/it]\u001b[A\n","Steps:  46% 953/2092 [16:16<19:21,  1.02s/it]\u001b[A\n","Steps:  46% 954/2092 [16:17<19:38,  1.04s/it]\u001b[A\n","Steps:  46% 955/2092 [16:18<19:35,  1.03s/it]\u001b[A\n","Steps:  46% 956/2092 [16:19<19:33,  1.03s/it]\u001b[A\n","Steps:  46% 957/2092 [16:20<19:30,  1.03s/it]\u001b[A\n","Steps:  46% 958/2092 [16:21<19:26,  1.03s/it]\u001b[A\n","Steps:  46% 959/2092 [16:22<19:19,  1.02s/it]\u001b[A\n","Steps:  46% 960/2092 [16:23<19:14,  1.02s/it]\u001b[A\n","Steps:  46% 961/2092 [16:24<19:10,  1.02s/it]\u001b[A\n","Steps:  46% 962/2092 [16:25<19:07,  1.02s/it]\u001b[A\n","Steps:  46% 963/2092 [16:26<19:04,  1.01s/it]\u001b[A\n","Steps:  46% 964/2092 [16:27<19:02,  1.01s/it]\u001b[A\n","Steps:  46% 965/2092 [16:28<19:05,  1.02s/it]\u001b[A\n","Steps:  46% 966/2092 [16:29<19:05,  1.02s/it]\u001b[A\n","Steps:  46% 967/2092 [16:30<19:01,  1.02s/it]\u001b[A\n","Steps:  46% 968/2092 [16:31<19:05,  1.02s/it]\u001b[A\n","Steps:  46% 969/2092 [16:32<19:07,  1.02s/it]\u001b[A\n","Steps:  46% 970/2092 [16:33<19:05,  1.02s/it]\u001b[A\n","Steps:  46% 971/2092 [16:34<19:07,  1.02s/it]\u001b[A\n","Steps:  46% 972/2092 [16:35<19:00,  1.02s/it]\u001b[A\n","Steps:  47% 973/2092 [16:36<19:01,  1.02s/it]\u001b[A\n","Steps:  47% 974/2092 [16:37<19:02,  1.02s/it]\u001b[A\n","Steps:  47% 975/2092 [16:38<19:01,  1.02s/it]\u001b[A\n","Steps:  47% 976/2092 [16:39<19:01,  1.02s/it]\u001b[A\n","Steps:  47% 977/2092 [16:40<18:59,  1.02s/it]\u001b[A\n","Steps:  47% 978/2092 [16:41<18:59,  1.02s/it]\u001b[A\n","Steps:  47% 979/2092 [16:42<18:57,  1.02s/it]\u001b[A\n","Steps:  47% 980/2092 [16:43<18:56,  1.02s/it]\u001b[A\n","Steps:  47% 981/2092 [16:44<18:54,  1.02s/it]\u001b[A\n","Steps:  47% 982/2092 [16:45<18:52,  1.02s/it]\u001b[A\n","Steps:  47% 983/2092 [16:46<18:53,  1.02s/it]\u001b[A\n","Steps:  47% 984/2092 [16:47<18:51,  1.02s/it]\u001b[A\n","Steps:  47% 985/2092 [16:48<18:50,  1.02s/it]\u001b[A\n","Steps:  47% 986/2092 [16:49<18:46,  1.02s/it]\u001b[A\n","Steps:  47% 987/2092 [16:50<18:45,  1.02s/it]\u001b[A\n","Steps:  47% 988/2092 [16:52<18:44,  1.02s/it]\u001b[A\n","Steps:  47% 989/2092 [16:53<18:45,  1.02s/it]\u001b[A\n","Steps:  47% 990/2092 [16:54<18:43,  1.02s/it]\u001b[A\n","Steps:  47% 991/2092 [16:55<18:42,  1.02s/it]\u001b[A\n","Steps:  47% 992/2092 [16:56<18:42,  1.02s/it]\u001b[A\n","Steps:  47% 993/2092 [16:57<18:40,  1.02s/it]\u001b[A\n","Steps:  48% 994/2092 [16:58<18:41,  1.02s/it]\u001b[A\n","Steps:  48% 995/2092 [16:59<18:43,  1.02s/it]\u001b[A\n","Steps:  48% 996/2092 [17:00<18:42,  1.02s/it]\u001b[A\n","Steps:  48% 997/2092 [17:01<18:43,  1.03s/it]\u001b[A\n","Steps:  48% 998/2092 [17:02<18:35,  1.02s/it]\u001b[A\n","Steps:  48% 999/2092 [17:03<18:33,  1.02s/it]\u001b[A\n","Steps:  48% 1000/2092 [17:04<18:32,  1.02s/it]\u001b[A\n","Steps:  48% 1001/2092 [17:05<18:33,  1.02s/it]\u001b[A\n","Steps:  48% 1002/2092 [17:06<18:32,  1.02s/it]\u001b[A\n","Steps:  48% 1003/2092 [17:07<18:27,  1.02s/it]\u001b[A\n","Steps:  48% 1004/2092 [17:08<18:23,  1.01s/it]\u001b[A\n","Steps:  48% 1005/2092 [17:09<18:23,  1.02s/it]\u001b[A\n","Steps:  48% 1006/2092 [17:10<18:25,  1.02s/it]\u001b[A\n","Steps:  48% 1007/2092 [17:11<18:25,  1.02s/it]\u001b[A\n","Steps:  48% 1008/2092 [17:12<18:23,  1.02s/it]\u001b[A\n","Steps:  48% 1009/2092 [17:13<18:26,  1.02s/it]\u001b[A\n","Steps:  48% 1010/2092 [17:14<18:26,  1.02s/it]\u001b[A\n","Steps:  48% 1011/2092 [17:15<18:22,  1.02s/it]\u001b[A\n","Steps:  48% 1012/2092 [17:16<18:20,  1.02s/it]\u001b[A\n","Steps:  48% 1013/2092 [17:17<18:16,  1.02s/it]\u001b[A\n","Steps:  48% 1014/2092 [17:18<18:15,  1.02s/it]\u001b[A\n","Steps:  49% 1015/2092 [17:19<18:18,  1.02s/it]\u001b[A\n","Steps:  49% 1016/2092 [17:20<18:20,  1.02s/it]\u001b[A\n","Steps:  49% 1017/2092 [17:21<18:15,  1.02s/it]\u001b[A\n","Steps:  49% 1018/2092 [17:22<18:13,  1.02s/it]\u001b[A\n","Steps:  49% 1019/2092 [17:23<18:10,  1.02s/it]\u001b[A\n","Steps:  49% 1020/2092 [17:24<18:12,  1.02s/it]\u001b[A\n","Steps:  49% 1021/2092 [17:25<18:11,  1.02s/it]\u001b[A\n","Steps:  49% 1022/2092 [17:26<18:10,  1.02s/it]\u001b[A\n","Steps:  49% 1023/2092 [17:27<18:08,  1.02s/it]\u001b[A\n","Steps:  49% 1024/2092 [17:28<18:03,  1.01s/it]\u001b[A\n","Steps:  49% 1025/2092 [17:29<18:06,  1.02s/it]\u001b[A\n","Steps:  49% 1026/2092 [17:30<18:03,  1.02s/it]\u001b[A\n","Steps:  49% 1027/2092 [17:31<18:02,  1.02s/it]\u001b[A\n","Steps:  49% 1028/2092 [17:32<18:02,  1.02s/it]\u001b[A\n","Steps:  49% 1029/2092 [17:33<18:04,  1.02s/it]\u001b[A\n","Steps:  49% 1030/2092 [17:34<18:02,  1.02s/it]\u001b[A\n","Steps:  49% 1031/2092 [17:35<18:03,  1.02s/it]\u001b[A\n","Steps:  49% 1032/2092 [17:36<18:04,  1.02s/it]\u001b[A\n","Steps:  49% 1033/2092 [17:37<18:02,  1.02s/it]\u001b[A\n","Steps:  49% 1034/2092 [17:38<17:58,  1.02s/it]\u001b[A\n","Steps:  49% 1035/2092 [17:39<17:58,  1.02s/it]\u001b[A\n","Steps:  50% 1036/2092 [17:40<17:57,  1.02s/it]\u001b[A\n","Steps:  50% 1037/2092 [17:41<17:59,  1.02s/it]\u001b[A\n","Steps:  50% 1038/2092 [17:42<17:56,  1.02s/it]\u001b[A\n","Steps:  50% 1039/2092 [17:44<17:56,  1.02s/it]\u001b[A\n","Steps:  50% 1040/2092 [17:45<17:55,  1.02s/it]\u001b[A\n","Steps:  50% 1041/2092 [17:46<17:57,  1.02s/it]\u001b[A\n","Steps:  50% 1042/2092 [17:47<17:57,  1.03s/it]\u001b[A\n","Steps:  50% 1043/2092 [17:48<17:53,  1.02s/it]\u001b[A\n","Steps:  50% 1044/2092 [17:49<17:53,  1.02s/it]\u001b[A\n","Steps:  50% 1045/2092 [17:50<17:53,  1.03s/it]\u001b[A\n","Steps:  50% 1046/2092 [17:51<17:50,  1.02s/it]\u001b[A\n","Steps:  50% 1047/2092 [17:52<17:51,  1.02s/it]\u001b[A\n","Steps:  50% 1048/2092 [17:53<17:48,  1.02s/it]\u001b[A\n","Steps:  50% 1049/2092 [17:54<17:48,  1.02s/it]\u001b[A\n","Steps:  50% 1050/2092 [17:55<17:43,  1.02s/it]\u001b[A\n","Steps:  50% 1051/2092 [17:56<17:44,  1.02s/it]\u001b[A\n","Steps:  50% 1052/2092 [17:57<17:45,  1.02s/it]\u001b[A\n","Steps:  50% 1053/2092 [17:58<17:43,  1.02s/it]\u001b[A\n","Steps:  50% 1054/2092 [17:59<17:41,  1.02s/it]\u001b[A\n","Steps:  50% 1055/2092 [18:00<17:39,  1.02s/it]\u001b[A\n","Steps:  50% 1056/2092 [18:01<17:40,  1.02s/it]\u001b[A\n","Steps:  51% 1057/2092 [18:02<17:40,  1.02s/it]\u001b[A\n","Steps:  51% 1058/2092 [18:03<17:38,  1.02s/it]\u001b[A\n","Steps:  51% 1059/2092 [18:04<17:38,  1.03s/it]\u001b[A\n","Steps:  51% 1060/2092 [18:05<17:38,  1.03s/it]\u001b[A\n","Steps:  51% 1061/2092 [18:06<17:38,  1.03s/it]\u001b[A\n","Steps:  51% 1062/2092 [18:07<17:32,  1.02s/it]\u001b[A\n","Steps:  51% 1063/2092 [18:08<17:33,  1.02s/it]\u001b[A\n","Steps:  51% 1064/2092 [18:09<17:33,  1.03s/it]\u001b[A\n","Steps:  51% 1065/2092 [18:10<17:31,  1.02s/it]\u001b[A\n","Steps:  51% 1066/2092 [18:11<17:32,  1.03s/it]\u001b[A\n","Steps:  51% 1067/2092 [18:12<17:31,  1.03s/it]\u001b[A\n","Steps:  51% 1068/2092 [18:13<17:27,  1.02s/it]\u001b[A\n","Steps:  51% 1069/2092 [18:14<17:26,  1.02s/it]\u001b[A\n","Steps:  51% 1070/2092 [18:15<17:27,  1.02s/it]\u001b[A\n","Steps:  51% 1071/2092 [18:16<17:28,  1.03s/it]\u001b[A\n","Steps:  51% 1072/2092 [18:17<17:26,  1.03s/it]\u001b[A\n","Steps:  51% 1073/2092 [18:18<17:22,  1.02s/it]\u001b[A\n","Steps:  51% 1074/2092 [18:19<17:23,  1.03s/it]\u001b[A\n","Steps:  51% 1075/2092 [18:20<17:17,  1.02s/it]\u001b[A\n","Steps:  51% 1076/2092 [18:21<17:15,  1.02s/it]\u001b[A\n","Steps:  51% 1077/2092 [18:22<17:14,  1.02s/it]\u001b[A\n","Steps:  52% 1078/2092 [18:23<17:10,  1.02s/it]\u001b[A\n","Steps:  52% 1079/2092 [18:24<17:12,  1.02s/it]\u001b[A\n","Steps:  52% 1080/2092 [18:25<17:08,  1.02s/it]\u001b[A\n","Steps:  52% 1081/2092 [18:26<17:06,  1.01s/it]\u001b[A\n","Steps:  52% 1082/2092 [18:27<17:06,  1.02s/it]\u001b[A\n","Steps:  52% 1083/2092 [18:29<17:05,  1.02s/it]\u001b[A\n","Steps:  52% 1084/2092 [18:30<17:05,  1.02s/it]\u001b[A\n","Steps:  52% 1085/2092 [18:31<17:05,  1.02s/it]\u001b[A\n","Steps:  52% 1086/2092 [18:32<17:02,  1.02s/it]\u001b[A\n","Steps:  52% 1087/2092 [18:33<17:05,  1.02s/it]\u001b[A\n","Steps:  52% 1088/2092 [18:34<17:04,  1.02s/it]\u001b[A\n","Steps:  52% 1089/2092 [18:35<17:02,  1.02s/it]\u001b[A\n","Steps:  52% 1090/2092 [18:36<16:58,  1.02s/it]\u001b[A\n","Steps:  52% 1091/2092 [18:37<17:00,  1.02s/it]\u001b[A\n","Steps:  52% 1092/2092 [18:38<17:02,  1.02s/it]\u001b[A\n","Steps:  52% 1093/2092 [18:39<16:57,  1.02s/it]\u001b[A\n","Steps:  52% 1094/2092 [18:40<16:52,  1.01s/it]\u001b[A\n","Steps:  52% 1095/2092 [18:41<16:54,  1.02s/it]\u001b[A\n","Steps:  52% 1096/2092 [18:42<16:56,  1.02s/it]\u001b[A\n","Steps:  52% 1097/2092 [18:43<16:51,  1.02s/it]\u001b[A\n","Steps:  52% 1098/2092 [18:44<16:50,  1.02s/it]\u001b[A\n","Steps:  53% 1099/2092 [18:45<16:53,  1.02s/it]\u001b[A\n","Steps:  53% 1100/2092 [18:46<16:49,  1.02s/it]\u001b[A\n","Steps:  53% 1101/2092 [18:47<16:49,  1.02s/it]\u001b[A\n","Steps:  53% 1102/2092 [18:48<16:51,  1.02s/it]\u001b[A\n","Steps:  53% 1103/2092 [18:49<16:50,  1.02s/it]\u001b[A\n","Steps:  53% 1104/2092 [18:50<16:49,  1.02s/it]\u001b[A\n","Steps:  53% 1105/2092 [18:51<16:50,  1.02s/it]\u001b[A\n","Steps:  53% 1106/2092 [18:52<16:50,  1.02s/it]\u001b[A\n","Steps:  53% 1107/2092 [18:53<16:46,  1.02s/it]\u001b[A\n","Steps:  53% 1108/2092 [18:54<16:47,  1.02s/it]\u001b[A\n","Steps:  53% 1109/2092 [18:55<16:41,  1.02s/it]\u001b[A\n","Steps:  53% 1110/2092 [18:56<16:39,  1.02s/it]\u001b[A\n","Steps:  53% 1111/2092 [18:57<16:38,  1.02s/it]\u001b[A\n","Steps:  53% 1112/2092 [18:58<16:37,  1.02s/it]\u001b[A\n","Steps:  53% 1113/2092 [18:59<16:36,  1.02s/it]\u001b[A\n","Steps:  53% 1114/2092 [19:00<16:36,  1.02s/it]\u001b[A\n","Steps:  53% 1115/2092 [19:01<16:34,  1.02s/it]\u001b[A\n","Steps:  53% 1116/2092 [19:02<16:37,  1.02s/it]\u001b[A\n","Steps:  53% 1117/2092 [19:03<16:37,  1.02s/it]\u001b[A\n","Steps:  53% 1118/2092 [19:04<16:37,  1.02s/it]\u001b[A\n","Steps:  53% 1119/2092 [19:05<16:34,  1.02s/it]\u001b[A\n","Steps:  54% 1120/2092 [19:06<16:34,  1.02s/it]\u001b[A\n","Steps:  54% 1121/2092 [19:07<16:32,  1.02s/it]\u001b[A\n","Steps:  54% 1122/2092 [19:08<16:31,  1.02s/it]\u001b[A\n","Steps:  54% 1123/2092 [19:09<16:29,  1.02s/it]\u001b[A\n","Steps:  54% 1124/2092 [19:10<16:27,  1.02s/it]\u001b[A\n","Steps:  54% 1125/2092 [19:11<16:29,  1.02s/it]\u001b[A\n","Steps:  54% 1126/2092 [19:12<16:29,  1.02s/it]\u001b[A\n","Steps:  54% 1127/2092 [19:13<16:24,  1.02s/it]\u001b[A\n","Steps:  54% 1128/2092 [19:14<16:24,  1.02s/it]\u001b[A\n","Steps:  54% 1129/2092 [19:15<16:23,  1.02s/it]\u001b[A\n","Steps:  54% 1130/2092 [19:16<16:24,  1.02s/it]\u001b[A\n","Steps:  54% 1131/2092 [19:18<16:23,  1.02s/it]\u001b[A\n","Steps:  54% 1132/2092 [19:19<16:17,  1.02s/it]\u001b[A\n","Steps:  54% 1133/2092 [19:20<16:14,  1.02s/it]\u001b[A\n","Steps:  54% 1134/2092 [19:21<16:16,  1.02s/it]\u001b[A\n","Steps:  54% 1135/2092 [19:22<16:17,  1.02s/it]\u001b[A\n","Steps:  54% 1136/2092 [19:23<16:16,  1.02s/it]\u001b[A\n","Steps:  54% 1137/2092 [19:24<16:16,  1.02s/it]\u001b[A\n","Steps:  54% 1138/2092 [19:25<16:14,  1.02s/it]\u001b[A\n","Steps:  54% 1139/2092 [19:26<16:12,  1.02s/it]\u001b[A\n","Steps:  54% 1140/2092 [19:27<16:11,  1.02s/it]\u001b[A\n","Steps:  55% 1141/2092 [19:28<16:07,  1.02s/it]\u001b[A\n","Steps:  55% 1142/2092 [19:29<16:06,  1.02s/it]\u001b[A\n","Steps:  55% 1143/2092 [19:30<16:05,  1.02s/it]\u001b[A\n","Steps:  55% 1144/2092 [19:31<16:07,  1.02s/it]\u001b[A\n","Steps:  55% 1145/2092 [19:32<16:05,  1.02s/it]\u001b[A\n","Steps:  55% 1146/2092 [19:33<16:01,  1.02s/it]\u001b[A\n","Steps:  55% 1147/2092 [19:34<16:01,  1.02s/it]\u001b[A\n","Steps:  55% 1148/2092 [19:35<16:00,  1.02s/it]\u001b[A\n","Steps:  55% 1149/2092 [19:36<19:00,  1.21s/it]\u001b[A\n","Steps:  55% 1150/2092 [19:37<18:05,  1.15s/it]\u001b[A\n","Steps:  55% 1151/2092 [19:39<17:25,  1.11s/it]\u001b[A\n","Steps:  55% 1152/2092 [19:40<17:01,  1.09s/it]\u001b[A\n","Steps:  55% 1153/2092 [19:41<16:38,  1.06s/it]\u001b[A\n","Steps:  55% 1154/2092 [19:42<16:26,  1.05s/it]\u001b[A\n","Steps:  55% 1155/2092 [19:43<16:17,  1.04s/it]\u001b[A\n","Steps:  55% 1156/2092 [19:44<16:07,  1.03s/it]\u001b[A\n","Steps:  55% 1157/2092 [19:45<15:59,  1.03s/it]\u001b[A\n","Steps:  55% 1158/2092 [19:46<15:59,  1.03s/it]\u001b[A\n","Steps:  55% 1159/2092 [19:47<15:58,  1.03s/it]\u001b[A\n","Steps:  55% 1160/2092 [19:48<15:57,  1.03s/it]\u001b[A\n","Steps:  55% 1161/2092 [19:49<15:56,  1.03s/it]\u001b[A\n","Steps:  56% 1162/2092 [19:50<15:51,  1.02s/it]\u001b[A\n","Steps:  56% 1163/2092 [19:51<15:45,  1.02s/it]\u001b[A\n","Steps:  56% 1164/2092 [19:52<15:46,  1.02s/it]\u001b[A\n","Steps:  56% 1165/2092 [19:53<15:47,  1.02s/it]\u001b[A\n","Steps:  56% 1166/2092 [19:54<15:45,  1.02s/it]\u001b[A\n","Steps:  56% 1167/2092 [19:55<15:45,  1.02s/it]\u001b[A\n","Steps:  56% 1168/2092 [19:56<15:44,  1.02s/it]\u001b[A\n","Steps:  56% 1169/2092 [19:57<15:45,  1.02s/it]\u001b[A\n","Steps:  56% 1170/2092 [19:58<15:40,  1.02s/it]\u001b[A\n","Steps:  56% 1171/2092 [19:59<15:41,  1.02s/it]\u001b[A\n","Steps:  56% 1172/2092 [20:00<15:39,  1.02s/it]\u001b[A\n","Steps:  56% 1173/2092 [20:01<15:38,  1.02s/it]\u001b[A\n","Steps:  56% 1174/2092 [20:02<15:39,  1.02s/it]\u001b[A\n","Steps:  56% 1175/2092 [20:03<15:37,  1.02s/it]\u001b[A\n","Steps:  56% 1176/2092 [20:04<15:35,  1.02s/it]\u001b[A\n","Steps:  56% 1177/2092 [20:05<15:33,  1.02s/it]\u001b[A\n","Steps:  56% 1178/2092 [20:06<15:29,  1.02s/it]\u001b[A\n","Steps:  56% 1179/2092 [20:07<15:32,  1.02s/it]\u001b[A\n","Steps:  56% 1180/2092 [20:08<15:32,  1.02s/it]\u001b[A\n","Steps:  56% 1181/2092 [20:09<15:30,  1.02s/it]\u001b[A\n","Steps:  57% 1182/2092 [20:10<15:31,  1.02s/it]\u001b[A\n","Steps:  57% 1183/2092 [20:11<15:27,  1.02s/it]\u001b[A\n","Steps:  57% 1184/2092 [20:12<15:23,  1.02s/it]\u001b[A\n","Steps:  57% 1185/2092 [20:13<15:22,  1.02s/it]\u001b[A\n","Steps:  57% 1186/2092 [20:14<15:18,  1.01s/it]\u001b[A\n","Steps:  57% 1187/2092 [20:15<15:18,  1.02s/it]\u001b[A\n","Steps:  57% 1188/2092 [20:16<15:14,  1.01s/it]\u001b[A\n","Steps:  57% 1189/2092 [20:17<15:16,  1.01s/it]\u001b[A\n","Steps:  57% 1190/2092 [20:18<15:18,  1.02s/it]\u001b[A\n","Steps:  57% 1191/2092 [20:19<15:18,  1.02s/it]\u001b[A\n","Steps:  57% 1192/2092 [20:20<15:14,  1.02s/it]\u001b[A\n","Steps:  57% 1193/2092 [20:21<15:14,  1.02s/it]\u001b[A\n","Steps:  57% 1194/2092 [20:22<15:16,  1.02s/it]\u001b[A\n","Steps:  57% 1195/2092 [20:23<15:15,  1.02s/it]\u001b[A\n","Steps:  57% 1196/2092 [20:24<15:11,  1.02s/it]\u001b[A\n","Steps:  57% 1197/2092 [20:25<15:12,  1.02s/it]\u001b[A\n","Steps:  57% 1198/2092 [20:26<15:11,  1.02s/it]\u001b[A\n","Steps:  57% 1199/2092 [20:27<15:11,  1.02s/it]\u001b[A\n","Steps:  57% 1200/2092 [20:28<15:10,  1.02s/it]\u001b[A\n","Steps:  57% 1201/2092 [20:30<15:11,  1.02s/it]\u001b[A\n","Steps:  57% 1202/2092 [20:31<15:10,  1.02s/it]\u001b[A\n","Steps:  58% 1203/2092 [20:32<15:08,  1.02s/it]\u001b[A\n","Steps:  58% 1204/2092 [20:33<15:04,  1.02s/it]\u001b[A\n","Steps:  58% 1205/2092 [20:34<15:03,  1.02s/it]\u001b[A\n","Steps:  58% 1206/2092 [20:35<14:59,  1.02s/it]\u001b[A\n","Steps:  58% 1207/2092 [20:36<14:59,  1.02s/it]\u001b[A\n","Steps:  58% 1208/2092 [20:37<14:54,  1.01s/it]\u001b[A\n","Steps:  58% 1209/2092 [20:38<14:56,  1.01s/it]\u001b[A\n","Steps:  58% 1210/2092 [20:39<14:58,  1.02s/it]\u001b[A\n","Steps:  58% 1211/2092 [20:40<14:57,  1.02s/it]\u001b[A\n","Steps:  58% 1212/2092 [20:41<14:58,  1.02s/it]\u001b[A\n","Steps:  58% 1213/2092 [20:42<14:54,  1.02s/it]\u001b[A\n","Steps:  58% 1214/2092 [20:43<14:55,  1.02s/it]\u001b[A\n","Steps:  58% 1215/2092 [20:44<14:57,  1.02s/it]\u001b[A\n","Steps:  58% 1216/2092 [20:45<14:55,  1.02s/it]\u001b[A\n","Steps:  58% 1217/2092 [20:46<14:53,  1.02s/it]\u001b[A\n","Steps:  58% 1218/2092 [20:47<14:53,  1.02s/it]\u001b[A\n","Steps:  58% 1219/2092 [20:48<14:51,  1.02s/it]\u001b[A\n","Steps:  58% 1220/2092 [20:49<14:51,  1.02s/it]\u001b[A\n","Steps:  58% 1221/2092 [20:50<14:49,  1.02s/it]\u001b[A\n","Steps:  58% 1222/2092 [20:51<14:48,  1.02s/it]\u001b[A\n","Steps:  58% 1223/2092 [20:52<14:49,  1.02s/it]\u001b[A\n","Steps:  59% 1224/2092 [20:53<14:46,  1.02s/it]\u001b[A\n","Steps:  59% 1225/2092 [20:54<14:46,  1.02s/it]\u001b[A\n","Steps:  59% 1226/2092 [20:55<14:45,  1.02s/it]\u001b[A\n","Steps:  59% 1227/2092 [20:56<14:43,  1.02s/it]\u001b[A\n","Steps:  59% 1228/2092 [20:57<14:42,  1.02s/it]\u001b[A\n","Steps:  59% 1229/2092 [20:58<14:41,  1.02s/it]\u001b[A\n","Steps:  59% 1230/2092 [20:59<14:37,  1.02s/it]\u001b[A\n","Steps:  59% 1231/2092 [21:00<14:36,  1.02s/it]\u001b[A\n","Steps:  59% 1232/2092 [21:01<14:33,  1.02s/it]\u001b[A\n","Steps:  59% 1233/2092 [21:02<14:34,  1.02s/it]\u001b[A\n","Steps:  59% 1234/2092 [21:03<14:35,  1.02s/it]\u001b[A\n","Steps:  59% 1235/2092 [21:04<14:35,  1.02s/it]\u001b[A\n","Steps:  59% 1236/2092 [21:05<14:33,  1.02s/it]\u001b[A\n","Steps:  59% 1237/2092 [21:06<14:33,  1.02s/it]\u001b[A\n","Steps:  59% 1238/2092 [21:07<14:34,  1.02s/it]\u001b[A\n","Steps:  59% 1239/2092 [21:08<14:30,  1.02s/it]\u001b[A\n","Steps:  59% 1240/2092 [21:09<14:26,  1.02s/it]\u001b[A\n","Steps:  59% 1241/2092 [21:10<14:28,  1.02s/it]\u001b[A\n","Steps:  59% 1242/2092 [21:11<14:29,  1.02s/it]\u001b[A\n","Steps:  59% 1243/2092 [21:12<14:24,  1.02s/it]\u001b[A\n","Steps:  59% 1244/2092 [21:13<14:24,  1.02s/it]\u001b[A\n","Steps:  60% 1245/2092 [21:14<14:20,  1.02s/it]\u001b[A\n","Steps:  60% 1246/2092 [21:15<14:21,  1.02s/it]\u001b[A\n","Steps:  60% 1247/2092 [21:16<14:20,  1.02s/it]\u001b[A\n","Steps:  60% 1248/2092 [21:17<14:19,  1.02s/it]\u001b[A\n","Steps:  60% 1249/2092 [21:18<14:19,  1.02s/it]\u001b[A\n","Steps:  60% 1250/2092 [21:19<14:19,  1.02s/it]\u001b[A\n","Steps:  60% 1251/2092 [21:20<14:18,  1.02s/it]\u001b[A\n","Steps:  60% 1252/2092 [21:22<14:19,  1.02s/it]\u001b[A\n","Steps:  60% 1253/2092 [21:23<14:18,  1.02s/it]\u001b[A\n","Steps:  60% 1254/2092 [21:24<14:14,  1.02s/it]\u001b[A\n","Steps:  60% 1255/2092 [21:25<14:11,  1.02s/it]\u001b[A\n","Steps:  60% 1256/2092 [21:26<14:10,  1.02s/it]\u001b[A\n","Steps:  60% 1257/2092 [21:27<14:11,  1.02s/it]\u001b[A\n","Steps:  60% 1258/2092 [21:28<14:07,  1.02s/it]\u001b[A\n","Steps:  60% 1259/2092 [21:29<14:05,  1.01s/it]\u001b[A\n","Steps:  60% 1260/2092 [21:30<14:06,  1.02s/it]\u001b[A\n","Steps:  60% 1261/2092 [21:31<14:03,  1.01s/it]\u001b[A\n","Steps:  60% 1262/2092 [21:32<14:05,  1.02s/it]\u001b[A\n","Steps:  60% 1263/2092 [21:33<14:06,  1.02s/it]\u001b[A\n","Steps:  60% 1264/2092 [21:34<14:05,  1.02s/it]\u001b[A\n","Steps:  60% 1265/2092 [21:35<14:06,  1.02s/it]\u001b[A\n","Steps:  61% 1266/2092 [21:36<14:04,  1.02s/it]\u001b[A\n","Steps:  61% 1267/2092 [21:37<13:59,  1.02s/it]\u001b[A\n","Steps:  61% 1268/2092 [21:38<14:00,  1.02s/it]\u001b[A\n","Steps:  61% 1269/2092 [21:39<13:55,  1.02s/it]\u001b[A\n","Steps:  61% 1270/2092 [21:40<13:59,  1.02s/it]\u001b[A\n","Steps:  61% 1271/2092 [21:41<14:00,  1.02s/it]\u001b[A\n","Steps:  61% 1272/2092 [21:42<14:00,  1.02s/it]\u001b[A\n","Steps:  61% 1273/2092 [21:43<13:59,  1.02s/it]\u001b[A\n","Steps:  61% 1274/2092 [21:44<13:59,  1.03s/it]\u001b[A\n","Steps:  61% 1275/2092 [21:45<13:54,  1.02s/it]\u001b[A\n","Steps:  61% 1276/2092 [21:46<13:52,  1.02s/it]\u001b[A\n","Steps:  61% 1277/2092 [21:47<13:49,  1.02s/it]\u001b[A\n","Steps:  61% 1278/2092 [21:48<13:50,  1.02s/it]\u001b[A\n","Steps:  61% 1279/2092 [21:49<13:49,  1.02s/it]\u001b[A\n","Steps:  61% 1280/2092 [21:50<13:48,  1.02s/it]\u001b[A\n","Steps:  61% 1281/2092 [21:51<13:42,  1.01s/it]\u001b[A\n","Steps:  61% 1282/2092 [21:52<13:43,  1.02s/it]\u001b[A\n","Steps:  61% 1283/2092 [21:53<13:44,  1.02s/it]\u001b[A\n","Steps:  61% 1284/2092 [21:54<13:39,  1.01s/it]\u001b[A\n","Steps:  61% 1285/2092 [21:55<13:39,  1.02s/it]\u001b[A\n","Steps:  61% 1286/2092 [21:56<13:39,  1.02s/it]\u001b[A\n","Steps:  62% 1287/2092 [21:57<13:39,  1.02s/it]\u001b[A\n","Steps:  62% 1288/2092 [21:58<13:39,  1.02s/it]\u001b[A\n","Steps:  62% 1289/2092 [21:59<13:39,  1.02s/it]\u001b[A\n","Steps:  62% 1290/2092 [22:00<13:38,  1.02s/it]\u001b[A\n","Steps:  62% 1291/2092 [22:01<13:39,  1.02s/it]\u001b[A\n","Steps:  62% 1292/2092 [22:02<13:37,  1.02s/it]\u001b[A\n","Steps:  62% 1293/2092 [22:03<13:35,  1.02s/it]\u001b[A\n","Steps:  62% 1294/2092 [22:04<13:34,  1.02s/it]\u001b[A\n","Steps:  62% 1295/2092 [22:05<13:34,  1.02s/it]\u001b[A\n","Steps:  62% 1296/2092 [22:06<13:30,  1.02s/it]\u001b[A\n","Steps:  62% 1297/2092 [22:07<13:30,  1.02s/it]\u001b[A\n","Steps:  62% 1298/2092 [22:08<13:30,  1.02s/it]\u001b[A\n","Steps:  62% 1299/2092 [22:09<13:26,  1.02s/it]\u001b[A\n","Steps:  62% 1300/2092 [22:10<13:25,  1.02s/it]\u001b[A\n","Steps:  62% 1301/2092 [22:11<13:25,  1.02s/it]\u001b[A\n","Steps:  62% 1302/2092 [22:13<13:27,  1.02s/it]\u001b[A\n","Steps:  62% 1303/2092 [22:14<13:25,  1.02s/it]\u001b[A\n","Steps:  62% 1304/2092 [22:15<13:21,  1.02s/it]\u001b[A\n","Steps:  62% 1305/2092 [22:16<13:20,  1.02s/it]\u001b[A\n","Steps:  62% 1306/2092 [22:17<13:18,  1.02s/it]\u001b[A\n","Steps:  62% 1307/2092 [22:18<13:17,  1.02s/it]\u001b[A\n","Steps:  63% 1308/2092 [22:19<13:15,  1.02s/it]\u001b[A\n","Steps:  63% 1309/2092 [22:20<13:17,  1.02s/it]\u001b[A\n","Steps:  63% 1310/2092 [22:21<13:16,  1.02s/it]\u001b[A\n","Steps:  63% 1311/2092 [22:22<13:16,  1.02s/it]\u001b[A\n","Steps:  63% 1312/2092 [22:23<13:13,  1.02s/it]\u001b[A\n","Steps:  63% 1313/2092 [22:24<13:11,  1.02s/it]\u001b[A\n","Steps:  63% 1314/2092 [22:25<13:11,  1.02s/it]\u001b[A\n","Steps:  63% 1315/2092 [22:26<13:11,  1.02s/it]\u001b[A\n","Steps:  63% 1316/2092 [22:27<13:12,  1.02s/it]\u001b[A\n","Steps:  63% 1317/2092 [22:28<13:11,  1.02s/it]\u001b[A\n","Steps:  63% 1318/2092 [22:29<13:11,  1.02s/it]\u001b[A\n","Steps:  63% 1319/2092 [22:30<13:10,  1.02s/it]\u001b[A\n","Steps:  63% 1320/2092 [22:31<13:09,  1.02s/it]\u001b[A\n","Steps:  63% 1321/2092 [22:32<13:07,  1.02s/it]\u001b[A\n","Steps:  63% 1322/2092 [22:33<13:06,  1.02s/it]\u001b[A\n","Steps:  63% 1323/2092 [22:34<13:05,  1.02s/it]\u001b[A\n","Steps:  63% 1324/2092 [22:35<13:03,  1.02s/it]\u001b[A\n","Steps:  63% 1325/2092 [22:36<13:04,  1.02s/it]\u001b[A\n","Steps:  63% 1326/2092 [22:37<13:02,  1.02s/it]\u001b[A\n","Steps:  63% 1327/2092 [22:38<13:03,  1.02s/it]\u001b[A\n","Steps:  63% 1328/2092 [22:39<13:01,  1.02s/it]\u001b[A\n","Steps:  64% 1329/2092 [22:40<13:00,  1.02s/it]\u001b[A\n","Steps:  64% 1330/2092 [22:41<12:58,  1.02s/it]\u001b[A\n","Steps:  64% 1331/2092 [22:42<12:56,  1.02s/it]\u001b[A\n","Steps:  64% 1332/2092 [22:43<12:52,  1.02s/it]\u001b[A\n","Steps:  64% 1333/2092 [22:44<12:54,  1.02s/it]\u001b[A\n","Steps:  64% 1334/2092 [22:45<12:56,  1.02s/it]\u001b[A\n","Steps:  64% 1335/2092 [22:46<12:51,  1.02s/it]\u001b[A\n","Steps:  64% 1336/2092 [22:47<12:46,  1.01s/it]\u001b[A\n","Steps:  64% 1337/2092 [22:48<12:44,  1.01s/it]\u001b[A\n","Steps:  64% 1338/2092 [22:49<12:41,  1.01s/it]\u001b[A\n","Steps:  64% 1339/2092 [22:50<12:40,  1.01s/it]\u001b[A\n","Steps:  64% 1340/2092 [22:51<12:39,  1.01s/it]\u001b[A\n","Steps:  64% 1341/2092 [22:52<12:42,  1.02s/it]\u001b[A\n","Steps:  64% 1342/2092 [22:53<12:42,  1.02s/it]\u001b[A\n","Steps:  64% 1343/2092 [22:54<12:44,  1.02s/it]\u001b[A\n","Steps:  64% 1344/2092 [22:55<12:43,  1.02s/it]\u001b[A\n","Steps:  64% 1345/2092 [22:56<12:42,  1.02s/it]\u001b[A\n","Steps:  64% 1346/2092 [22:57<12:39,  1.02s/it]\u001b[A\n","Steps:  64% 1347/2092 [22:58<12:40,  1.02s/it]\u001b[A\n","Steps:  64% 1348/2092 [22:59<12:41,  1.02s/it]\u001b[A\n","Steps:  64% 1349/2092 [23:00<12:38,  1.02s/it]\u001b[A\n","Steps:  65% 1350/2092 [23:01<12:37,  1.02s/it]\u001b[A\n","Steps:  65% 1351/2092 [23:02<12:35,  1.02s/it]\u001b[A\n","Steps:  65% 1352/2092 [23:03<12:36,  1.02s/it]\u001b[A\n","Steps:  65% 1353/2092 [23:04<12:32,  1.02s/it]\u001b[A\n","Steps:  65% 1354/2092 [23:05<12:32,  1.02s/it]\u001b[A\n","Steps:  65% 1355/2092 [23:07<12:29,  1.02s/it]\u001b[A\n","Steps:  65% 1356/2092 [23:08<12:30,  1.02s/it]\u001b[A\n","Steps:  65% 1357/2092 [23:09<12:31,  1.02s/it]\u001b[A\n","Steps:  65% 1358/2092 [23:10<12:27,  1.02s/it]\u001b[A\n","Steps:  65% 1359/2092 [23:11<12:26,  1.02s/it]\u001b[A\n","Steps:  65% 1360/2092 [23:12<12:28,  1.02s/it]\u001b[A\n","Steps:  65% 1361/2092 [23:13<12:28,  1.02s/it]\u001b[A\n","Steps:  65% 1362/2092 [23:14<12:27,  1.02s/it]\u001b[A\n","Steps:  65% 1363/2092 [23:15<12:24,  1.02s/it]\u001b[A\n","Steps:  65% 1364/2092 [23:16<12:22,  1.02s/it]\u001b[A\n","Steps:  65% 1365/2092 [23:17<12:23,  1.02s/it]\u001b[A\n","Steps:  65% 1366/2092 [23:18<12:24,  1.02s/it]\u001b[A\n","Steps:  65% 1367/2092 [23:19<12:22,  1.02s/it]\u001b[A\n","Steps:  65% 1368/2092 [23:20<12:21,  1.02s/it]\u001b[A\n","Steps:  65% 1369/2092 [23:21<12:21,  1.03s/it]\u001b[A\n","Steps:  65% 1370/2092 [23:22<12:18,  1.02s/it]\u001b[A\n","Steps:  66% 1371/2092 [23:23<12:15,  1.02s/it]\u001b[A\n","Steps:  66% 1372/2092 [23:24<12:14,  1.02s/it]\u001b[A\n","Steps:  66% 1373/2092 [23:25<12:15,  1.02s/it]\u001b[A\n","Steps:  66% 1374/2092 [23:26<12:11,  1.02s/it]\u001b[A\n","Steps:  66% 1375/2092 [23:27<12:09,  1.02s/it]\u001b[A\n","Steps:  66% 1376/2092 [23:28<12:08,  1.02s/it]\u001b[A\n","Steps:  66% 1377/2092 [23:29<12:07,  1.02s/it]\u001b[A\n","Steps:  66% 1378/2092 [23:30<12:06,  1.02s/it]\u001b[A\n","Steps:  66% 1379/2092 [23:31<12:08,  1.02s/it]\u001b[A\n","Steps:  66% 1380/2092 [23:32<12:06,  1.02s/it]\u001b[A\n","Steps:  66% 1381/2092 [23:33<12:05,  1.02s/it]\u001b[A\n","Steps:  66% 1382/2092 [23:34<12:01,  1.02s/it]\u001b[A\n","Steps:  66% 1383/2092 [23:35<12:02,  1.02s/it]\u001b[A\n","Steps:  66% 1384/2092 [23:36<12:03,  1.02s/it]\u001b[A\n","Steps:  66% 1385/2092 [23:37<12:01,  1.02s/it]\u001b[A\n","Steps:  66% 1386/2092 [23:38<11:59,  1.02s/it]\u001b[A\n","Steps:  66% 1387/2092 [23:39<11:56,  1.02s/it]\u001b[A\n","Steps:  66% 1388/2092 [23:40<11:56,  1.02s/it]\u001b[A\n","Steps:  66% 1389/2092 [23:41<11:57,  1.02s/it]\u001b[A\n","Steps:  66% 1390/2092 [23:42<11:54,  1.02s/it]\u001b[A\n","Steps:  66% 1391/2092 [23:43<11:55,  1.02s/it]\u001b[A\n","Steps:  67% 1392/2092 [23:44<11:55,  1.02s/it]\u001b[A\n","Steps:  67% 1393/2092 [23:45<11:52,  1.02s/it]\u001b[A\n","Steps:  67% 1394/2092 [23:46<11:50,  1.02s/it]\u001b[A\n","Steps:  67% 1395/2092 [23:47<11:47,  1.02s/it]\u001b[A\n","Steps:  67% 1396/2092 [23:48<11:49,  1.02s/it]\u001b[A\n","Steps:  67% 1397/2092 [23:49<11:46,  1.02s/it]\u001b[A\n","Steps:  67% 1398/2092 [23:50<11:48,  1.02s/it]\u001b[A\n","Steps:  67% 1399/2092 [23:51<11:47,  1.02s/it]\u001b[A\n","Steps:  67% 1400/2092 [23:52<11:44,  1.02s/it]\u001b[A\n","Steps:  67% 1401/2092 [23:53<11:44,  1.02s/it]\u001b[A\n","Steps:  67% 1402/2092 [23:54<11:41,  1.02s/it]\u001b[A\n","Steps:  67% 1403/2092 [23:55<11:42,  1.02s/it]\u001b[A\n","Steps:  67% 1404/2092 [23:56<11:40,  1.02s/it]\u001b[A\n","Steps:  67% 1405/2092 [23:58<11:40,  1.02s/it]\u001b[A\n","Steps:  67% 1406/2092 [23:59<11:41,  1.02s/it]\u001b[A\n","Steps:  67% 1407/2092 [24:00<11:39,  1.02s/it]\u001b[A\n","Steps:  67% 1408/2092 [24:01<11:40,  1.02s/it]\u001b[A\n","Steps:  67% 1409/2092 [24:02<11:38,  1.02s/it]\u001b[A\n","Steps:  67% 1410/2092 [24:03<11:34,  1.02s/it]\u001b[A\n","Steps:  67% 1411/2092 [24:04<11:33,  1.02s/it]\u001b[A\n","Steps:  67% 1412/2092 [24:05<11:30,  1.01s/it]\u001b[A\n","Steps:  68% 1413/2092 [24:06<11:31,  1.02s/it]\u001b[A\n","Steps:  68% 1414/2092 [24:07<11:30,  1.02s/it]\u001b[A\n","Steps:  68% 1415/2092 [24:08<11:30,  1.02s/it]\u001b[A\n","Steps:  68% 1416/2092 [24:09<11:27,  1.02s/it]\u001b[A\n","Steps:  68% 1417/2092 [24:10<11:28,  1.02s/it]\u001b[A\n","Steps:  68% 1418/2092 [24:11<11:28,  1.02s/it]\u001b[A\n","Steps:  68% 1419/2092 [24:12<11:27,  1.02s/it]\u001b[A\n","Steps:  68% 1420/2092 [24:13<11:27,  1.02s/it]\u001b[A\n","Steps:  68% 1421/2092 [24:14<11:25,  1.02s/it]\u001b[A\n","Steps:  68% 1422/2092 [24:15<11:24,  1.02s/it]\u001b[A\n","Steps:  68% 1423/2092 [24:16<11:22,  1.02s/it]\u001b[A\n","Steps:  68% 1424/2092 [24:17<11:21,  1.02s/it]\u001b[A\n","Steps:  68% 1425/2092 [24:18<11:22,  1.02s/it]\u001b[A\n","Steps:  68% 1426/2092 [24:19<11:22,  1.02s/it]\u001b[A\n","Steps:  68% 1427/2092 [24:20<11:22,  1.03s/it]\u001b[A\n","Steps:  68% 1428/2092 [24:21<11:20,  1.02s/it]\u001b[A\n","Steps:  68% 1429/2092 [24:22<11:20,  1.03s/it]\u001b[A\n","Steps:  68% 1430/2092 [24:23<11:18,  1.02s/it]\u001b[A\n","Steps:  68% 1431/2092 [24:24<11:16,  1.02s/it]\u001b[A\n","Steps:  68% 1432/2092 [24:25<11:17,  1.03s/it]\u001b[A\n","Steps:  68% 1433/2092 [24:26<11:16,  1.03s/it]\u001b[A\n","Steps:  69% 1434/2092 [24:27<11:15,  1.03s/it]\u001b[A\n","Steps:  69% 1435/2092 [24:28<11:12,  1.02s/it]\u001b[A\n","Steps:  69% 1436/2092 [24:29<11:10,  1.02s/it]\u001b[A\n","Steps:  69% 1437/2092 [24:30<11:08,  1.02s/it]\u001b[A\n","Steps:  69% 1438/2092 [24:31<11:08,  1.02s/it]\u001b[A\n","Steps:  69% 1439/2092 [24:32<11:07,  1.02s/it]\u001b[A\n","Steps:  69% 1440/2092 [24:33<11:05,  1.02s/it]\u001b[A\n","Steps:  69% 1441/2092 [24:34<11:04,  1.02s/it]\u001b[A\n","Steps:  69% 1442/2092 [24:35<11:00,  1.02s/it]\u001b[A\n","Steps:  69% 1443/2092 [24:36<11:00,  1.02s/it]\u001b[A\n","Steps:  69% 1444/2092 [24:37<11:01,  1.02s/it]\u001b[A\n","Steps:  69% 1445/2092 [24:38<10:58,  1.02s/it]\u001b[A\n","Steps:  69% 1446/2092 [24:39<10:55,  1.02s/it]\u001b[A\n","Steps:  69% 1447/2092 [24:40<10:57,  1.02s/it]\u001b[A\n","Steps:  69% 1448/2092 [24:41<10:57,  1.02s/it]\u001b[A\n","Steps:  69% 1449/2092 [24:42<10:57,  1.02s/it]\u001b[A\n","Steps:  69% 1450/2092 [24:43<10:56,  1.02s/it]\u001b[A\n","Steps:  69% 1451/2092 [24:44<10:54,  1.02s/it]\u001b[A\n","Steps:  69% 1452/2092 [24:46<10:54,  1.02s/it]\u001b[A\n","Steps:  69% 1453/2092 [24:47<10:53,  1.02s/it]\u001b[A\n","Steps:  70% 1454/2092 [24:48<10:52,  1.02s/it]\u001b[A\n","Steps:  70% 1455/2092 [24:49<10:48,  1.02s/it]\u001b[A\n","Steps:  70% 1456/2092 [24:50<10:49,  1.02s/it]\u001b[A\n","Steps:  70% 1457/2092 [24:51<10:48,  1.02s/it]\u001b[A\n","Steps:  70% 1458/2092 [24:52<10:49,  1.02s/it]\u001b[A\n","Steps:  70% 1459/2092 [24:53<10:49,  1.03s/it]\u001b[A\n","Steps:  70% 1460/2092 [24:54<10:49,  1.03s/it]\u001b[A\n","Steps:  70% 1461/2092 [24:55<10:47,  1.03s/it]\u001b[A\n","Steps:  70% 1462/2092 [24:56<10:42,  1.02s/it]\u001b[A\n","Steps:  70% 1463/2092 [24:57<10:42,  1.02s/it]\u001b[A\n","Steps:  70% 1464/2092 [24:58<10:38,  1.02s/it]\u001b[A\n","Steps:  70% 1465/2092 [24:59<10:34,  1.01s/it]\u001b[A\n","Steps:  70% 1466/2092 [25:00<10:35,  1.02s/it]\u001b[A\n","Steps:  70% 1467/2092 [25:01<10:32,  1.01s/it]\u001b[A\n","Steps:  70% 1468/2092 [25:02<10:32,  1.01s/it]\u001b[A\n","Steps:  70% 1469/2092 [25:03<10:32,  1.02s/it]\u001b[A\n","Steps:  70% 1470/2092 [25:04<10:34,  1.02s/it]\u001b[A\n","Steps:  70% 1471/2092 [25:05<10:33,  1.02s/it]\u001b[A\n","Steps:  70% 1472/2092 [25:06<10:30,  1.02s/it]\u001b[A\n","Steps:  70% 1473/2092 [25:07<10:28,  1.01s/it]\u001b[A\n","Steps:  70% 1474/2092 [25:08<10:27,  1.02s/it]\u001b[A\n","Steps:  71% 1475/2092 [25:09<10:27,  1.02s/it]\u001b[A\n","Steps:  71% 1476/2092 [25:10<10:26,  1.02s/it]\u001b[A\n","Steps:  71% 1477/2092 [25:11<10:26,  1.02s/it]\u001b[A\n","Steps:  71% 1478/2092 [25:12<10:25,  1.02s/it]\u001b[A\n","Steps:  71% 1479/2092 [25:13<10:25,  1.02s/it]\u001b[A\n","Steps:  71% 1480/2092 [25:14<10:24,  1.02s/it]\u001b[A\n","Steps:  71% 1481/2092 [25:15<10:24,  1.02s/it]\u001b[A\n","Steps:  71% 1482/2092 [25:16<10:22,  1.02s/it]\u001b[A\n","Steps:  71% 1483/2092 [25:17<10:19,  1.02s/it]\u001b[A\n","Steps:  71% 1484/2092 [25:18<10:18,  1.02s/it]\u001b[A\n","Steps:  71% 1485/2092 [25:19<10:18,  1.02s/it]\u001b[A\n","Steps:  71% 1486/2092 [25:20<10:18,  1.02s/it]\u001b[A\n","Steps:  71% 1487/2092 [25:21<10:17,  1.02s/it]\u001b[A\n","Steps:  71% 1488/2092 [25:22<10:14,  1.02s/it]\u001b[A\n","Steps:  71% 1489/2092 [25:23<10:15,  1.02s/it]\u001b[A\n","Steps:  71% 1490/2092 [25:24<10:15,  1.02s/it]\u001b[A\n","Steps:  71% 1491/2092 [25:25<10:13,  1.02s/it]\u001b[A\n","Steps:  71% 1492/2092 [25:26<10:14,  1.02s/it]\u001b[A\n","Steps:  71% 1493/2092 [25:27<10:13,  1.02s/it]\u001b[A\n","Steps:  71% 1494/2092 [25:28<10:12,  1.02s/it]\u001b[A\n","Steps:  71% 1495/2092 [25:29<10:10,  1.02s/it]\u001b[A\n","Steps:  72% 1496/2092 [25:30<10:10,  1.02s/it]\u001b[A\n","Steps:  72% 1497/2092 [25:31<10:07,  1.02s/it]\u001b[A\n","Steps:  72% 1498/2092 [25:32<10:07,  1.02s/it]\u001b[A\n","Steps:  72% 1499/2092 [25:33<10:04,  1.02s/it]\u001b[A\n","Steps:  72% 1500/2092 [25:34<10:02,  1.02s/it]\u001b[A\n","Steps:  72% 1501/2092 [25:36<10:03,  1.02s/it]\u001b[A\n","Steps:  72% 1502/2092 [25:37<10:00,  1.02s/it]\u001b[A\n","Steps:  72% 1503/2092 [25:38<10:01,  1.02s/it]\u001b[A\n","Steps:  72% 1504/2092 [25:39<10:01,  1.02s/it]\u001b[A\n","Steps:  72% 1505/2092 [25:40<10:00,  1.02s/it]\u001b[A\n","Steps:  72% 1506/2092 [25:41<09:59,  1.02s/it]\u001b[A\n","Steps:  72% 1507/2092 [25:42<09:59,  1.02s/it]\u001b[A\n","Steps:  72% 1508/2092 [25:43<09:54,  1.02s/it]\u001b[A\n","Steps:  72% 1509/2092 [25:44<09:53,  1.02s/it]\u001b[A\n","Steps:  72% 1510/2092 [25:45<09:54,  1.02s/it]\u001b[A\n","Steps:  72% 1511/2092 [25:46<09:54,  1.02s/it]\u001b[A\n","Steps:  72% 1512/2092 [25:47<09:52,  1.02s/it]\u001b[A\n","Steps:  72% 1513/2092 [25:48<09:51,  1.02s/it]\u001b[A\n","Steps:  72% 1514/2092 [25:49<09:50,  1.02s/it]\u001b[A\n","Steps:  72% 1515/2092 [25:50<09:49,  1.02s/it]\u001b[A\n","Steps:  72% 1516/2092 [25:51<09:49,  1.02s/it]\u001b[A\n","Steps:  73% 1517/2092 [25:52<09:47,  1.02s/it]\u001b[A\n","Steps:  73% 1518/2092 [25:53<09:47,  1.02s/it]\u001b[A\n","Steps:  73% 1519/2092 [25:54<09:46,  1.02s/it]\u001b[A\n","Steps:  73% 1520/2092 [25:55<09:45,  1.02s/it]\u001b[A\n","Steps:  73% 1521/2092 [25:56<09:44,  1.02s/it]\u001b[A\n","Steps:  73% 1522/2092 [25:57<09:44,  1.03s/it]\u001b[A\n","Steps:  73% 1523/2092 [25:58<09:43,  1.03s/it]\u001b[A\n","Steps:  73% 1524/2092 [25:59<09:41,  1.02s/it]\u001b[A\n","Steps:  73% 1525/2092 [26:00<09:39,  1.02s/it]\u001b[A\n","Steps:  73% 1526/2092 [26:01<09:40,  1.03s/it]\u001b[A\n","Steps:  73% 1527/2092 [26:02<09:38,  1.02s/it]\u001b[A\n","Steps:  73% 1528/2092 [26:03<09:36,  1.02s/it]\u001b[A\n","Steps:  73% 1529/2092 [26:04<09:36,  1.02s/it]\u001b[A\n","Steps:  73% 1530/2092 [26:05<09:36,  1.03s/it]\u001b[A\n","Steps:  73% 1531/2092 [26:06<09:34,  1.02s/it]\u001b[A\n","Steps:  73% 1532/2092 [26:07<09:32,  1.02s/it]\u001b[A\n","Steps:  73% 1533/2092 [26:08<09:30,  1.02s/it]\u001b[A\n","Steps:  73% 1534/2092 [26:09<09:30,  1.02s/it]\u001b[A\n","Steps:  73% 1535/2092 [26:10<09:30,  1.02s/it]\u001b[A\n","Steps:  73% 1536/2092 [26:11<09:29,  1.03s/it]\u001b[A\n","Steps:  73% 1537/2092 [26:12<09:29,  1.03s/it]\u001b[A\n","Steps:  74% 1538/2092 [26:13<09:26,  1.02s/it]\u001b[A\n","Steps:  74% 1539/2092 [26:14<09:25,  1.02s/it]\u001b[A\n","Steps:  74% 1540/2092 [26:15<09:25,  1.02s/it]\u001b[A\n","Steps:  74% 1541/2092 [26:16<09:23,  1.02s/it]\u001b[A\n","Steps:  74% 1542/2092 [26:17<09:23,  1.02s/it]\u001b[A\n","Steps:  74% 1543/2092 [26:18<09:21,  1.02s/it]\u001b[A\n","Steps:  74% 1544/2092 [26:19<09:21,  1.02s/it]\u001b[A\n","Steps:  74% 1545/2092 [26:21<09:19,  1.02s/it]\u001b[A\n","Steps:  74% 1546/2092 [26:22<09:17,  1.02s/it]\u001b[A\n","Steps:  74% 1547/2092 [26:23<09:17,  1.02s/it]\u001b[A\n","Steps:  74% 1548/2092 [26:24<09:15,  1.02s/it]\u001b[A\n","Steps:  74% 1549/2092 [26:25<09:14,  1.02s/it]\u001b[A\n","Steps:  74% 1550/2092 [26:26<09:13,  1.02s/it]\u001b[A\n","Steps:  74% 1551/2092 [26:27<09:11,  1.02s/it]\u001b[A\n","Steps:  74% 1552/2092 [26:28<09:10,  1.02s/it]\u001b[A\n","Steps:  74% 1553/2092 [26:29<09:10,  1.02s/it]\u001b[A\n","Steps:  74% 1554/2092 [26:30<09:07,  1.02s/it]\u001b[A\n","Steps:  74% 1555/2092 [26:31<09:07,  1.02s/it]\u001b[A\n","Steps:  74% 1556/2092 [26:32<09:07,  1.02s/it]\u001b[A\n","Steps:  74% 1557/2092 [26:33<09:04,  1.02s/it]\u001b[A\n","Steps:  74% 1558/2092 [26:34<09:04,  1.02s/it]\u001b[A\n","Steps:  75% 1559/2092 [26:35<09:01,  1.02s/it]\u001b[A\n","Steps:  75% 1560/2092 [26:36<09:02,  1.02s/it]\u001b[A\n","Steps:  75% 1561/2092 [26:37<09:02,  1.02s/it]\u001b[A\n","Steps:  75% 1562/2092 [26:38<09:02,  1.02s/it]\u001b[A\n","Steps:  75% 1563/2092 [26:39<09:01,  1.02s/it]\u001b[A\n","Steps:  75% 1564/2092 [26:40<08:59,  1.02s/it]\u001b[A\n","Steps:  75% 1565/2092 [26:41<08:56,  1.02s/it]\u001b[A\n","Steps:  75% 1566/2092 [26:42<08:56,  1.02s/it]\u001b[A\n","Steps:  75% 1567/2092 [26:43<08:54,  1.02s/it]\u001b[A\n","Steps:  75% 1568/2092 [26:44<08:53,  1.02s/it]\u001b[A\n","Steps:  75% 1569/2092 [26:45<08:52,  1.02s/it]\u001b[A\n","Steps:  75% 1570/2092 [26:46<08:51,  1.02s/it]\u001b[A\n","Steps:  75% 1571/2092 [26:47<08:50,  1.02s/it]\u001b[A\n","Steps:  75% 1572/2092 [26:48<08:49,  1.02s/it]\u001b[A\n","Steps:  75% 1573/2092 [26:49<08:50,  1.02s/it]\u001b[A\n","Steps:  75% 1574/2092 [26:50<08:50,  1.02s/it]\u001b[A\n","Steps:  75% 1575/2092 [26:51<08:49,  1.02s/it]\u001b[A\n","Steps:  75% 1576/2092 [26:52<08:48,  1.02s/it]\u001b[A\n","Steps:  75% 1577/2092 [26:53<08:45,  1.02s/it]\u001b[A\n","Steps:  75% 1578/2092 [26:54<08:46,  1.02s/it]\u001b[A\n","Steps:  75% 1579/2092 [26:55<08:45,  1.02s/it]\u001b[A\n","Steps:  76% 1580/2092 [26:56<08:42,  1.02s/it]\u001b[A\n","Steps:  76% 1581/2092 [26:57<08:39,  1.02s/it]\u001b[A\n","Steps:  76% 1582/2092 [26:58<08:39,  1.02s/it]\u001b[A\n","Steps:  76% 1583/2092 [26:59<08:39,  1.02s/it]\u001b[A\n","Steps:  76% 1584/2092 [27:00<08:38,  1.02s/it]\u001b[A\n","Steps:  76% 1585/2092 [27:01<08:35,  1.02s/it]\u001b[A\n","Steps:  76% 1586/2092 [27:02<08:33,  1.01s/it]\u001b[A\n","Steps:  76% 1587/2092 [27:03<08:33,  1.02s/it]\u001b[A\n","Steps:  76% 1588/2092 [27:04<08:32,  1.02s/it]\u001b[A\n","Steps:  76% 1589/2092 [27:05<08:33,  1.02s/it]\u001b[A\n","Steps:  76% 1590/2092 [27:06<08:32,  1.02s/it]\u001b[A\n","Steps:  76% 1591/2092 [27:07<08:30,  1.02s/it]\u001b[A\n","Steps:  76% 1592/2092 [27:08<08:28,  1.02s/it]\u001b[A\n","Steps:  76% 1593/2092 [27:09<08:29,  1.02s/it]\u001b[A\n","Steps:  76% 1594/2092 [27:10<08:28,  1.02s/it]\u001b[A\n","Steps:  76% 1595/2092 [27:12<08:25,  1.02s/it]\u001b[A\n","Steps:  76% 1596/2092 [27:13<08:25,  1.02s/it]\u001b[A\n","Steps:  76% 1597/2092 [27:14<08:25,  1.02s/it]\u001b[A\n","Steps:  76% 1598/2092 [27:15<08:26,  1.02s/it]\u001b[A\n","Steps:  76% 1599/2092 [27:16<08:24,  1.02s/it]\u001b[A\n","Steps:  76% 1600/2092 [27:17<08:24,  1.02s/it]\u001b[A\n","Steps:  77% 1601/2092 [27:18<08:23,  1.03s/it]\u001b[A\n","Steps:  77% 1602/2092 [27:19<08:19,  1.02s/it]\u001b[A\n","Steps:  77% 1603/2092 [27:20<08:18,  1.02s/it]\u001b[A\n","Steps:  77% 1604/2092 [27:21<08:18,  1.02s/it]\u001b[A\n","Steps:  77% 1605/2092 [27:22<08:17,  1.02s/it]\u001b[A\n","Steps:  77% 1606/2092 [27:23<08:15,  1.02s/it]\u001b[A\n","Steps:  77% 1607/2092 [27:24<08:14,  1.02s/it]\u001b[A\n","Steps:  77% 1608/2092 [27:25<08:14,  1.02s/it]\u001b[A\n","Steps:  77% 1609/2092 [27:26<08:12,  1.02s/it]\u001b[A\n","Steps:  77% 1610/2092 [27:27<08:12,  1.02s/it]\u001b[A\n","Steps:  77% 1611/2092 [27:28<08:10,  1.02s/it]\u001b[A\n","Steps:  77% 1612/2092 [27:29<08:10,  1.02s/it]\u001b[A\n","Steps:  77% 1613/2092 [27:30<08:10,  1.02s/it]\u001b[A\n","Steps:  77% 1614/2092 [27:31<08:08,  1.02s/it]\u001b[A\n","Steps:  77% 1615/2092 [27:32<08:06,  1.02s/it]\u001b[A\n","Steps:  77% 1616/2092 [27:33<08:04,  1.02s/it]\u001b[A\n","Steps:  77% 1617/2092 [27:34<08:00,  1.01s/it]\u001b[A\n","Steps:  77% 1618/2092 [27:35<08:00,  1.01s/it]\u001b[A\n","Steps:  77% 1619/2092 [27:36<08:00,  1.02s/it]\u001b[A\n","Steps:  77% 1620/2092 [27:37<07:58,  1.01s/it]\u001b[A\n","Steps:  77% 1621/2092 [27:38<07:59,  1.02s/it]\u001b[A\n","Steps:  78% 1622/2092 [27:39<07:58,  1.02s/it]\u001b[A\n","Steps:  78% 1623/2092 [27:40<07:59,  1.02s/it]\u001b[A\n","Steps:  78% 1624/2092 [27:41<07:59,  1.02s/it]\u001b[A\n","Steps:  78% 1625/2092 [27:42<07:58,  1.02s/it]\u001b[A\n","Steps:  78% 1626/2092 [27:43<07:57,  1.03s/it]\u001b[A\n","Steps:  78% 1627/2092 [27:44<07:54,  1.02s/it]\u001b[A\n","Steps:  78% 1628/2092 [27:45<07:52,  1.02s/it]\u001b[A\n","Steps:  78% 1629/2092 [27:46<07:51,  1.02s/it]\u001b[A\n","Steps:  78% 1630/2092 [27:47<07:51,  1.02s/it]\u001b[A\n","Steps:  78% 1631/2092 [27:48<07:49,  1.02s/it]\u001b[A\n","Steps:  78% 1632/2092 [27:49<07:50,  1.02s/it]\u001b[A\n","Steps:  78% 1633/2092 [27:50<07:47,  1.02s/it]\u001b[A\n","Steps:  78% 1634/2092 [27:51<07:46,  1.02s/it]\u001b[A\n","Steps:  78% 1635/2092 [27:52<07:45,  1.02s/it]\u001b[A\n","Steps:  78% 1636/2092 [27:53<07:45,  1.02s/it]\u001b[A\n","Steps:  78% 1637/2092 [27:54<07:45,  1.02s/it]\u001b[A\n","Steps:  78% 1638/2092 [27:55<07:42,  1.02s/it]\u001b[A\n","Steps:  78% 1639/2092 [27:56<07:43,  1.02s/it]\u001b[A\n","Steps:  78% 1640/2092 [27:57<07:43,  1.02s/it]\u001b[A\n","Steps:  78% 1641/2092 [27:58<07:41,  1.02s/it]\u001b[A\n","Steps:  78% 1642/2092 [27:59<07:38,  1.02s/it]\u001b[A\n","Steps:  79% 1643/2092 [28:00<07:37,  1.02s/it]\u001b[A\n","Steps:  79% 1644/2092 [28:02<07:37,  1.02s/it]\u001b[A\n","Steps:  79% 1645/2092 [28:03<07:34,  1.02s/it]\u001b[A\n","Steps:  79% 1646/2092 [28:04<07:33,  1.02s/it]\u001b[A\n","Steps:  79% 1647/2092 [28:05<07:32,  1.02s/it]\u001b[A\n","Steps:  79% 1648/2092 [28:06<07:31,  1.02s/it]\u001b[A\n","Steps:  79% 1649/2092 [28:07<07:30,  1.02s/it]\u001b[A\n","Steps:  79% 1650/2092 [28:08<07:30,  1.02s/it]\u001b[A\n","Steps:  79% 1651/2092 [28:09<07:31,  1.02s/it]\u001b[A\n","Steps:  79% 1652/2092 [28:10<07:29,  1.02s/it]\u001b[A\n","Steps:  79% 1653/2092 [28:11<07:29,  1.02s/it]\u001b[A\n","Steps:  79% 1654/2092 [28:12<07:27,  1.02s/it]\u001b[A\n","Steps:  79% 1655/2092 [28:13<07:24,  1.02s/it]\u001b[A\n","Steps:  79% 1656/2092 [28:14<07:23,  1.02s/it]\u001b[A\n","Steps:  79% 1657/2092 [28:15<07:21,  1.01s/it]\u001b[A\n","Steps:  79% 1658/2092 [28:16<07:19,  1.01s/it]\u001b[A\n","Steps:  79% 1659/2092 [28:17<07:19,  1.01s/it]\u001b[A\n","Steps:  79% 1660/2092 [28:18<07:19,  1.02s/it]\u001b[A\n","Steps:  79% 1661/2092 [28:19<07:19,  1.02s/it]\u001b[A\n","Steps:  79% 1662/2092 [28:20<07:17,  1.02s/it]\u001b[A\n","Steps:  79% 1663/2092 [28:21<07:17,  1.02s/it]\u001b[A\n","Steps:  80% 1664/2092 [28:22<07:17,  1.02s/it]\u001b[A\n","Steps:  80% 1665/2092 [28:23<07:17,  1.03s/it]\u001b[A\n","Steps:  80% 1666/2092 [28:24<07:16,  1.03s/it]\u001b[A\n","Steps:  80% 1667/2092 [28:25<07:13,  1.02s/it]\u001b[A\n","Steps:  80% 1668/2092 [28:26<07:13,  1.02s/it]\u001b[A\n","Steps:  80% 1669/2092 [28:27<07:13,  1.02s/it]\u001b[A\n","Steps:  80% 1670/2092 [28:28<07:10,  1.02s/it]\u001b[A\n","Steps:  80% 1671/2092 [28:29<07:10,  1.02s/it]\u001b[A\n","Steps:  80% 1672/2092 [28:30<07:08,  1.02s/it]\u001b[A\n","Steps:  80% 1673/2092 [28:31<07:08,  1.02s/it]\u001b[A\n","Steps:  80% 1674/2092 [28:32<07:08,  1.02s/it]\u001b[A\n","Steps:  80% 1675/2092 [28:33<07:06,  1.02s/it]\u001b[A\n","Steps:  80% 1676/2092 [28:34<07:05,  1.02s/it]\u001b[A\n","Steps:  80% 1677/2092 [28:35<07:04,  1.02s/it]\u001b[A\n","Steps:  80% 1678/2092 [28:36<07:03,  1.02s/it]\u001b[A\n","Steps:  80% 1679/2092 [28:37<07:02,  1.02s/it]\u001b[A\n","Steps:  80% 1680/2092 [28:38<07:01,  1.02s/it]\u001b[A\n","Steps:  80% 1681/2092 [28:39<07:00,  1.02s/it]\u001b[A\n","Steps:  80% 1682/2092 [28:40<06:59,  1.02s/it]\u001b[A\n","Steps:  80% 1683/2092 [28:41<06:58,  1.02s/it]\u001b[A\n","Steps:  80% 1684/2092 [28:42<06:57,  1.02s/it]\u001b[A\n","Steps:  81% 1685/2092 [28:43<06:57,  1.03s/it]\u001b[A\n","Steps:  81% 1686/2092 [28:44<06:56,  1.02s/it]\u001b[A\n","Steps:  81% 1687/2092 [28:45<06:54,  1.02s/it]\u001b[A\n","Steps:  81% 1688/2092 [28:46<06:53,  1.02s/it]\u001b[A\n","Steps:  81% 1689/2092 [28:47<06:56,  1.03s/it]\u001b[A\n","Steps:  81% 1690/2092 [28:49<06:52,  1.03s/it]\u001b[A\n","Steps:  81% 1691/2092 [28:50<06:51,  1.03s/it]\u001b[A\n","Steps:  81% 1692/2092 [28:51<06:49,  1.02s/it]\u001b[A\n","Steps:  81% 1693/2092 [28:52<06:47,  1.02s/it]\u001b[A\n","Steps:  81% 1694/2092 [28:53<06:46,  1.02s/it]\u001b[A\n","Steps:  81% 1695/2092 [28:54<06:43,  1.02s/it]\u001b[A\n","Steps:  81% 1696/2092 [28:55<06:43,  1.02s/it]\u001b[A\n","Steps:  81% 1697/2092 [28:56<06:43,  1.02s/it]\u001b[A\n","Steps:  81% 1698/2092 [28:57<06:40,  1.02s/it]\u001b[A\n","Steps:  81% 1699/2092 [28:58<06:40,  1.02s/it]\u001b[A\n","Steps:  81% 1700/2092 [28:59<06:40,  1.02s/it]\u001b[A\n","Steps:  81% 1701/2092 [29:00<06:38,  1.02s/it]\u001b[A\n","Steps:  81% 1702/2092 [29:01<06:36,  1.02s/it]\u001b[A\n","Steps:  81% 1703/2092 [29:02<06:36,  1.02s/it]\u001b[A\n","Steps:  81% 1704/2092 [29:03<06:33,  1.01s/it]\u001b[A\n","Steps:  82% 1705/2092 [29:04<06:34,  1.02s/it]\u001b[A\n","Steps:  82% 1706/2092 [29:05<06:33,  1.02s/it]\u001b[A\n","Steps:  82% 1707/2092 [29:06<06:32,  1.02s/it]\u001b[A\n","Steps:  82% 1708/2092 [29:07<06:31,  1.02s/it]\u001b[A\n","Steps:  82% 1709/2092 [29:08<06:30,  1.02s/it]\u001b[A\n","Steps:  82% 1710/2092 [29:09<06:30,  1.02s/it]\u001b[A\n","Steps:  82% 1711/2092 [29:10<06:29,  1.02s/it]\u001b[A\n","Steps:  82% 1712/2092 [29:11<06:29,  1.03s/it]\u001b[A\n","Steps:  82% 1713/2092 [29:12<06:27,  1.02s/it]\u001b[A\n","Steps:  82% 1714/2092 [29:13<06:24,  1.02s/it]\u001b[A\n","Steps:  82% 1715/2092 [29:14<06:22,  1.02s/it]\u001b[A\n","Steps:  82% 1716/2092 [29:15<06:21,  1.02s/it]\u001b[A\n","Steps:  82% 1717/2092 [29:16<06:21,  1.02s/it]\u001b[A\n","Steps:  82% 1718/2092 [29:17<06:20,  1.02s/it]\u001b[A\n","Steps:  82% 1719/2092 [29:18<06:20,  1.02s/it]\u001b[A\n","Steps:  82% 1720/2092 [29:19<06:20,  1.02s/it]\u001b[A\n","Steps:  82% 1721/2092 [29:20<06:19,  1.02s/it]\u001b[A\n","Steps:  82% 1722/2092 [29:21<06:19,  1.02s/it]\u001b[A\n","Steps:  82% 1723/2092 [29:22<06:17,  1.02s/it]\u001b[A\n","Steps:  82% 1724/2092 [29:23<06:16,  1.02s/it]\u001b[A\n","Steps:  82% 1725/2092 [29:24<06:15,  1.02s/it]\u001b[A\n","Steps:  83% 1726/2092 [29:25<06:13,  1.02s/it]\u001b[A\n","Steps:  83% 1727/2092 [29:26<06:13,  1.02s/it]\u001b[A\n","Steps:  83% 1728/2092 [29:27<06:12,  1.02s/it]\u001b[A\n","Steps:  83% 1729/2092 [29:28<06:10,  1.02s/it]\u001b[A\n","Steps:  83% 1730/2092 [29:29<06:09,  1.02s/it]\u001b[A\n","Steps:  83% 1731/2092 [29:30<06:08,  1.02s/it]\u001b[A\n","Steps:  83% 1732/2092 [29:31<06:06,  1.02s/it]\u001b[A\n","Steps:  83% 1733/2092 [29:32<06:05,  1.02s/it]\u001b[A\n","Steps:  83% 1734/2092 [29:33<06:05,  1.02s/it]\u001b[A\n","Steps:  83% 1735/2092 [29:34<06:05,  1.02s/it]\u001b[A\n","Steps:  83% 1736/2092 [29:35<06:05,  1.03s/it]\u001b[A\n","Steps:  83% 1737/2092 [29:36<06:03,  1.02s/it]\u001b[A\n","Steps:  83% 1738/2092 [29:37<06:01,  1.02s/it]\u001b[A\n","Steps:  83% 1739/2092 [29:39<06:00,  1.02s/it]\u001b[A\n","Steps:  83% 1740/2092 [29:40<05:59,  1.02s/it]\u001b[A\n","Steps:  83% 1741/2092 [29:41<05:57,  1.02s/it]\u001b[A\n","Steps:  83% 1742/2092 [29:42<05:57,  1.02s/it]\u001b[A\n","Steps:  83% 1743/2092 [29:43<05:55,  1.02s/it]\u001b[A\n","Steps:  83% 1744/2092 [29:44<05:54,  1.02s/it]\u001b[A\n","Steps:  83% 1745/2092 [29:45<05:53,  1.02s/it]\u001b[A\n","Steps:  83% 1746/2092 [29:46<05:52,  1.02s/it]\u001b[A\n","Steps:  84% 1747/2092 [29:47<05:50,  1.02s/it]\u001b[A\n","Steps:  84% 1748/2092 [29:48<05:49,  1.02s/it]\u001b[A\n","Steps:  84% 1749/2092 [29:49<05:48,  1.01s/it]\u001b[A\n","Steps:  84% 1750/2092 [29:50<05:47,  1.02s/it]\u001b[A\n","Steps:  84% 1751/2092 [29:51<05:47,  1.02s/it]\u001b[A\n","Steps:  84% 1752/2092 [29:52<05:44,  1.01s/it]\u001b[A\n","Steps:  84% 1753/2092 [29:53<05:45,  1.02s/it]\u001b[A\n","Steps:  84% 1754/2092 [29:54<05:44,  1.02s/it]\u001b[A\n","Steps:  84% 1755/2092 [29:55<05:43,  1.02s/it]\u001b[A\n","Steps:  84% 1756/2092 [29:56<05:41,  1.02s/it]\u001b[A\n","Steps:  84% 1757/2092 [29:57<05:41,  1.02s/it]\u001b[A\n","Steps:  84% 1758/2092 [29:58<05:41,  1.02s/it]\u001b[A\n","Steps:  84% 1759/2092 [29:59<05:40,  1.02s/it]\u001b[A\n","Steps:  84% 1760/2092 [30:00<05:39,  1.02s/it]\u001b[A\n","Steps:  84% 1761/2092 [30:01<05:39,  1.02s/it]\u001b[A\n","Steps:  84% 1762/2092 [30:02<05:38,  1.03s/it]\u001b[A\n","Steps:  84% 1763/2092 [30:03<05:37,  1.03s/it]\u001b[A\n","Steps:  84% 1764/2092 [30:04<05:36,  1.02s/it]\u001b[A\n","Steps:  84% 1765/2092 [30:05<05:38,  1.04s/it]\u001b[A\n","Steps:  84% 1766/2092 [30:06<05:35,  1.03s/it]\u001b[A\n","Steps:  84% 1767/2092 [30:07<05:33,  1.03s/it]\u001b[A\n","Steps:  85% 1768/2092 [30:08<05:32,  1.03s/it]\u001b[A\n","Steps:  85% 1769/2092 [30:09<05:31,  1.03s/it]\u001b[A\n","Steps:  85% 1770/2092 [30:10<05:30,  1.03s/it]\u001b[A\n","Steps:  85% 1771/2092 [30:11<05:28,  1.02s/it]\u001b[A\n","Steps:  85% 1772/2092 [30:12<05:26,  1.02s/it]\u001b[A\n","Steps:  85% 1773/2092 [30:13<05:25,  1.02s/it]\u001b[A\n","Steps:  85% 1774/2092 [30:14<05:25,  1.02s/it]\u001b[A\n","Steps:  85% 1775/2092 [30:15<05:23,  1.02s/it]\u001b[A\n","Steps:  85% 1776/2092 [30:16<05:22,  1.02s/it]\u001b[A\n","Steps:  85% 1777/2092 [30:17<05:20,  1.02s/it]\u001b[A\n","Steps:  85% 1778/2092 [30:18<05:20,  1.02s/it]\u001b[A\n","Steps:  85% 1779/2092 [30:19<05:20,  1.02s/it]\u001b[A\n","Steps:  85% 1780/2092 [30:20<05:19,  1.03s/it]\u001b[A\n","Steps:  85% 1781/2092 [30:21<05:18,  1.02s/it]\u001b[A\n","Steps:  85% 1782/2092 [30:22<05:16,  1.02s/it]\u001b[A\n","Steps:  85% 1783/2092 [30:23<05:15,  1.02s/it]\u001b[A\n","Steps:  85% 1784/2092 [30:24<05:14,  1.02s/it]\u001b[A\n","Steps:  85% 1785/2092 [30:25<05:12,  1.02s/it]\u001b[A\n","Steps:  85% 1786/2092 [30:26<05:10,  1.02s/it]\u001b[A\n","Steps:  85% 1787/2092 [30:28<05:09,  1.02s/it]\u001b[A\n","Steps:  85% 1788/2092 [30:29<05:10,  1.02s/it]\u001b[A\n","Steps:  86% 1789/2092 [30:30<05:10,  1.02s/it]\u001b[A\n","Steps:  86% 1790/2092 [30:31<05:08,  1.02s/it]\u001b[A\n","Steps:  86% 1791/2092 [30:32<05:07,  1.02s/it]\u001b[A\n","Steps:  86% 1792/2092 [30:33<05:07,  1.02s/it]\u001b[A\n","Steps:  86% 1793/2092 [30:34<05:05,  1.02s/it]\u001b[A\n","Steps:  86% 1794/2092 [30:35<05:03,  1.02s/it]\u001b[A\n","Steps:  86% 1795/2092 [30:36<05:02,  1.02s/it]\u001b[A\n","Steps:  86% 1796/2092 [30:37<05:00,  1.02s/it]\u001b[A\n","Steps:  86% 1797/2092 [30:38<05:00,  1.02s/it]\u001b[A\n","Steps:  86% 1798/2092 [30:39<04:59,  1.02s/it]\u001b[A\n","Steps:  86% 1799/2092 [30:40<04:57,  1.02s/it]\u001b[A\n","Steps:  86% 1800/2092 [30:41<04:57,  1.02s/it]\u001b[A\n","Steps:  86% 1801/2092 [30:42<04:56,  1.02s/it]\u001b[A\n","Steps:  86% 1802/2092 [30:43<04:55,  1.02s/it]\u001b[A\n","Steps:  86% 1803/2092 [30:44<04:55,  1.02s/it]\u001b[A\n","Steps:  86% 1804/2092 [30:45<04:54,  1.02s/it]\u001b[A\n","Steps:  86% 1805/2092 [30:46<04:52,  1.02s/it]\u001b[A\n","Steps:  86% 1806/2092 [30:47<04:51,  1.02s/it]\u001b[A\n","Steps:  86% 1807/2092 [30:48<04:50,  1.02s/it]\u001b[A\n","Steps:  86% 1808/2092 [30:49<04:48,  1.02s/it]\u001b[A\n","Steps:  86% 1809/2092 [30:50<04:47,  1.02s/it]\u001b[A\n","Steps:  87% 1810/2092 [30:51<04:47,  1.02s/it]\u001b[A\n","Steps:  87% 1811/2092 [30:52<04:46,  1.02s/it]\u001b[A\n","Steps:  87% 1812/2092 [30:53<04:45,  1.02s/it]\u001b[A\n","Steps:  87% 1813/2092 [30:54<04:45,  1.02s/it]\u001b[A\n","Steps:  87% 1814/2092 [30:55<04:44,  1.02s/it]\u001b[A\n","Steps:  87% 1815/2092 [30:56<04:43,  1.02s/it]\u001b[A\n","Steps:  87% 1816/2092 [30:57<04:41,  1.02s/it]\u001b[A\n","Steps:  87% 1817/2092 [30:58<04:39,  1.02s/it]\u001b[A\n","Steps:  87% 1818/2092 [30:59<04:39,  1.02s/it]\u001b[A\n","Steps:  87% 1819/2092 [31:00<04:39,  1.02s/it]\u001b[A\n","Steps:  87% 1820/2092 [31:01<04:36,  1.02s/it]\u001b[A\n","Steps:  87% 1821/2092 [31:02<04:35,  1.02s/it]\u001b[A\n","Steps:  87% 1822/2092 [31:03<04:35,  1.02s/it]\u001b[A\n","Steps:  87% 1823/2092 [31:04<04:34,  1.02s/it]\u001b[A\n","Steps:  87% 1824/2092 [31:05<04:33,  1.02s/it]\u001b[A\n","Steps:  87% 1825/2092 [31:06<04:31,  1.02s/it]\u001b[A\n","Steps:  87% 1826/2092 [31:07<04:31,  1.02s/it]\u001b[A\n","Steps:  87% 1827/2092 [31:08<04:30,  1.02s/it]\u001b[A\n","Steps:  87% 1828/2092 [31:09<04:29,  1.02s/it]\u001b[A\n","Steps:  87% 1829/2092 [31:10<04:28,  1.02s/it]\u001b[A\n","Steps:  87% 1830/2092 [31:11<04:27,  1.02s/it]\u001b[A\n","Steps:  88% 1831/2092 [31:12<04:26,  1.02s/it]\u001b[A\n","Steps:  88% 1832/2092 [31:13<04:25,  1.02s/it]\u001b[A\n","Steps:  88% 1833/2092 [31:14<04:24,  1.02s/it]\u001b[A\n","Steps:  88% 1834/2092 [31:15<04:22,  1.02s/it]\u001b[A\n","Steps:  88% 1835/2092 [31:16<04:21,  1.02s/it]\u001b[A\n","Steps:  88% 1836/2092 [31:17<04:20,  1.02s/it]\u001b[A\n","Steps:  88% 1837/2092 [31:19<04:20,  1.02s/it]\u001b[A\n","Steps:  88% 1838/2092 [31:20<04:19,  1.02s/it]\u001b[A\n","Steps:  88% 1839/2092 [31:21<04:19,  1.02s/it]\u001b[A\n","Steps:  88% 1840/2092 [31:22<04:17,  1.02s/it]\u001b[A\n","Steps:  88% 1841/2092 [31:23<04:16,  1.02s/it]\u001b[A\n","Steps:  88% 1842/2092 [31:24<04:15,  1.02s/it]\u001b[A\n","Steps:  88% 1843/2092 [31:25<04:15,  1.02s/it]\u001b[A\n","Steps:  88% 1844/2092 [31:26<04:14,  1.03s/it]\u001b[A\n","Steps:  88% 1845/2092 [31:27<04:12,  1.02s/it]\u001b[A\n","Steps:  88% 1846/2092 [31:28<04:11,  1.02s/it]\u001b[A\n","Steps:  88% 1847/2092 [31:29<04:10,  1.02s/it]\u001b[A\n","Steps:  88% 1848/2092 [31:30<04:08,  1.02s/it]\u001b[A\n","Steps:  88% 1849/2092 [31:31<04:08,  1.02s/it]\u001b[A\n","Steps:  88% 1850/2092 [31:32<04:06,  1.02s/it]\u001b[A\n","Steps:  88% 1851/2092 [31:33<04:06,  1.02s/it]\u001b[A\n","Steps:  89% 1852/2092 [31:34<04:04,  1.02s/it]\u001b[A\n","Steps:  89% 1853/2092 [31:35<04:03,  1.02s/it]\u001b[A\n","Steps:  89% 1854/2092 [31:36<04:02,  1.02s/it]\u001b[A\n","Steps:  89% 1855/2092 [31:37<04:02,  1.02s/it]\u001b[A\n","Steps:  89% 1856/2092 [31:38<04:01,  1.02s/it]\u001b[A\n","Steps:  89% 1857/2092 [31:39<04:00,  1.02s/it]\u001b[A\n","Steps:  89% 1858/2092 [31:40<03:59,  1.02s/it]\u001b[A\n","Steps:  89% 1859/2092 [31:41<03:58,  1.02s/it]\u001b[A\n","Steps:  89% 1860/2092 [31:42<03:57,  1.03s/it]\u001b[A\n","Steps:  89% 1861/2092 [31:43<03:57,  1.03s/it]\u001b[A\n","Steps:  89% 1862/2092 [31:44<03:56,  1.03s/it]\u001b[A\n","Steps:  89% 1863/2092 [31:45<03:55,  1.03s/it]\u001b[A\n","Steps:  89% 1864/2092 [31:46<03:54,  1.03s/it]\u001b[A\n","Steps:  89% 1865/2092 [31:47<03:53,  1.03s/it]\u001b[A\n","Steps:  89% 1866/2092 [31:48<03:52,  1.03s/it]\u001b[A\n","Steps:  89% 1867/2092 [31:49<03:50,  1.03s/it]\u001b[A\n","Steps:  89% 1868/2092 [31:50<03:49,  1.03s/it]\u001b[A\n","Steps:  89% 1869/2092 [31:51<03:47,  1.02s/it]\u001b[A\n","Steps:  89% 1870/2092 [31:52<03:47,  1.02s/it]\u001b[A\n","Steps:  89% 1871/2092 [31:53<03:45,  1.02s/it]\u001b[A\n","Steps:  89% 1872/2092 [31:54<03:45,  1.02s/it]\u001b[A\n","Steps:  90% 1873/2092 [31:55<03:44,  1.03s/it]\u001b[A\n","Steps:  90% 1874/2092 [31:56<03:43,  1.03s/it]\u001b[A\n","Steps:  90% 1875/2092 [31:57<03:42,  1.02s/it]\u001b[A\n","Steps:  90% 1876/2092 [31:58<03:41,  1.02s/it]\u001b[A\n","Steps:  90% 1877/2092 [31:59<03:40,  1.02s/it]\u001b[A\n","Steps:  90% 1878/2092 [32:00<03:38,  1.02s/it]\u001b[A\n","Steps:  90% 1879/2092 [32:02<03:37,  1.02s/it]\u001b[A\n","Steps:  90% 1880/2092 [32:03<03:36,  1.02s/it]\u001b[A\n","Steps:  90% 1881/2092 [32:04<03:35,  1.02s/it]\u001b[A\n","Steps:  90% 1882/2092 [32:05<03:33,  1.02s/it]\u001b[A\n","Steps:  90% 1883/2092 [32:06<03:32,  1.02s/it]\u001b[A\n","Steps:  90% 1884/2092 [32:07<03:32,  1.02s/it]\u001b[A\n","Steps:  90% 1885/2092 [32:08<03:31,  1.02s/it]\u001b[A\n","Steps:  90% 1886/2092 [32:09<03:31,  1.02s/it]\u001b[A\n","Steps:  90% 1887/2092 [32:10<03:29,  1.02s/it]\u001b[A\n","Steps:  90% 1888/2092 [32:11<03:28,  1.02s/it]\u001b[A\n","Steps:  90% 1889/2092 [32:12<03:27,  1.02s/it]\u001b[A\n","Steps:  90% 1890/2092 [32:13<03:26,  1.02s/it]\u001b[A\n","Steps:  90% 1891/2092 [32:14<03:25,  1.02s/it]\u001b[A\n","Steps:  90% 1892/2092 [32:15<03:23,  1.02s/it]\u001b[A\n","Steps:  90% 1893/2092 [32:16<03:23,  1.02s/it]\u001b[A\n","Steps:  91% 1894/2092 [32:17<03:22,  1.02s/it]\u001b[A\n","Steps:  91% 1895/2092 [32:18<03:21,  1.02s/it]\u001b[A\n","Steps:  91% 1896/2092 [32:19<03:19,  1.02s/it]\u001b[A\n","Steps:  91% 1897/2092 [32:20<03:18,  1.02s/it]\u001b[A\n","Steps:  91% 1898/2092 [32:21<03:18,  1.02s/it]\u001b[A\n","Steps:  91% 1899/2092 [32:22<03:17,  1.02s/it]\u001b[A\n","Steps:  91% 1900/2092 [32:23<03:16,  1.02s/it]\u001b[A\n","Steps:  91% 1901/2092 [32:24<03:15,  1.03s/it]\u001b[A\n","Steps:  91% 1902/2092 [32:25<03:14,  1.02s/it]\u001b[A\n","Steps:  91% 1903/2092 [32:26<03:13,  1.02s/it]\u001b[A\n","Steps:  91% 1904/2092 [32:27<03:12,  1.02s/it]\u001b[A\n","Steps:  91% 1905/2092 [32:28<03:11,  1.02s/it]\u001b[A\n","Steps:  91% 1906/2092 [32:29<03:10,  1.02s/it]\u001b[A\n","Steps:  91% 1907/2092 [32:30<03:09,  1.02s/it]\u001b[A\n","Steps:  91% 1908/2092 [32:31<03:08,  1.02s/it]\u001b[A\n","Steps:  91% 1909/2092 [32:32<03:07,  1.02s/it]\u001b[A\n","Steps:  91% 1910/2092 [32:33<03:05,  1.02s/it]\u001b[A\n","Steps:  91% 1911/2092 [32:34<03:05,  1.02s/it]\u001b[A\n","Steps:  91% 1912/2092 [32:35<03:03,  1.02s/it]\u001b[A\n","Steps:  91% 1913/2092 [32:36<03:02,  1.02s/it]\u001b[A\n","Steps:  91% 1914/2092 [32:37<03:01,  1.02s/it]\u001b[A\n","Steps:  92% 1915/2092 [32:38<03:00,  1.02s/it]\u001b[A\n","Steps:  92% 1916/2092 [32:39<02:59,  1.02s/it]\u001b[A\n","Steps:  92% 1917/2092 [32:40<02:58,  1.02s/it]\u001b[A\n","Steps:  92% 1918/2092 [32:41<02:57,  1.02s/it]\u001b[A\n","Steps:  92% 1919/2092 [32:42<02:56,  1.02s/it]\u001b[A\n","Steps:  92% 1920/2092 [32:43<02:55,  1.02s/it]\u001b[A\n","Steps:  92% 1921/2092 [32:44<02:54,  1.02s/it]\u001b[A\n","Steps:  92% 1922/2092 [32:45<02:53,  1.02s/it]\u001b[A\n","Steps:  92% 1923/2092 [32:46<02:52,  1.02s/it]\u001b[A\n","Steps:  92% 1924/2092 [32:47<02:51,  1.02s/it]\u001b[A\n","Steps:  92% 1925/2092 [32:49<02:50,  1.02s/it]\u001b[A\n","Steps:  92% 1926/2092 [32:50<02:49,  1.02s/it]\u001b[A\n","Steps:  92% 1927/2092 [32:51<02:48,  1.02s/it]\u001b[A\n","Steps:  92% 1928/2092 [32:52<02:47,  1.02s/it]\u001b[A\n","Steps:  92% 1929/2092 [32:53<02:45,  1.02s/it]\u001b[A\n","Steps:  92% 1930/2092 [32:54<02:44,  1.02s/it]\u001b[A\n","Steps:  92% 1931/2092 [32:55<02:44,  1.02s/it]\u001b[A\n","Steps:  92% 1932/2092 [32:56<02:43,  1.02s/it]\u001b[A\n","Steps:  92% 1933/2092 [32:57<02:42,  1.02s/it]\u001b[A\n","Steps:  92% 1934/2092 [32:58<02:41,  1.02s/it]\u001b[A\n","Steps:  92% 1935/2092 [32:59<02:40,  1.02s/it]\u001b[A\n","Steps:  93% 1936/2092 [33:00<02:39,  1.02s/it]\u001b[A\n","Steps:  93% 1937/2092 [33:01<02:37,  1.02s/it]\u001b[A\n","Steps:  93% 1938/2092 [33:02<02:36,  1.02s/it]\u001b[A\n","Steps:  93% 1939/2092 [33:03<02:35,  1.01s/it]\u001b[A\n","Steps:  93% 1940/2092 [33:04<02:34,  1.02s/it]\u001b[A\n","Steps:  93% 1941/2092 [33:05<02:33,  1.01s/it]\u001b[A\n","Steps:  93% 1942/2092 [33:06<02:32,  1.02s/it]\u001b[A\n","Steps:  93% 1943/2092 [33:07<02:31,  1.02s/it]\u001b[A\n","Steps:  93% 1944/2092 [33:08<02:30,  1.02s/it]\u001b[A\n","Steps:  93% 1945/2092 [33:09<02:30,  1.02s/it]\u001b[A\n","Steps:  93% 1946/2092 [33:10<02:29,  1.02s/it]\u001b[A\n","Steps:  93% 1947/2092 [33:11<02:28,  1.02s/it]\u001b[A\n","Steps:  93% 1948/2092 [33:12<02:26,  1.02s/it]\u001b[A\n","Steps:  93% 1949/2092 [33:13<02:25,  1.02s/it]\u001b[A\n","Steps:  93% 1950/2092 [33:14<02:24,  1.02s/it]\u001b[A\n","Steps:  93% 1951/2092 [33:15<02:23,  1.02s/it]\u001b[A\n","Steps:  93% 1952/2092 [33:16<02:23,  1.02s/it]\u001b[A\n","Steps:  93% 1953/2092 [33:17<02:22,  1.02s/it]\u001b[A\n","Steps:  93% 1954/2092 [33:18<02:21,  1.03s/it]\u001b[A\n","Steps:  93% 1955/2092 [33:19<02:20,  1.02s/it]\u001b[A\n","Steps:  93% 1956/2092 [33:20<02:18,  1.02s/it]\u001b[A\n","Steps:  94% 1957/2092 [33:21<02:17,  1.02s/it]\u001b[A\n","Steps:  94% 1958/2092 [33:22<02:16,  1.02s/it]\u001b[A\n","Steps:  94% 1959/2092 [33:23<02:15,  1.02s/it]\u001b[A\n","Steps:  94% 1960/2092 [33:24<02:14,  1.02s/it]\u001b[A\n","Steps:  94% 1961/2092 [33:25<02:13,  1.02s/it]\u001b[A\n","Steps:  94% 1962/2092 [33:26<02:12,  1.02s/it]\u001b[A\n","Steps:  94% 1963/2092 [33:27<02:11,  1.02s/it]\u001b[A\n","Steps:  94% 1964/2092 [33:28<02:10,  1.02s/it]\u001b[A\n","Steps:  94% 1965/2092 [33:29<02:09,  1.02s/it]\u001b[A\n","Steps:  94% 1966/2092 [33:30<02:08,  1.02s/it]\u001b[A\n","Steps:  94% 1967/2092 [33:31<02:07,  1.02s/it]\u001b[A\n","Steps:  94% 1968/2092 [33:32<02:06,  1.02s/it]\u001b[A\n","Steps:  94% 1969/2092 [33:33<02:05,  1.02s/it]\u001b[A\n","Steps:  94% 1970/2092 [33:34<02:04,  1.02s/it]\u001b[A\n","Steps:  94% 1971/2092 [33:35<02:03,  1.02s/it]\u001b[A\n","Steps:  94% 1972/2092 [33:36<02:02,  1.02s/it]\u001b[A\n","Steps:  94% 1973/2092 [33:37<02:02,  1.03s/it]\u001b[A\n","Steps:  94% 1974/2092 [33:39<02:00,  1.02s/it]\u001b[A\n","Steps:  94% 1975/2092 [33:40<01:59,  1.02s/it]\u001b[A\n","Steps:  94% 1976/2092 [33:41<01:58,  1.02s/it]\u001b[A\n","Steps:  95% 1977/2092 [33:42<01:57,  1.02s/it]\u001b[A\n","Steps:  95% 1978/2092 [33:43<01:56,  1.02s/it]\u001b[A\n","Steps:  95% 1979/2092 [33:44<01:55,  1.02s/it]\u001b[A\n","Steps:  95% 1980/2092 [33:45<01:54,  1.02s/it]\u001b[A\n","Steps:  95% 1981/2092 [33:46<01:53,  1.02s/it]\u001b[A\n","Steps:  95% 1982/2092 [33:47<01:52,  1.02s/it]\u001b[A\n","Steps:  95% 1983/2092 [33:48<01:51,  1.02s/it]\u001b[A\n","Steps:  95% 1984/2092 [33:49<01:50,  1.02s/it]\u001b[A\n","Steps:  95% 1985/2092 [33:50<01:48,  1.02s/it]\u001b[A\n","Steps:  95% 1986/2092 [33:51<01:48,  1.02s/it]\u001b[A\n","Steps:  95% 1987/2092 [33:52<01:46,  1.02s/it]\u001b[A\n","Steps:  95% 1988/2092 [33:53<01:45,  1.02s/it]\u001b[A\n","Steps:  95% 1989/2092 [33:54<01:44,  1.02s/it]\u001b[A\n","Steps:  95% 1990/2092 [33:55<01:43,  1.02s/it]\u001b[A\n","Steps:  95% 1991/2092 [33:56<01:42,  1.02s/it]\u001b[A\n","Steps:  95% 1992/2092 [33:57<01:41,  1.02s/it]\u001b[A\n","Steps:  95% 1993/2092 [33:58<01:40,  1.02s/it]\u001b[A\n","Steps:  95% 1994/2092 [33:59<01:39,  1.02s/it]\u001b[A\n","Steps:  95% 1995/2092 [34:00<01:38,  1.01s/it]\u001b[A\n","Steps:  95% 1996/2092 [34:01<01:37,  1.02s/it]\u001b[A\n","Steps:  95% 1997/2092 [34:02<01:36,  1.02s/it]\u001b[A\n","Steps:  96% 1998/2092 [34:03<01:35,  1.02s/it]\u001b[A\n","Steps:  96% 1999/2092 [34:04<01:34,  1.02s/it]\u001b[A\n","Steps:  96% 2000/2092 [34:05<01:33,  1.02s/it]\u001b[A11/24/2024 17:40:25 - INFO - train_eval -   *****0 - 2000: loss: 1.7908519506454468 *****\n","\n","Steps:  96% 2001/2092 [34:06<01:32,  1.02s/it]\u001b[A\n","Steps:  96% 2002/2092 [34:07<01:31,  1.02s/it]\u001b[A\n","Steps:  96% 2003/2092 [34:08<01:31,  1.02s/it]\u001b[A\n","Steps:  96% 2004/2092 [34:09<01:29,  1.02s/it]\u001b[A\n","Steps:  96% 2005/2092 [34:10<01:28,  1.02s/it]\u001b[A\n","Steps:  96% 2006/2092 [34:11<01:27,  1.02s/it]\u001b[A\n","Steps:  96% 2007/2092 [34:12<01:26,  1.02s/it]\u001b[A\n","Steps:  96% 2008/2092 [34:13<01:26,  1.02s/it]\u001b[A\n","Steps:  96% 2009/2092 [34:14<01:25,  1.02s/it]\u001b[A\n","Steps:  96% 2010/2092 [34:15<01:24,  1.03s/it]\u001b[A\n","Steps:  96% 2011/2092 [34:16<01:23,  1.03s/it]\u001b[A\n","Steps:  96% 2012/2092 [34:17<01:22,  1.03s/it]\u001b[A\n","Steps:  96% 2013/2092 [34:18<01:20,  1.02s/it]\u001b[A\n","Steps:  96% 2014/2092 [34:19<01:19,  1.02s/it]\u001b[A\n","Steps:  96% 2015/2092 [34:20<01:18,  1.02s/it]\u001b[A\n","Steps:  96% 2016/2092 [34:21<01:17,  1.02s/it]\u001b[A\n","Steps:  96% 2017/2092 [34:22<01:16,  1.02s/it]\u001b[A\n","Steps:  96% 2018/2092 [34:23<01:15,  1.02s/it]\u001b[A\n","Steps:  97% 2019/2092 [34:24<01:14,  1.02s/it]\u001b[A\n","Steps:  97% 2020/2092 [34:25<01:13,  1.02s/it]\u001b[A\n","Steps:  97% 2021/2092 [34:26<01:12,  1.02s/it]\u001b[A\n","Steps:  97% 2022/2092 [34:27<01:11,  1.03s/it]\u001b[A\n","Steps:  97% 2023/2092 [34:29<01:10,  1.02s/it]\u001b[A\n","Steps:  97% 2024/2092 [34:30<01:09,  1.02s/it]\u001b[A\n","Steps:  97% 2025/2092 [34:31<01:08,  1.02s/it]\u001b[A\n","Steps:  97% 2026/2092 [34:32<01:07,  1.02s/it]\u001b[A\n","Steps:  97% 2027/2092 [34:33<01:06,  1.02s/it]\u001b[A\n","Steps:  97% 2028/2092 [34:34<01:05,  1.02s/it]\u001b[A\n","Steps:  97% 2029/2092 [34:35<01:04,  1.02s/it]\u001b[A\n","Steps:  97% 2030/2092 [34:36<01:03,  1.02s/it]\u001b[A\n","Steps:  97% 2031/2092 [34:37<01:02,  1.02s/it]\u001b[A\n","Steps:  97% 2032/2092 [34:38<01:01,  1.03s/it]\u001b[A\n","Steps:  97% 2033/2092 [34:39<01:00,  1.02s/it]\u001b[A\n","Steps:  97% 2034/2092 [34:40<00:59,  1.02s/it]\u001b[A\n","Steps:  97% 2035/2092 [34:41<00:58,  1.02s/it]\u001b[A\n","Steps:  97% 2036/2092 [34:42<00:56,  1.02s/it]\u001b[A\n","Steps:  97% 2037/2092 [34:43<00:56,  1.02s/it]\u001b[A\n","Steps:  97% 2038/2092 [34:44<00:54,  1.02s/it]\u001b[A\n","Steps:  97% 2039/2092 [34:45<00:54,  1.02s/it]\u001b[A\n","Steps:  98% 2040/2092 [34:46<00:52,  1.02s/it]\u001b[A\n","Steps:  98% 2041/2092 [34:47<00:52,  1.02s/it]\u001b[A\n","Steps:  98% 2042/2092 [34:48<00:51,  1.02s/it]\u001b[A\n","Steps:  98% 2043/2092 [34:49<00:50,  1.02s/it]\u001b[A\n","Steps:  98% 2044/2092 [34:50<00:49,  1.03s/it]\u001b[A\n","Steps:  98% 2045/2092 [34:51<00:48,  1.02s/it]\u001b[A\n","Steps:  98% 2046/2092 [34:52<00:46,  1.02s/it]\u001b[A\n","Steps:  98% 2047/2092 [34:53<00:45,  1.02s/it]\u001b[A\n","Steps:  98% 2048/2092 [34:54<00:44,  1.02s/it]\u001b[A\n","Steps:  98% 2049/2092 [34:55<00:43,  1.02s/it]\u001b[A\n","Steps:  98% 2050/2092 [34:56<00:42,  1.02s/it]\u001b[A\n","Steps:  98% 2051/2092 [34:57<00:41,  1.02s/it]\u001b[A\n","Steps:  98% 2052/2092 [34:58<00:40,  1.02s/it]\u001b[A\n","Steps:  98% 2053/2092 [34:59<00:39,  1.02s/it]\u001b[A\n","Steps:  98% 2054/2092 [35:00<00:38,  1.02s/it]\u001b[A\n","Steps:  98% 2055/2092 [35:01<00:37,  1.02s/it]\u001b[A\n","Steps:  98% 2056/2092 [35:02<00:36,  1.02s/it]\u001b[A\n","Steps:  98% 2057/2092 [35:03<00:35,  1.02s/it]\u001b[A\n","Steps:  98% 2058/2092 [35:04<00:34,  1.02s/it]\u001b[A\n","Steps:  98% 2059/2092 [35:05<00:33,  1.02s/it]\u001b[A\n","Steps:  98% 2060/2092 [35:06<00:32,  1.02s/it]\u001b[A\n","Steps:  99% 2061/2092 [35:07<00:31,  1.02s/it]\u001b[A\n","Steps:  99% 2062/2092 [35:08<00:30,  1.02s/it]\u001b[A\n","Steps:  99% 2063/2092 [35:09<00:29,  1.02s/it]\u001b[A\n","Steps:  99% 2064/2092 [35:10<00:28,  1.03s/it]\u001b[A\n","Steps:  99% 2065/2092 [35:11<00:27,  1.02s/it]\u001b[A\n","Steps:  99% 2066/2092 [35:12<00:26,  1.02s/it]\u001b[A\n","Steps:  99% 2067/2092 [35:13<00:25,  1.02s/it]\u001b[A\n","Steps:  99% 2068/2092 [35:14<00:24,  1.02s/it]\u001b[A\n","Steps:  99% 2069/2092 [35:15<00:23,  1.02s/it]\u001b[A\n","Steps:  99% 2070/2092 [35:17<00:22,  1.02s/it]\u001b[A\n","Steps:  99% 2071/2092 [35:18<00:21,  1.02s/it]\u001b[A\n","Steps:  99% 2072/2092 [35:19<00:20,  1.03s/it]\u001b[A\n","Steps:  99% 2073/2092 [35:20<00:19,  1.02s/it]\u001b[A\n","Steps:  99% 2074/2092 [35:21<00:18,  1.02s/it]\u001b[A\n","Steps:  99% 2075/2092 [35:22<00:17,  1.02s/it]\u001b[A\n","Steps:  99% 2076/2092 [35:23<00:16,  1.02s/it]\u001b[A\n","Steps:  99% 2077/2092 [35:24<00:15,  1.02s/it]\u001b[A\n","Steps:  99% 2078/2092 [35:25<00:14,  1.03s/it]\u001b[A\n","Steps:  99% 2079/2092 [35:26<00:13,  1.03s/it]\u001b[A\n","Steps:  99% 2080/2092 [35:27<00:12,  1.02s/it]\u001b[A\n","Steps:  99% 2081/2092 [35:28<00:11,  1.02s/it]\u001b[A\n","Steps: 100% 2082/2092 [35:29<00:10,  1.02s/it]\u001b[A\n","Steps: 100% 2083/2092 [35:30<00:09,  1.02s/it]\u001b[A\n","Steps: 100% 2084/2092 [35:31<00:08,  1.02s/it]\u001b[A\n","Steps: 100% 2085/2092 [35:32<00:07,  1.02s/it]\u001b[A\n","Steps: 100% 2086/2092 [35:33<00:06,  1.02s/it]\u001b[A\n","Steps: 100% 2087/2092 [35:34<00:05,  1.02s/it]\u001b[A\n","Steps: 100% 2088/2092 [35:35<00:04,  1.02s/it]\u001b[A\n","Steps: 100% 2089/2092 [35:36<00:03,  1.02s/it]\u001b[A\n","Steps: 100% 2090/2092 [35:37<00:02,  1.02s/it]\u001b[A\n","Steps: 100% 2091/2092 [35:38<00:01,  1.02s/it]\u001b[A\n","Steps: 100% 2092/2092 [35:39<00:00,  1.01s/it]\u001b[A\n","                                              \u001b[A11/24/2024 17:42:04 - INFO - train_eval -   Saving model checkpoint to ckpt/30_integrated_model_test/checkpoint-2093\n","Epoch 1 done\n","Epochs:  50% 1/2 [35:44<35:44, 2144.91s/it]\n","Steps:   0% 0/2092 [00:00<?, ?it/s]\u001b[A\n","Steps:   0% 1/2092 [00:01<37:38,  1.08s/it]\u001b[A\n","Steps:   0% 2/2092 [00:02<36:35,  1.05s/it]\u001b[A\n","Steps:   0% 3/2092 [00:03<35:55,  1.03s/it]\u001b[A\n","Steps:   0% 4/2092 [00:04<35:44,  1.03s/it]\u001b[A\n","Steps:   0% 5/2092 [00:05<35:37,  1.02s/it]\u001b[A\n","Steps:   0% 6/2092 [00:06<35:36,  1.02s/it]\u001b[A\n","Steps:   0% 7/2092 [00:07<35:36,  1.02s/it]\u001b[A\n","Steps:   0% 8/2092 [00:08<35:26,  1.02s/it]\u001b[A\n","Steps:   0% 9/2092 [00:09<35:23,  1.02s/it]\u001b[A\n","Steps:   0% 10/2092 [00:10<35:21,  1.02s/it]\u001b[A\n","Steps:   1% 11/2092 [00:11<35:26,  1.02s/it]\u001b[A\n","Steps:   1% 12/2092 [00:12<35:18,  1.02s/it]\u001b[A\n","Steps:   1% 13/2092 [00:13<35:24,  1.02s/it]\u001b[A\n","Steps:   1% 14/2092 [00:14<35:27,  1.02s/it]\u001b[A\n","Steps:   1% 15/2092 [00:15<35:16,  1.02s/it]\u001b[A\n","Steps:   1% 16/2092 [00:16<35:22,  1.02s/it]\u001b[A\n","Steps:   1% 17/2092 [00:17<35:12,  1.02s/it]\u001b[A\n","Steps:   1% 18/2092 [00:18<35:07,  1.02s/it]\u001b[A\n","Steps:   1% 19/2092 [00:19<35:13,  1.02s/it]\u001b[A\n","Steps:   1% 20/2092 [00:20<35:13,  1.02s/it]\u001b[A\n","Steps:   1% 21/2092 [00:21<35:17,  1.02s/it]\u001b[A\n","Steps:   1% 22/2092 [00:22<35:09,  1.02s/it]\u001b[A\n","Steps:   1% 23/2092 [00:23<35:12,  1.02s/it]\u001b[A\n","Steps:   1% 24/2092 [00:24<35:06,  1.02s/it]\u001b[A\n","Steps:   1% 25/2092 [00:25<35:05,  1.02s/it]\u001b[A\n","Steps:   1% 26/2092 [00:26<35:10,  1.02s/it]\u001b[A\n","Steps:   1% 27/2092 [00:27<35:09,  1.02s/it]\u001b[A\n","Steps:   1% 28/2092 [00:28<34:58,  1.02s/it]\u001b[A\n","Steps:   1% 29/2092 [00:29<34:51,  1.01s/it]\u001b[A\n","Steps:   1% 30/2092 [00:30<34:57,  1.02s/it]\u001b[A\n","Steps:   1% 31/2092 [00:31<35:00,  1.02s/it]\u001b[A\n","Steps:   2% 32/2092 [00:32<34:59,  1.02s/it]\u001b[A\n","Steps:   2% 33/2092 [00:33<34:59,  1.02s/it]\u001b[A\n","Steps:   2% 34/2092 [00:34<34:49,  1.02s/it]\u001b[A\n","Steps:   2% 35/2092 [00:35<34:50,  1.02s/it]\u001b[A\n","Steps:   2% 36/2092 [00:36<34:44,  1.01s/it]\u001b[A\n","Steps:   2% 37/2092 [00:37<34:45,  1.01s/it]\u001b[A\n","Steps:   2% 38/2092 [00:38<34:41,  1.01s/it]\u001b[A\n","Steps:   2% 39/2092 [00:39<34:47,  1.02s/it]\u001b[A\n","Steps:   2% 40/2092 [00:40<34:49,  1.02s/it]\u001b[A\n","Steps:   2% 41/2092 [00:41<34:50,  1.02s/it]\u001b[A\n","Steps:   2% 42/2092 [00:42<34:49,  1.02s/it]\u001b[A\n","Steps:   2% 43/2092 [00:43<34:48,  1.02s/it]\u001b[A\n","Steps:   2% 44/2092 [00:44<34:41,  1.02s/it]\u001b[A\n","Steps:   2% 45/2092 [00:45<34:41,  1.02s/it]\u001b[A\n","Steps:   2% 46/2092 [00:46<34:37,  1.02s/it]\u001b[A\n","Steps:   2% 47/2092 [00:47<34:38,  1.02s/it]\u001b[A\n","Steps:   2% 48/2092 [00:48<34:45,  1.02s/it]\u001b[A\n","Steps:   2% 49/2092 [00:49<34:43,  1.02s/it]\u001b[A\n","Steps:   2% 50/2092 [00:50<34:43,  1.02s/it]\u001b[A\n","Steps:   2% 51/2092 [00:52<34:40,  1.02s/it]\u001b[A\n","Steps:   2% 52/2092 [00:53<34:38,  1.02s/it]\u001b[A\n","Steps:   3% 53/2092 [00:54<34:30,  1.02s/it]\u001b[A\n","Steps:   3% 54/2092 [00:55<34:37,  1.02s/it]\u001b[A\n","Steps:   3% 55/2092 [00:56<34:38,  1.02s/it]\u001b[A\n","Steps:   3% 56/2092 [00:57<34:41,  1.02s/it]\u001b[A\n","Steps:   3% 57/2092 [00:58<34:28,  1.02s/it]\u001b[A\n","Steps:   3% 58/2092 [00:59<34:23,  1.01s/it]\u001b[A\n","Steps:   3% 59/2092 [01:00<34:15,  1.01s/it]\u001b[A\n","Steps:   3% 60/2092 [01:01<34:17,  1.01s/it]\u001b[A\n","Steps:   3% 61/2092 [01:02<34:14,  1.01s/it]\u001b[A\n","Steps:   3% 62/2092 [01:03<34:09,  1.01s/it]\u001b[A\n","Steps:   3% 63/2092 [01:04<34:12,  1.01s/it]\u001b[A\n","Steps:   3% 64/2092 [01:05<34:22,  1.02s/it]\u001b[A\n","Steps:   3% 65/2092 [01:06<34:21,  1.02s/it]\u001b[A\n","Steps:   3% 66/2092 [01:07<34:16,  1.02s/it]\u001b[A\n","Steps:   3% 67/2092 [01:08<34:13,  1.01s/it]\u001b[A\n","Steps:   3% 68/2092 [01:09<34:21,  1.02s/it]\u001b[A\n","Steps:   3% 69/2092 [01:10<34:24,  1.02s/it]\u001b[A\n","Steps:   3% 70/2092 [01:11<34:28,  1.02s/it]\u001b[A\n","Steps:   3% 71/2092 [01:12<34:30,  1.02s/it]\u001b[A\n","Steps:   3% 72/2092 [01:13<34:27,  1.02s/it]\u001b[A\n","Steps:   3% 73/2092 [01:14<34:29,  1.03s/it]\u001b[A\n","Steps:   4% 74/2092 [01:15<34:29,  1.03s/it]\u001b[A\n","Steps:   4% 75/2092 [01:16<34:29,  1.03s/it]\u001b[A\n","Steps:   4% 76/2092 [01:17<34:23,  1.02s/it]\u001b[A\n","Steps:   4% 77/2092 [01:18<34:21,  1.02s/it]\u001b[A\n","Steps:   4% 78/2092 [01:19<34:19,  1.02s/it]\u001b[A\n","Steps:   4% 79/2092 [01:20<34:10,  1.02s/it]\u001b[A\n","Steps:   4% 80/2092 [01:21<34:09,  1.02s/it]\u001b[A\n","Steps:   4% 81/2092 [01:22<34:03,  1.02s/it]\u001b[A\n","Steps:   4% 82/2092 [01:23<34:02,  1.02s/it]\u001b[A\n","Steps:   4% 83/2092 [01:24<34:02,  1.02s/it]\u001b[A\n","Steps:   4% 84/2092 [01:25<34:09,  1.02s/it]\u001b[A\n","Steps:   4% 85/2092 [01:26<34:14,  1.02s/it]\u001b[A\n","Steps:   4% 86/2092 [01:27<34:15,  1.02s/it]\u001b[A\n","Steps:   4% 87/2092 [01:28<34:11,  1.02s/it]\u001b[A\n","Steps:   4% 88/2092 [01:29<34:13,  1.02s/it]\u001b[A\n","Steps:   4% 89/2092 [01:30<34:03,  1.02s/it]\u001b[A\n","Steps:   4% 90/2092 [01:31<34:00,  1.02s/it]\u001b[A\n","Steps:   4% 91/2092 [01:32<34:03,  1.02s/it]\u001b[A\n","Steps:   4% 92/2092 [01:33<34:02,  1.02s/it]\u001b[A\n","Steps:   4% 93/2092 [01:34<34:06,  1.02s/it]\u001b[A\n","Steps:   4% 94/2092 [01:35<34:04,  1.02s/it]\u001b[A\n","Steps:   5% 95/2092 [01:36<34:06,  1.02s/it]\u001b[A\n","Steps:   5% 96/2092 [01:37<34:00,  1.02s/it]\u001b[A\n","Steps:   5% 97/2092 [01:38<33:56,  1.02s/it]\u001b[A\n","Steps:   5% 98/2092 [01:39<33:49,  1.02s/it]\u001b[A\n","Steps:   5% 99/2092 [01:40<33:49,  1.02s/it]\u001b[A\n","Steps:   5% 100/2092 [01:41<33:54,  1.02s/it]\u001b[A\n","Steps:   5% 101/2092 [01:42<33:48,  1.02s/it]\u001b[A\n","Steps:   5% 102/2092 [01:44<33:46,  1.02s/it]\u001b[A\n","Steps:   5% 103/2092 [01:45<33:51,  1.02s/it]\u001b[A\n","Steps:   5% 104/2092 [01:46<33:39,  1.02s/it]\u001b[A\n","Steps:   5% 105/2092 [01:47<33:31,  1.01s/it]\u001b[A\n","Steps:   5% 106/2092 [01:48<33:32,  1.01s/it]\u001b[A\n","Steps:   5% 107/2092 [01:49<33:39,  1.02s/it]\u001b[A\n","Steps:   5% 108/2092 [01:50<33:45,  1.02s/it]\u001b[A\n","Steps:   5% 109/2092 [01:51<33:44,  1.02s/it]\u001b[A\n","Steps:   5% 110/2092 [01:52<33:45,  1.02s/it]\u001b[A\n","Steps:   5% 111/2092 [01:53<33:42,  1.02s/it]\u001b[A\n","Steps:   5% 112/2092 [01:54<33:34,  1.02s/it]\u001b[A\n","Steps:   5% 113/2092 [01:55<33:41,  1.02s/it]\u001b[A\n","Steps:   5% 114/2092 [01:56<33:38,  1.02s/it]\u001b[A\n","Steps:   5% 115/2092 [01:57<33:31,  1.02s/it]\u001b[A\n","Steps:   6% 116/2092 [01:58<33:25,  1.02s/it]\u001b[A\n","Steps:   6% 117/2092 [01:59<33:26,  1.02s/it]\u001b[A\n","Steps:   6% 118/2092 [02:00<33:28,  1.02s/it]\u001b[A\n","Steps:   6% 119/2092 [02:01<33:22,  1.02s/it]\u001b[A\n","Steps:   6% 120/2092 [02:02<33:25,  1.02s/it]\u001b[A\n","Steps:   6% 121/2092 [02:03<33:31,  1.02s/it]\u001b[A\n","Steps:   6% 122/2092 [02:04<33:29,  1.02s/it]\u001b[A\n","Steps:   6% 123/2092 [02:05<33:18,  1.02s/it]\u001b[A\n","Steps:   6% 124/2092 [02:06<33:19,  1.02s/it]\u001b[A\n","Steps:   6% 125/2092 [02:07<33:14,  1.01s/it]\u001b[A\n","Steps:   6% 126/2092 [02:08<33:22,  1.02s/it]\u001b[A\n","Steps:   6% 127/2092 [02:09<33:20,  1.02s/it]\u001b[A\n","Steps:   6% 128/2092 [02:10<33:20,  1.02s/it]\u001b[A\n","Steps:   6% 129/2092 [02:11<33:14,  1.02s/it]\u001b[A\n","Steps:   6% 130/2092 [02:12<33:16,  1.02s/it]\u001b[A\n","Steps:   6% 131/2092 [02:13<33:23,  1.02s/it]\u001b[A\n","Steps:   6% 132/2092 [02:14<33:23,  1.02s/it]\u001b[A\n","Steps:   6% 133/2092 [02:15<33:24,  1.02s/it]\u001b[A\n","Steps:   6% 134/2092 [02:16<33:23,  1.02s/it]\u001b[A\n","Steps:   6% 135/2092 [02:17<33:24,  1.02s/it]\u001b[A\n","Steps:   7% 136/2092 [02:18<33:23,  1.02s/it]\u001b[A\n","Steps:   7% 137/2092 [02:19<33:19,  1.02s/it]\u001b[A\n","Steps:   7% 138/2092 [02:20<33:19,  1.02s/it]\u001b[A\n","Steps:   7% 139/2092 [02:21<33:15,  1.02s/it]\u001b[A\n","Steps:   7% 140/2092 [02:22<33:12,  1.02s/it]\u001b[A\n","Steps:   7% 141/2092 [02:23<33:03,  1.02s/it]\u001b[A\n","Steps:   7% 142/2092 [02:24<33:08,  1.02s/it]\u001b[A\n","Steps:   7% 143/2092 [02:25<33:11,  1.02s/it]\u001b[A\n","Steps:   7% 144/2092 [02:26<33:08,  1.02s/it]\u001b[A\n","Steps:   7% 145/2092 [02:27<33:00,  1.02s/it]\u001b[A\n","Steps:   7% 146/2092 [02:28<33:01,  1.02s/it]\u001b[A\n","Steps:   7% 147/2092 [02:29<32:55,  1.02s/it]\u001b[A\n","Steps:   7% 148/2092 [02:30<33:02,  1.02s/it]\u001b[A\n","Steps:   7% 149/2092 [02:31<33:05,  1.02s/it]\u001b[A\n","Steps:   7% 150/2092 [02:32<33:02,  1.02s/it]\u001b[A\n","Steps:   7% 151/2092 [02:33<33:05,  1.02s/it]\u001b[A\n","Steps:   7% 152/2092 [02:34<33:05,  1.02s/it]\u001b[A\n","Steps:   7% 153/2092 [02:36<33:07,  1.03s/it]\u001b[A\n","Steps:   7% 154/2092 [02:37<32:55,  1.02s/it]\u001b[A\n","Steps:   7% 155/2092 [02:38<32:56,  1.02s/it]\u001b[A\n","Steps:   7% 156/2092 [02:39<33:01,  1.02s/it]\u001b[A\n","Steps:   8% 157/2092 [02:40<32:52,  1.02s/it]\u001b[A\n","Steps:   8% 158/2092 [02:41<32:52,  1.02s/it]\u001b[A\n","Steps:   8% 159/2092 [02:42<32:47,  1.02s/it]\u001b[A\n","Steps:   8% 160/2092 [02:43<32:53,  1.02s/it]\u001b[A\n","Steps:   8% 161/2092 [02:44<32:55,  1.02s/it]\u001b[A\n","Steps:   8% 162/2092 [02:45<32:51,  1.02s/it]\u001b[A\n","Steps:   8% 163/2092 [02:46<32:56,  1.02s/it]\u001b[A\n","Steps:   8% 164/2092 [02:47<32:47,  1.02s/it]\u001b[A\n","Steps:   8% 165/2092 [02:48<32:50,  1.02s/it]\u001b[A\n","Steps:   8% 166/2092 [02:49<32:53,  1.02s/it]\u001b[A\n","Steps:   8% 167/2092 [02:50<32:48,  1.02s/it]\u001b[A\n","Steps:   8% 168/2092 [02:51<32:44,  1.02s/it]\u001b[A\n","Steps:   8% 169/2092 [02:52<32:37,  1.02s/it]\u001b[A\n","Steps:   8% 170/2092 [02:53<32:36,  1.02s/it]\u001b[A\n","Steps:   8% 171/2092 [02:54<32:40,  1.02s/it]\u001b[A\n","Steps:   8% 172/2092 [02:55<32:38,  1.02s/it]\u001b[A\n","Steps:   8% 173/2092 [02:56<32:33,  1.02s/it]\u001b[A\n","Steps:   8% 174/2092 [02:57<32:38,  1.02s/it]\u001b[A\n","Steps:   8% 175/2092 [02:58<32:37,  1.02s/it]\u001b[A\n","Steps:   8% 176/2092 [02:59<32:30,  1.02s/it]\u001b[A\n","Steps:   8% 177/2092 [03:00<32:35,  1.02s/it]\u001b[A\n","Steps:   9% 178/2092 [03:01<32:32,  1.02s/it]\u001b[A\n","Steps:   9% 179/2092 [03:02<32:25,  1.02s/it]\u001b[A\n","Steps:   9% 180/2092 [03:03<32:20,  1.01s/it]\u001b[A\n","Steps:   9% 181/2092 [03:04<32:19,  1.02s/it]\u001b[A\n","Steps:   9% 182/2092 [03:05<32:27,  1.02s/it]\u001b[A\n","Steps:   9% 183/2092 [03:06<32:25,  1.02s/it]\u001b[A\n","Steps:   9% 184/2092 [03:07<32:23,  1.02s/it]\u001b[A\n","Steps:   9% 185/2092 [03:08<32:29,  1.02s/it]\u001b[A\n","Steps:   9% 186/2092 [03:09<32:21,  1.02s/it]\u001b[A\n","Steps:   9% 187/2092 [03:10<32:26,  1.02s/it]\u001b[A\n","Steps:   9% 188/2092 [03:11<32:24,  1.02s/it]\u001b[A\n","Steps:   9% 189/2092 [03:12<32:15,  1.02s/it]\u001b[A\n","Steps:   9% 190/2092 [03:13<32:22,  1.02s/it]\u001b[A\n","Steps:   9% 191/2092 [03:14<32:20,  1.02s/it]\u001b[A\n","Steps:   9% 192/2092 [03:15<32:24,  1.02s/it]\u001b[A\n","Steps:   9% 193/2092 [03:16<32:26,  1.03s/it]\u001b[A\n","Steps:   9% 194/2092 [03:17<32:28,  1.03s/it]\u001b[A\n","Steps:   9% 195/2092 [03:18<32:18,  1.02s/it]\u001b[A\n","Steps:   9% 196/2092 [03:19<32:14,  1.02s/it]\u001b[A\n","Steps:   9% 197/2092 [03:20<32:12,  1.02s/it]\u001b[A\n","Steps:   9% 198/2092 [03:21<32:14,  1.02s/it]\u001b[A\n","Steps:  10% 199/2092 [03:22<32:17,  1.02s/it]\u001b[A\n","Steps:  10% 200/2092 [03:23<32:13,  1.02s/it]\u001b[A\n","Steps:  10% 201/2092 [03:24<32:13,  1.02s/it]\u001b[A\n","Steps:  10% 202/2092 [03:26<32:05,  1.02s/it]\u001b[A\n","Steps:  10% 203/2092 [03:27<32:01,  1.02s/it]\u001b[A\n","Steps:  10% 204/2092 [03:28<32:06,  1.02s/it]\u001b[A\n","Steps:  10% 205/2092 [03:29<32:04,  1.02s/it]\u001b[A\n","Steps:  10% 206/2092 [03:30<32:07,  1.02s/it]\u001b[A\n","Steps:  10% 207/2092 [03:31<32:03,  1.02s/it]\u001b[A\n","Steps:  10% 208/2092 [03:32<32:08,  1.02s/it]\u001b[A\n","Steps:  10% 209/2092 [03:33<32:11,  1.03s/it]\u001b[A\n","Steps:  10% 210/2092 [03:34<32:10,  1.03s/it]\u001b[A\n","Steps:  10% 211/2092 [03:35<32:01,  1.02s/it]\u001b[A\n","Steps:  10% 212/2092 [03:36<31:54,  1.02s/it]\u001b[A\n","Steps:  10% 213/2092 [03:37<31:56,  1.02s/it]\u001b[A\n","Steps:  10% 214/2092 [03:38<32:00,  1.02s/it]\u001b[A\n","Steps:  10% 215/2092 [03:39<32:01,  1.02s/it]\u001b[A\n","Steps:  10% 216/2092 [03:40<31:58,  1.02s/it]\u001b[A\n","Steps:  10% 217/2092 [03:41<31:59,  1.02s/it]\u001b[A\n","Steps:  10% 218/2092 [03:42<31:55,  1.02s/it]\u001b[A\n","Steps:  10% 219/2092 [03:43<31:57,  1.02s/it]\u001b[A\n","Steps:  11% 220/2092 [03:44<31:57,  1.02s/it]\u001b[A\n","Steps:  11% 221/2092 [03:45<31:50,  1.02s/it]\u001b[A\n","Steps:  11% 222/2092 [03:46<31:49,  1.02s/it]\u001b[A\n","Steps:  11% 223/2092 [03:47<31:52,  1.02s/it]\u001b[A\n","Steps:  11% 224/2092 [03:48<31:52,  1.02s/it]\u001b[A\n","Steps:  11% 225/2092 [03:49<31:52,  1.02s/it]\u001b[A\n","Steps:  11% 226/2092 [03:50<31:53,  1.03s/it]\u001b[A\n","Steps:  11% 227/2092 [03:51<31:48,  1.02s/it]\u001b[A\n","Steps:  11% 228/2092 [03:52<31:51,  1.03s/it]\u001b[A\n","Steps:  11% 229/2092 [03:53<31:45,  1.02s/it]\u001b[A\n","Steps:  11% 230/2092 [03:54<31:46,  1.02s/it]\u001b[A\n","Steps:  11% 231/2092 [03:55<31:41,  1.02s/it]\u001b[A\n","Steps:  11% 232/2092 [03:56<31:38,  1.02s/it]\u001b[A\n","Steps:  11% 233/2092 [03:57<31:30,  1.02s/it]\u001b[A\n","Steps:  11% 234/2092 [03:58<31:29,  1.02s/it]\u001b[A\n","Steps:  11% 235/2092 [03:59<31:29,  1.02s/it]\u001b[A\n","Steps:  11% 236/2092 [04:00<31:32,  1.02s/it]\u001b[A\n","Steps:  11% 237/2092 [04:01<31:32,  1.02s/it]\u001b[A\n","Steps:  11% 238/2092 [04:02<31:25,  1.02s/it]\u001b[A\n","Steps:  11% 239/2092 [04:03<31:24,  1.02s/it]\u001b[A\n","Steps:  11% 240/2092 [04:04<31:24,  1.02s/it]\u001b[A\n","Steps:  12% 241/2092 [04:05<31:28,  1.02s/it]\u001b[A\n","Steps:  12% 242/2092 [04:06<31:33,  1.02s/it]\u001b[A\n","Steps:  12% 243/2092 [04:07<31:30,  1.02s/it]\u001b[A\n","Steps:  12% 244/2092 [04:08<31:28,  1.02s/it]\u001b[A\n","Steps:  12% 245/2092 [04:09<31:29,  1.02s/it]\u001b[A\n","Steps:  12% 246/2092 [04:10<31:25,  1.02s/it]\u001b[A\n","Steps:  12% 247/2092 [04:11<31:13,  1.02s/it]\u001b[A\n","Steps:  12% 248/2092 [04:12<31:15,  1.02s/it]\u001b[A\n","Steps:  12% 249/2092 [04:14<31:15,  1.02s/it]\u001b[A\n","Steps:  12% 250/2092 [04:15<31:14,  1.02s/it]\u001b[A\n","Steps:  12% 251/2092 [04:16<31:19,  1.02s/it]\u001b[A\n","Steps:  12% 252/2092 [04:17<31:23,  1.02s/it]\u001b[A\n","Steps:  12% 253/2092 [04:18<31:23,  1.02s/it]\u001b[A\n","Steps:  12% 254/2092 [04:19<31:19,  1.02s/it]\u001b[A\n","Steps:  12% 255/2092 [04:20<31:08,  1.02s/it]\u001b[A\n","Steps:  12% 256/2092 [04:21<31:02,  1.01s/it]\u001b[A\n","Steps:  12% 257/2092 [04:22<31:07,  1.02s/it]\u001b[A\n","Steps:  12% 258/2092 [04:23<31:10,  1.02s/it]\u001b[A\n","Steps:  12% 259/2092 [04:24<31:15,  1.02s/it]\u001b[A\n","Steps:  12% 260/2092 [04:25<31:06,  1.02s/it]\u001b[A\n","Steps:  12% 261/2092 [04:26<31:05,  1.02s/it]\u001b[A\n","Steps:  13% 262/2092 [04:27<30:59,  1.02s/it]\u001b[A\n","Steps:  13% 263/2092 [04:28<31:01,  1.02s/it]\u001b[A\n","Steps:  13% 264/2092 [04:29<31:06,  1.02s/it]\u001b[A\n","Steps:  13% 265/2092 [04:30<31:10,  1.02s/it]\u001b[A\n","Steps:  13% 266/2092 [04:31<31:07,  1.02s/it]\u001b[A\n","Steps:  13% 267/2092 [04:32<31:10,  1.02s/it]\u001b[A\n","Steps:  13% 268/2092 [04:33<31:11,  1.03s/it]\u001b[A\n","Steps:  13% 269/2092 [04:34<31:08,  1.02s/it]\u001b[A\n","Steps:  13% 270/2092 [04:35<31:10,  1.03s/it]\u001b[A\n","Steps:  13% 271/2092 [04:36<31:04,  1.02s/it]\u001b[A\n","Steps:  13% 272/2092 [04:37<31:00,  1.02s/it]\u001b[A\n","Steps:  13% 273/2092 [04:38<31:03,  1.02s/it]\u001b[A\n","Steps:  13% 274/2092 [04:39<31:05,  1.03s/it]\u001b[A\n","Steps:  13% 275/2092 [04:40<31:05,  1.03s/it]\u001b[A\n","Steps:  13% 276/2092 [04:41<30:59,  1.02s/it]\u001b[A\n","Steps:  13% 277/2092 [04:42<30:50,  1.02s/it]\u001b[A\n","Steps:  13% 278/2092 [04:43<30:55,  1.02s/it]\u001b[A\n","Steps:  13% 279/2092 [04:44<30:42,  1.02s/it]\u001b[A\n","Steps:  13% 280/2092 [04:45<30:43,  1.02s/it]\u001b[A\n","Steps:  13% 281/2092 [04:46<30:43,  1.02s/it]\u001b[A\n","Steps:  13% 282/2092 [04:47<30:38,  1.02s/it]\u001b[A\n","Steps:  14% 283/2092 [04:48<30:38,  1.02s/it]\u001b[A\n","Steps:  14% 284/2092 [04:49<30:39,  1.02s/it]\u001b[A\n","Steps:  14% 285/2092 [04:50<30:39,  1.02s/it]\u001b[A\n","Steps:  14% 286/2092 [04:51<30:40,  1.02s/it]\u001b[A\n","Steps:  14% 287/2092 [04:52<30:40,  1.02s/it]\u001b[A\n","Steps:  14% 288/2092 [04:53<30:41,  1.02s/it]\u001b[A\n","Steps:  14% 289/2092 [04:54<30:32,  1.02s/it]\u001b[A\n","Steps:  14% 290/2092 [04:55<30:27,  1.01s/it]\u001b[A\n","Steps:  14% 291/2092 [04:56<30:31,  1.02s/it]\u001b[A\n","Steps:  14% 292/2092 [04:57<30:30,  1.02s/it]\u001b[A\n","Steps:  14% 293/2092 [04:58<30:26,  1.02s/it]\u001b[A\n","Steps:  14% 294/2092 [04:59<30:28,  1.02s/it]\u001b[A\n","Steps:  14% 295/2092 [05:00<30:35,  1.02s/it]\u001b[A\n","Steps:  14% 296/2092 [05:01<30:28,  1.02s/it]\u001b[A\n","Steps:  14% 297/2092 [05:02<30:19,  1.01s/it]\u001b[A\n","Steps:  14% 298/2092 [05:03<30:26,  1.02s/it]\u001b[A\n","Steps:  14% 299/2092 [05:05<30:24,  1.02s/it]\u001b[A\n","Steps:  14% 300/2092 [05:06<30:19,  1.02s/it]\u001b[A\n","Steps:  14% 301/2092 [05:07<30:21,  1.02s/it]\u001b[A\n","Steps:  14% 302/2092 [05:08<30:22,  1.02s/it]\u001b[A\n","Steps:  14% 303/2092 [05:09<30:27,  1.02s/it]\u001b[A\n","Steps:  15% 304/2092 [05:10<30:28,  1.02s/it]\u001b[A\n","Steps:  15% 305/2092 [05:11<30:25,  1.02s/it]\u001b[A\n","Steps:  15% 306/2092 [05:12<30:27,  1.02s/it]\u001b[A\n","Steps:  15% 307/2092 [05:13<30:29,  1.03s/it]\u001b[A\n","Steps:  15% 308/2092 [05:14<30:26,  1.02s/it]\u001b[A\n","Steps:  15% 309/2092 [05:15<30:18,  1.02s/it]\u001b[A\n","Steps:  15% 310/2092 [05:16<30:16,  1.02s/it]\u001b[A\n","Steps:  15% 311/2092 [05:17<30:20,  1.02s/it]\u001b[A\n","Steps:  15% 312/2092 [05:18<30:17,  1.02s/it]\u001b[A\n","Steps:  15% 313/2092 [05:19<30:06,  1.02s/it]\u001b[A\n","Steps:  15% 314/2092 [05:20<30:08,  1.02s/it]\u001b[A\n","Steps:  15% 315/2092 [05:21<30:13,  1.02s/it]\u001b[A\n","Steps:  15% 316/2092 [05:22<30:09,  1.02s/it]\u001b[A\n","Steps:  15% 317/2092 [05:23<30:14,  1.02s/it]\u001b[A\n","Steps:  15% 318/2092 [05:24<30:16,  1.02s/it]\u001b[A\n","Steps:  15% 319/2092 [05:25<30:18,  1.03s/it]\u001b[A\n","Steps:  15% 320/2092 [05:26<30:20,  1.03s/it]\u001b[A\n","Steps:  15% 321/2092 [05:27<30:10,  1.02s/it]\u001b[A\n","Steps:  15% 322/2092 [05:28<30:02,  1.02s/it]\u001b[A\n","Steps:  15% 323/2092 [05:29<29:57,  1.02s/it]\u001b[A\n","Steps:  15% 324/2092 [05:30<29:57,  1.02s/it]\u001b[A\n","Steps:  16% 325/2092 [05:31<30:03,  1.02s/it]\u001b[A\n","Steps:  16% 326/2092 [05:32<30:01,  1.02s/it]\u001b[A\n","Steps:  16% 327/2092 [05:33<30:05,  1.02s/it]\u001b[A\n","Steps:  16% 328/2092 [05:34<30:01,  1.02s/it]\u001b[A\n","Steps:  16% 329/2092 [05:35<30:04,  1.02s/it]\u001b[A\n","Steps:  16% 330/2092 [05:36<30:01,  1.02s/it]\u001b[A\n","Steps:  16% 331/2092 [05:37<29:56,  1.02s/it]\u001b[A\n","Steps:  16% 332/2092 [05:38<29:54,  1.02s/it]\u001b[A\n","Steps:  16% 333/2092 [05:39<29:57,  1.02s/it]\u001b[A\n","Steps:  16% 334/2092 [05:40<29:55,  1.02s/it]\u001b[A\n","Steps:  16% 335/2092 [05:41<29:52,  1.02s/it]\u001b[A\n","Steps:  16% 336/2092 [05:42<29:47,  1.02s/it]\u001b[A\n","Steps:  16% 337/2092 [05:43<29:41,  1.02s/it]\u001b[A\n","Steps:  16% 338/2092 [05:44<29:37,  1.01s/it]\u001b[A\n","Steps:  16% 339/2092 [05:45<29:42,  1.02s/it]\u001b[A\n","Steps:  16% 340/2092 [05:46<29:37,  1.01s/it]\u001b[A\n","Steps:  16% 341/2092 [05:47<29:44,  1.02s/it]\u001b[A\n","Steps:  16% 342/2092 [05:48<29:44,  1.02s/it]\u001b[A\n","Steps:  16% 343/2092 [05:49<29:39,  1.02s/it]\u001b[A\n","Steps:  16% 344/2092 [05:50<29:41,  1.02s/it]\u001b[A\n","Steps:  16% 345/2092 [05:51<29:43,  1.02s/it]\u001b[A\n","Steps:  17% 346/2092 [05:52<29:41,  1.02s/it]\u001b[A\n","Steps:  17% 347/2092 [05:53<29:43,  1.02s/it]\u001b[A\n","Steps:  17% 348/2092 [05:54<29:40,  1.02s/it]\u001b[A\n","Steps:  17% 349/2092 [05:56<29:41,  1.02s/it]\u001b[A\n","Steps:  17% 350/2092 [05:57<29:39,  1.02s/it]\u001b[A\n","Steps:  17% 351/2092 [05:58<29:31,  1.02s/it]\u001b[A\n","Steps:  17% 352/2092 [05:59<29:34,  1.02s/it]\u001b[A\n","Steps:  17% 353/2092 [06:00<29:33,  1.02s/it]\u001b[A\n","Steps:  17% 354/2092 [06:01<29:30,  1.02s/it]\u001b[A\n","Steps:  17% 355/2092 [06:02<29:33,  1.02s/it]\u001b[A\n","Steps:  17% 356/2092 [06:03<29:31,  1.02s/it]\u001b[A\n","Steps:  17% 357/2092 [06:04<29:29,  1.02s/it]\u001b[A\n","Steps:  17% 358/2092 [06:05<29:29,  1.02s/it]\u001b[A\n","Steps:  17% 359/2092 [06:06<29:28,  1.02s/it]\u001b[A\n","Steps:  17% 360/2092 [06:07<29:25,  1.02s/it]\u001b[A\n","Steps:  17% 361/2092 [06:08<29:21,  1.02s/it]\u001b[A\n","Steps:  17% 362/2092 [06:09<29:21,  1.02s/it]\u001b[A\n","Steps:  17% 363/2092 [06:10<29:16,  1.02s/it]\u001b[A\n","Steps:  17% 364/2092 [06:11<29:22,  1.02s/it]\u001b[A\n","Steps:  17% 365/2092 [06:12<29:24,  1.02s/it]\u001b[A\n","Steps:  17% 366/2092 [06:13<29:16,  1.02s/it]\u001b[A\n","Steps:  18% 367/2092 [06:14<29:21,  1.02s/it]\u001b[A\n","Steps:  18% 368/2092 [06:15<29:14,  1.02s/it]\u001b[A\n","Steps:  18% 369/2092 [06:16<29:13,  1.02s/it]\u001b[A\n","Steps:  18% 370/2092 [06:17<29:19,  1.02s/it]\u001b[A\n","Steps:  18% 371/2092 [06:18<29:20,  1.02s/it]\u001b[A\n","Steps:  18% 372/2092 [06:19<29:13,  1.02s/it]\u001b[A\n","Steps:  18% 373/2092 [06:20<29:08,  1.02s/it]\u001b[A\n","Steps:  18% 374/2092 [06:21<29:13,  1.02s/it]\u001b[A\n","Steps:  18% 375/2092 [06:22<29:10,  1.02s/it]\u001b[A\n","Steps:  18% 376/2092 [06:23<29:15,  1.02s/it]\u001b[A\n","Steps:  18% 377/2092 [06:24<29:17,  1.02s/it]\u001b[A\n","Steps:  18% 378/2092 [06:25<29:19,  1.03s/it]\u001b[A\n","Steps:  18% 379/2092 [06:26<29:19,  1.03s/it]\u001b[A\n","Steps:  18% 380/2092 [06:27<29:15,  1.03s/it]\u001b[A\n","Steps:  18% 381/2092 [06:28<29:10,  1.02s/it]\u001b[A\n","Steps:  18% 382/2092 [06:29<29:07,  1.02s/it]\u001b[A\n","Steps:  18% 383/2092 [06:30<29:03,  1.02s/it]\u001b[A\n","Steps:  18% 384/2092 [06:31<29:01,  1.02s/it]\u001b[A\n","Steps:  18% 385/2092 [06:32<28:59,  1.02s/it]\u001b[A\n","Steps:  18% 386/2092 [06:33<29:01,  1.02s/it]\u001b[A\n","Steps:  18% 387/2092 [06:34<29:00,  1.02s/it]\u001b[A\n","Steps:  19% 388/2092 [06:35<29:03,  1.02s/it]\u001b[A\n","Steps:  19% 389/2092 [06:36<28:54,  1.02s/it]\u001b[A\n","Steps:  19% 390/2092 [06:37<28:53,  1.02s/it]\u001b[A\n","Steps:  19% 391/2092 [06:38<28:52,  1.02s/it]\u001b[A\n","Steps:  19% 392/2092 [06:39<28:46,  1.02s/it]\u001b[A\n","Steps:  19% 393/2092 [06:40<28:50,  1.02s/it]\u001b[A\n","Steps:  19% 394/2092 [06:41<28:54,  1.02s/it]\u001b[A\n","Steps:  19% 395/2092 [06:42<28:53,  1.02s/it]\u001b[A\n","Steps:  19% 396/2092 [06:43<28:51,  1.02s/it]\u001b[A\n","Steps:  19% 397/2092 [06:44<28:45,  1.02s/it]\u001b[A\n","Steps:  19% 398/2092 [06:46<28:46,  1.02s/it]\u001b[A\n","Steps:  19% 399/2092 [06:47<28:44,  1.02s/it]\u001b[A\n","Steps:  19% 400/2092 [06:48<28:47,  1.02s/it]\u001b[A\n","Steps:  19% 401/2092 [06:49<28:40,  1.02s/it]\u001b[A\n","Steps:  19% 402/2092 [06:50<28:40,  1.02s/it]\u001b[A\n","Steps:  19% 403/2092 [06:51<28:35,  1.02s/it]\u001b[A\n","Steps:  19% 404/2092 [06:52<28:35,  1.02s/it]\u001b[A\n","Steps:  19% 405/2092 [06:53<28:35,  1.02s/it]\u001b[A\n","Steps:  19% 406/2092 [06:54<28:37,  1.02s/it]\u001b[A\n","Steps:  19% 407/2092 [06:55<28:39,  1.02s/it]\u001b[A\n","Steps:  20% 408/2092 [06:56<28:36,  1.02s/it]\u001b[A\n","Steps:  20% 409/2092 [06:57<28:34,  1.02s/it]\u001b[A\n","Steps:  20% 410/2092 [06:58<28:35,  1.02s/it]\u001b[A\n","Steps:  20% 411/2092 [06:59<28:33,  1.02s/it]\u001b[A\n","Steps:  20% 412/2092 [07:00<28:32,  1.02s/it]\u001b[A\n","Steps:  20% 413/2092 [07:01<28:30,  1.02s/it]\u001b[A\n","Steps:  20% 414/2092 [07:02<28:34,  1.02s/it]\u001b[A\n","Steps:  20% 415/2092 [07:03<28:36,  1.02s/it]\u001b[A\n","Steps:  20% 416/2092 [07:04<28:38,  1.03s/it]\u001b[A\n","Steps:  20% 417/2092 [07:05<28:37,  1.03s/it]\u001b[A\n","Steps:  20% 418/2092 [07:06<28:32,  1.02s/it]\u001b[A\n","Steps:  20% 419/2092 [07:07<28:34,  1.02s/it]\u001b[A\n","Steps:  20% 420/2092 [07:08<28:29,  1.02s/it]\u001b[A\n","Steps:  20% 421/2092 [07:09<28:28,  1.02s/it]\u001b[A\n","Steps:  20% 422/2092 [07:10<28:20,  1.02s/it]\u001b[A\n","Steps:  20% 423/2092 [07:11<28:25,  1.02s/it]\u001b[A\n","Steps:  20% 424/2092 [07:12<28:29,  1.02s/it]\u001b[A\n","Steps:  20% 425/2092 [07:13<28:28,  1.02s/it]\u001b[A\n","Steps:  20% 426/2092 [07:14<28:24,  1.02s/it]\u001b[A\n","Steps:  20% 427/2092 [07:15<28:26,  1.02s/it]\u001b[A\n","Steps:  20% 428/2092 [07:16<28:23,  1.02s/it]\u001b[A\n","Steps:  21% 429/2092 [07:17<28:23,  1.02s/it]\u001b[A\n","Steps:  21% 430/2092 [07:18<28:15,  1.02s/it]\u001b[A\n","Steps:  21% 431/2092 [07:19<28:19,  1.02s/it]\u001b[A\n","Steps:  21% 432/2092 [07:20<28:20,  1.02s/it]\u001b[A\n","Steps:  21% 433/2092 [07:21<28:12,  1.02s/it]\u001b[A\n","Steps:  21% 434/2092 [07:22<28:10,  1.02s/it]\u001b[A\n","Steps:  21% 435/2092 [07:23<28:09,  1.02s/it]\u001b[A\n","Steps:  21% 436/2092 [07:24<28:09,  1.02s/it]\u001b[A\n","Steps:  21% 437/2092 [07:25<28:07,  1.02s/it]\u001b[A\n","Steps:  21% 438/2092 [07:26<28:11,  1.02s/it]\u001b[A\n","Steps:  21% 439/2092 [07:27<28:13,  1.02s/it]\u001b[A\n","Steps:  21% 440/2092 [07:28<28:13,  1.03s/it]\u001b[A\n","Steps:  21% 441/2092 [07:29<28:09,  1.02s/it]\u001b[A\n","Steps:  21% 442/2092 [07:30<28:04,  1.02s/it]\u001b[A\n","Steps:  21% 443/2092 [07:31<28:02,  1.02s/it]\u001b[A\n","Steps:  21% 444/2092 [07:32<28:00,  1.02s/it]\u001b[A\n","Steps:  21% 445/2092 [07:33<27:55,  1.02s/it]\u001b[A\n","Steps:  21% 446/2092 [07:35<27:58,  1.02s/it]\u001b[A\n","Steps:  21% 447/2092 [07:36<28:01,  1.02s/it]\u001b[A\n","Steps:  21% 448/2092 [07:37<28:02,  1.02s/it]\u001b[A\n","Steps:  21% 449/2092 [07:38<27:59,  1.02s/it]\u001b[A\n","Steps:  22% 450/2092 [07:39<27:58,  1.02s/it]\u001b[A\n","Steps:  22% 451/2092 [07:40<27:51,  1.02s/it]\u001b[A\n","Steps:  22% 452/2092 [07:41<27:49,  1.02s/it]\u001b[A\n","Steps:  22% 453/2092 [07:42<27:48,  1.02s/it]\u001b[A\n","Steps:  22% 454/2092 [07:43<27:47,  1.02s/it]\u001b[A\n","Steps:  22% 455/2092 [07:44<27:47,  1.02s/it]\u001b[A\n","Steps:  22% 456/2092 [07:45<27:52,  1.02s/it]\u001b[A\n","Steps:  22% 457/2092 [07:46<27:49,  1.02s/it]\u001b[A\n","Steps:  22% 458/2092 [07:47<27:46,  1.02s/it]\u001b[A\n","Steps:  22% 459/2092 [07:48<27:48,  1.02s/it]\u001b[A\n","Steps:  22% 460/2092 [07:49<27:41,  1.02s/it]\u001b[A\n","Steps:  22% 461/2092 [07:50<27:46,  1.02s/it]\u001b[A\n","Steps:  22% 462/2092 [07:51<27:49,  1.02s/it]\u001b[A\n","Steps:  22% 463/2092 [07:52<27:41,  1.02s/it]\u001b[A\n","Steps:  22% 464/2092 [07:53<27:39,  1.02s/it]\u001b[A\n","Steps:  22% 465/2092 [07:54<27:39,  1.02s/it]\u001b[A\n","Steps:  22% 466/2092 [07:55<27:33,  1.02s/it]\u001b[A\n","Steps:  22% 467/2092 [07:56<27:34,  1.02s/it]\u001b[A\n","Steps:  22% 468/2092 [07:57<27:32,  1.02s/it]\u001b[A\n","Steps:  22% 469/2092 [07:58<27:35,  1.02s/it]\u001b[A\n","Steps:  22% 470/2092 [07:59<27:36,  1.02s/it]\u001b[A\n","Steps:  23% 471/2092 [08:00<27:27,  1.02s/it]\u001b[A\n","Steps:  23% 472/2092 [08:01<27:32,  1.02s/it]\u001b[A\n","Steps:  23% 473/2092 [08:02<27:31,  1.02s/it]\u001b[A\n","Steps:  23% 474/2092 [08:03<27:33,  1.02s/it]\u001b[A\n","Steps:  23% 475/2092 [08:04<27:33,  1.02s/it]\u001b[A\n","Steps:  23% 476/2092 [08:05<27:31,  1.02s/it]\u001b[A\n","Steps:  23% 477/2092 [08:06<27:30,  1.02s/it]\u001b[A\n","Steps:  23% 478/2092 [08:07<27:26,  1.02s/it]\u001b[A\n","Steps:  23% 479/2092 [08:08<27:29,  1.02s/it]\u001b[A\n","Steps:  23% 480/2092 [08:09<27:29,  1.02s/it]\u001b[A\n","Steps:  23% 481/2092 [08:10<27:26,  1.02s/it]\u001b[A\n","Steps:  23% 482/2092 [08:11<27:24,  1.02s/it]\u001b[A\n","Steps:  23% 483/2092 [08:12<27:27,  1.02s/it]\u001b[A\n","Steps:  23% 484/2092 [08:13<27:29,  1.03s/it]\u001b[A\n","Steps:  23% 485/2092 [08:14<27:30,  1.03s/it]\u001b[A\n","Steps:  23% 486/2092 [08:15<27:21,  1.02s/it]\u001b[A\n","Steps:  23% 487/2092 [08:16<27:19,  1.02s/it]\u001b[A\n","Steps:  23% 488/2092 [08:17<27:22,  1.02s/it]\u001b[A\n","Steps:  23% 489/2092 [08:18<27:14,  1.02s/it]\u001b[A\n","Steps:  23% 490/2092 [08:19<27:13,  1.02s/it]\u001b[A\n","Steps:  23% 491/2092 [08:20<27:11,  1.02s/it]\u001b[A\n","Steps:  24% 492/2092 [08:21<27:15,  1.02s/it]\u001b[A\n","Steps:  24% 493/2092 [08:23<27:12,  1.02s/it]\u001b[A\n","Steps:  24% 494/2092 [08:24<27:13,  1.02s/it]\u001b[A\n","Steps:  24% 495/2092 [08:25<27:10,  1.02s/it]\u001b[A\n","Steps:  24% 496/2092 [08:26<27:08,  1.02s/it]\u001b[A\n","Steps:  24% 497/2092 [08:27<27:02,  1.02s/it]\u001b[A\n","Steps:  24% 498/2092 [08:28<27:00,  1.02s/it]\u001b[A\n","Steps:  24% 499/2092 [08:29<27:05,  1.02s/it]\u001b[A\n","Steps:  24% 500/2092 [08:30<27:06,  1.02s/it]\u001b[A\n","Steps:  24% 501/2092 [08:31<27:03,  1.02s/it]\u001b[A\n","Steps:  24% 502/2092 [08:32<27:01,  1.02s/it]\u001b[A\n","Steps:  24% 503/2092 [08:33<27:05,  1.02s/it]\u001b[A\n","Steps:  24% 504/2092 [08:34<27:07,  1.02s/it]\u001b[A\n","Steps:  24% 505/2092 [08:35<27:04,  1.02s/it]\u001b[A\n","Steps:  24% 506/2092 [08:36<27:01,  1.02s/it]\u001b[A\n","Steps:  24% 507/2092 [08:37<27:01,  1.02s/it]\u001b[A\n","Steps:  24% 508/2092 [08:38<27:03,  1.02s/it]\u001b[A\n","Steps:  24% 509/2092 [08:39<26:53,  1.02s/it]\u001b[A\n","Steps:  24% 510/2092 [08:40<26:47,  1.02s/it]\u001b[A\n","Steps:  24% 511/2092 [08:41<26:52,  1.02s/it]\u001b[A\n","Steps:  24% 512/2092 [08:42<26:54,  1.02s/it]\u001b[A\n","Steps:  25% 513/2092 [08:43<26:47,  1.02s/it]\u001b[A\n","Steps:  25% 514/2092 [08:44<26:47,  1.02s/it]\u001b[A\n","Steps:  25% 515/2092 [08:45<26:51,  1.02s/it]\u001b[A\n","Steps:  25% 516/2092 [08:46<26:49,  1.02s/it]\u001b[A\n","Steps:  25% 517/2092 [08:47<26:52,  1.02s/it]\u001b[A\n","Steps:  25% 518/2092 [08:48<26:49,  1.02s/it]\u001b[A\n","Steps:  25% 519/2092 [08:49<26:45,  1.02s/it]\u001b[A\n","Steps:  25% 520/2092 [08:50<26:43,  1.02s/it]\u001b[A\n","Steps:  25% 521/2092 [08:51<26:38,  1.02s/it]\u001b[A\n","Steps:  25% 522/2092 [08:52<26:40,  1.02s/it]\u001b[A\n","Steps:  25% 523/2092 [08:53<26:40,  1.02s/it]\u001b[A\n","Steps:  25% 524/2092 [08:54<26:38,  1.02s/it]\u001b[A\n","Steps:  25% 525/2092 [08:55<26:32,  1.02s/it]\u001b[A\n","Steps:  25% 526/2092 [08:56<26:36,  1.02s/it]\u001b[A\n","Steps:  25% 527/2092 [08:57<26:33,  1.02s/it]\u001b[A\n","Steps:  25% 528/2092 [08:58<26:27,  1.02s/it]\u001b[A\n","Steps:  25% 529/2092 [08:59<26:24,  1.01s/it]\u001b[A\n","Steps:  25% 530/2092 [09:00<26:28,  1.02s/it]\u001b[A\n","Steps:  25% 531/2092 [09:01<26:32,  1.02s/it]\u001b[A\n","Steps:  25% 532/2092 [09:02<26:35,  1.02s/it]\u001b[A\n","Steps:  25% 533/2092 [09:03<26:35,  1.02s/it]\u001b[A\n","Steps:  26% 534/2092 [09:04<26:32,  1.02s/it]\u001b[A\n","Steps:  26% 535/2092 [09:05<26:35,  1.02s/it]\u001b[A\n","Steps:  26% 536/2092 [09:06<26:28,  1.02s/it]\u001b[A\n","Steps:  26% 537/2092 [09:07<26:25,  1.02s/it]\u001b[A\n","Steps:  26% 538/2092 [09:08<26:26,  1.02s/it]\u001b[A\n","Steps:  26% 539/2092 [09:09<26:26,  1.02s/it]\u001b[A\n","Steps:  26% 540/2092 [09:10<26:23,  1.02s/it]\u001b[A\n","Steps:  26% 541/2092 [09:11<26:26,  1.02s/it]\u001b[A\n","Steps:  26% 542/2092 [09:13<26:27,  1.02s/it]\u001b[A\n","Steps:  26% 543/2092 [09:14<26:26,  1.02s/it]\u001b[A\n","Steps:  26% 544/2092 [09:15<26:18,  1.02s/it]\u001b[A\n","Steps:  26% 545/2092 [09:16<26:20,  1.02s/it]\u001b[A\n","Steps:  26% 546/2092 [09:17<26:23,  1.02s/it]\u001b[A\n","Steps:  26% 547/2092 [09:18<26:20,  1.02s/it]\u001b[A\n","Steps:  26% 548/2092 [09:19<26:13,  1.02s/it]\u001b[A\n","Steps:  26% 549/2092 [09:20<26:13,  1.02s/it]\u001b[A\n","Steps:  26% 550/2092 [09:21<26:11,  1.02s/it]\u001b[A\n","Steps:  26% 551/2092 [09:22<26:13,  1.02s/it]\u001b[A\n","Steps:  26% 552/2092 [09:23<26:16,  1.02s/it]\u001b[A\n","Steps:  26% 553/2092 [09:24<26:18,  1.03s/it]\u001b[A\n","Steps:  26% 554/2092 [09:25<26:17,  1.03s/it]\u001b[A\n","Steps:  27% 555/2092 [09:26<26:17,  1.03s/it]\u001b[A\n","Steps:  27% 556/2092 [09:27<26:15,  1.03s/it]\u001b[A\n","Steps:  27% 557/2092 [09:28<26:11,  1.02s/it]\u001b[A\n","Steps:  27% 558/2092 [09:29<26:12,  1.03s/it]\u001b[A\n","Steps:  27% 559/2092 [09:30<26:04,  1.02s/it]\u001b[A\n","Steps:  27% 560/2092 [09:31<26:05,  1.02s/it]\u001b[A\n","Steps:  27% 561/2092 [09:32<26:04,  1.02s/it]\u001b[A\n","Steps:  27% 562/2092 [09:33<26:06,  1.02s/it]\u001b[A\n","Steps:  27% 563/2092 [09:34<26:05,  1.02s/it]\u001b[A\n","Steps:  27% 564/2092 [09:35<26:07,  1.03s/it]\u001b[A\n","Steps:  27% 565/2092 [09:36<26:04,  1.02s/it]\u001b[A\n","Steps:  27% 566/2092 [09:37<25:57,  1.02s/it]\u001b[A\n","Steps:  27% 567/2092 [09:38<25:56,  1.02s/it]\u001b[A\n","Steps:  27% 568/2092 [09:39<25:54,  1.02s/it]\u001b[A\n","Steps:  27% 569/2092 [09:40<25:57,  1.02s/it]\u001b[A\n","Steps:  27% 570/2092 [09:41<25:52,  1.02s/it]\u001b[A\n","Steps:  27% 571/2092 [09:42<25:47,  1.02s/it]\u001b[A\n","Steps:  27% 572/2092 [09:43<25:51,  1.02s/it]\u001b[A\n","Steps:  27% 573/2092 [09:44<25:49,  1.02s/it]\u001b[A\n","Steps:  27% 574/2092 [09:45<25:49,  1.02s/it]\u001b[A\n","Steps:  27% 575/2092 [09:46<25:51,  1.02s/it]\u001b[A\n","Steps:  28% 576/2092 [09:47<25:48,  1.02s/it]\u001b[A\n","Steps:  28% 577/2092 [09:48<25:49,  1.02s/it]\u001b[A\n","Steps:  28% 578/2092 [09:49<25:46,  1.02s/it]\u001b[A\n","Steps:  28% 579/2092 [09:50<25:40,  1.02s/it]\u001b[A\n","Steps:  28% 580/2092 [09:51<25:38,  1.02s/it]\u001b[A\n","Steps:  28% 581/2092 [09:52<25:33,  1.01s/it]\u001b[A\n","Steps:  28% 582/2092 [09:53<25:33,  1.02s/it]\u001b[A\n","Steps:  28% 583/2092 [09:54<25:33,  1.02s/it]\u001b[A\n","Steps:  28% 584/2092 [09:55<25:39,  1.02s/it]\u001b[A\n","Steps:  28% 585/2092 [09:56<25:33,  1.02s/it]\u001b[A\n","Steps:  28% 586/2092 [09:57<25:33,  1.02s/it]\u001b[A\n","Steps:  28% 587/2092 [09:58<25:36,  1.02s/it]\u001b[A\n","Steps:  28% 588/2092 [09:59<25:34,  1.02s/it]\u001b[A\n","Steps:  28% 589/2092 [10:01<25:32,  1.02s/it]\u001b[A\n","Steps:  28% 590/2092 [10:02<25:34,  1.02s/it]\u001b[A\n","Steps:  28% 591/2092 [10:03<25:28,  1.02s/it]\u001b[A\n","Steps:  28% 592/2092 [10:04<25:24,  1.02s/it]\u001b[A\n","Steps:  28% 593/2092 [10:05<25:29,  1.02s/it]\u001b[A\n","Steps:  28% 594/2092 [10:06<25:28,  1.02s/it]\u001b[A\n","Steps:  28% 595/2092 [10:07<25:20,  1.02s/it]\u001b[A\n","Steps:  28% 596/2092 [10:08<25:19,  1.02s/it]\u001b[A\n","Steps:  29% 597/2092 [10:09<25:19,  1.02s/it]\u001b[A\n","Steps:  29% 598/2092 [10:10<25:15,  1.01s/it]\u001b[A\n","Steps:  29% 599/2092 [10:11<25:17,  1.02s/it]\u001b[A\n","Steps:  29% 600/2092 [10:12<25:16,  1.02s/it]\u001b[A\n","Steps:  29% 601/2092 [10:13<25:19,  1.02s/it]\u001b[A\n","Steps:  29% 602/2092 [10:14<25:14,  1.02s/it]\u001b[A\n","Steps:  29% 603/2092 [10:15<25:10,  1.01s/it]\u001b[A\n","Steps:  29% 604/2092 [10:16<25:13,  1.02s/it]\u001b[A\n","Steps:  29% 605/2092 [10:17<25:12,  1.02s/it]\u001b[A\n","Steps:  29% 606/2092 [10:18<25:13,  1.02s/it]\u001b[A\n","Steps:  29% 607/2092 [10:19<25:11,  1.02s/it]\u001b[A\n","Steps:  29% 608/2092 [10:20<25:10,  1.02s/it]\u001b[A\n","Steps:  29% 609/2092 [10:21<25:05,  1.01s/it]\u001b[A\n","Steps:  29% 610/2092 [10:22<25:01,  1.01s/it]\u001b[A\n","Steps:  29% 611/2092 [10:23<24:59,  1.01s/it]\u001b[A\n","Steps:  29% 612/2092 [10:24<25:05,  1.02s/it]\u001b[A\n","Steps:  29% 613/2092 [10:25<25:01,  1.02s/it]\u001b[A\n","Steps:  29% 614/2092 [10:26<25:06,  1.02s/it]\u001b[A\n","Steps:  29% 615/2092 [10:27<25:06,  1.02s/it]\u001b[A\n","Steps:  29% 616/2092 [10:28<25:09,  1.02s/it]\u001b[A\n","Steps:  29% 617/2092 [10:29<25:07,  1.02s/it]\u001b[A\n","Steps:  30% 618/2092 [10:30<25:06,  1.02s/it]\u001b[A\n","Steps:  30% 619/2092 [10:31<25:00,  1.02s/it]\u001b[A\n","Steps:  30% 620/2092 [10:32<24:59,  1.02s/it]\u001b[A\n","Steps:  30% 621/2092 [10:33<24:59,  1.02s/it]\u001b[A\n","Steps:  30% 622/2092 [10:34<24:57,  1.02s/it]\u001b[A\n","Steps:  30% 623/2092 [10:35<25:01,  1.02s/it]\u001b[A\n","Steps:  30% 624/2092 [10:36<25:03,  1.02s/it]\u001b[A\n","Steps:  30% 625/2092 [10:37<25:03,  1.03s/it]\u001b[A\n","Steps:  30% 626/2092 [10:38<25:00,  1.02s/it]\u001b[A\n","Steps:  30% 627/2092 [10:39<24:53,  1.02s/it]\u001b[A\n","Steps:  30% 628/2092 [10:40<24:55,  1.02s/it]\u001b[A\n","Steps:  30% 629/2092 [10:41<24:57,  1.02s/it]\u001b[A\n","Steps:  30% 630/2092 [10:42<24:50,  1.02s/it]\u001b[A\n","Steps:  30% 631/2092 [10:43<24:52,  1.02s/it]\u001b[A\n","Steps:  30% 632/2092 [10:44<24:50,  1.02s/it]\u001b[A\n","Steps:  30% 633/2092 [10:45<24:51,  1.02s/it]\u001b[A\n","Steps:  30% 634/2092 [10:46<24:45,  1.02s/it]\u001b[A\n","Steps:  30% 635/2092 [10:47<24:37,  1.01s/it]\u001b[A\n","Steps:  30% 636/2092 [10:48<24:42,  1.02s/it]\u001b[A\n","Steps:  30% 637/2092 [10:49<24:41,  1.02s/it]\u001b[A\n","Steps:  30% 638/2092 [10:50<24:44,  1.02s/it]\u001b[A\n","Steps:  31% 639/2092 [10:51<24:43,  1.02s/it]\u001b[A\n","Steps:  31% 640/2092 [10:52<24:40,  1.02s/it]\u001b[A\n","Steps:  31% 641/2092 [10:54<24:44,  1.02s/it]\u001b[A\n","Steps:  31% 642/2092 [10:55<24:37,  1.02s/it]\u001b[A\n","Steps:  31% 643/2092 [10:56<24:38,  1.02s/it]\u001b[A\n","Steps:  31% 644/2092 [10:57<24:33,  1.02s/it]\u001b[A\n","Steps:  31% 645/2092 [10:58<24:36,  1.02s/it]\u001b[A\n","Steps:  31% 646/2092 [10:59<24:37,  1.02s/it]\u001b[A\n","Steps:  31% 647/2092 [11:00<24:34,  1.02s/it]\u001b[A\n","Steps:  31% 648/2092 [11:01<24:36,  1.02s/it]\u001b[A\n","Steps:  31% 649/2092 [11:02<24:37,  1.02s/it]\u001b[A\n","Steps:  31% 650/2092 [11:03<24:35,  1.02s/it]\u001b[A\n","Steps:  31% 651/2092 [11:04<24:32,  1.02s/it]\u001b[A\n","Steps:  31% 652/2092 [11:05<24:31,  1.02s/it]\u001b[A\n","Steps:  31% 653/2092 [11:06<24:32,  1.02s/it]\u001b[A\n","Steps:  31% 654/2092 [11:07<24:32,  1.02s/it]\u001b[A\n","Steps:  31% 655/2092 [11:08<24:27,  1.02s/it]\u001b[A\n","Steps:  31% 656/2092 [11:09<24:26,  1.02s/it]\u001b[A\n","Steps:  31% 657/2092 [11:10<24:30,  1.02s/it]\u001b[A\n","Steps:  31% 658/2092 [11:11<24:30,  1.03s/it]\u001b[A\n","Steps:  32% 659/2092 [11:12<24:26,  1.02s/it]\u001b[A\n","Steps:  32% 660/2092 [11:13<24:19,  1.02s/it]\u001b[A\n","Steps:  32% 661/2092 [11:14<24:18,  1.02s/it]\u001b[A\n","Steps:  32% 662/2092 [11:15<24:16,  1.02s/it]\u001b[A\n","Steps:  32% 663/2092 [11:16<24:19,  1.02s/it]\u001b[A\n","Steps:  32% 664/2092 [11:17<24:17,  1.02s/it]\u001b[A\n","Steps:  32% 665/2092 [11:18<24:19,  1.02s/it]\u001b[A\n","Steps:  32% 666/2092 [11:19<24:20,  1.02s/it]\u001b[A\n","Steps:  32% 667/2092 [11:20<24:11,  1.02s/it]\u001b[A\n","Steps:  32% 668/2092 [11:21<24:10,  1.02s/it]\u001b[A\n","Steps:  32% 669/2092 [11:22<24:04,  1.02s/it]\u001b[A\n","Steps:  32% 670/2092 [11:23<24:05,  1.02s/it]\u001b[A\n","Steps:  32% 671/2092 [11:24<24:05,  1.02s/it]\u001b[A\n","Steps:  32% 672/2092 [11:25<24:09,  1.02s/it]\u001b[A\n","Steps:  32% 673/2092 [11:26<24:08,  1.02s/it]\u001b[A\n","Steps:  32% 674/2092 [11:27<24:00,  1.02s/it]\u001b[A\n","Steps:  32% 675/2092 [11:28<24:05,  1.02s/it]\u001b[A\n","Steps:  32% 676/2092 [11:29<23:59,  1.02s/it]\u001b[A\n","Steps:  32% 677/2092 [11:30<23:56,  1.02s/it]\u001b[A\n","Steps:  32% 678/2092 [11:31<23:56,  1.02s/it]\u001b[A\n","Steps:  32% 679/2092 [11:32<23:57,  1.02s/it]\u001b[A\n","Steps:  33% 680/2092 [11:33<23:53,  1.02s/it]\u001b[A\n","Steps:  33% 681/2092 [11:34<23:54,  1.02s/it]\u001b[A\n","Steps:  33% 682/2092 [11:35<23:55,  1.02s/it]\u001b[A\n","Steps:  33% 683/2092 [11:36<23:59,  1.02s/it]\u001b[A\n","Steps:  33% 684/2092 [11:37<24:01,  1.02s/it]\u001b[A\n","Steps:  33% 685/2092 [11:38<23:58,  1.02s/it]\u001b[A\n","Steps:  33% 686/2092 [11:39<23:56,  1.02s/it]\u001b[A\n","Steps:  33% 687/2092 [11:40<23:56,  1.02s/it]\u001b[A\n","Steps:  33% 688/2092 [11:41<23:49,  1.02s/it]\u001b[A\n","Steps:  33% 689/2092 [11:42<23:42,  1.01s/it]\u001b[A\n","Steps:  33% 690/2092 [11:43<23:42,  1.01s/it]\u001b[A\n","Steps:  33% 691/2092 [11:44<23:43,  1.02s/it]\u001b[A\n","Steps:  33% 692/2092 [11:46<23:42,  1.02s/it]\u001b[A\n","Steps:  33% 693/2092 [11:47<23:39,  1.01s/it]\u001b[A\n","Steps:  33% 694/2092 [11:48<23:42,  1.02s/it]\u001b[A\n","Steps:  33% 695/2092 [11:49<23:35,  1.01s/it]\u001b[A\n","Steps:  33% 696/2092 [11:50<23:36,  1.01s/it]\u001b[A\n","Steps:  33% 697/2092 [11:51<23:33,  1.01s/it]\u001b[A\n","Steps:  33% 698/2092 [11:52<23:39,  1.02s/it]\u001b[A\n","Steps:  33% 699/2092 [11:53<23:37,  1.02s/it]\u001b[A\n","Steps:  33% 700/2092 [11:54<23:39,  1.02s/it]\u001b[A\n","Steps:  34% 701/2092 [11:55<23:39,  1.02s/it]\u001b[A\n","Steps:  34% 702/2092 [11:56<23:40,  1.02s/it]\u001b[A\n","Steps:  34% 703/2092 [11:57<23:37,  1.02s/it]\u001b[A\n","Steps:  34% 704/2092 [11:58<23:36,  1.02s/it]\u001b[A\n","Steps:  34% 705/2092 [11:59<23:31,  1.02s/it]\u001b[A\n","Steps:  34% 706/2092 [12:00<23:34,  1.02s/it]\u001b[A\n","Steps:  34% 707/2092 [12:01<23:34,  1.02s/it]\u001b[A\n","Steps:  34% 708/2092 [12:02<23:32,  1.02s/it]\u001b[A\n","Steps:  34% 709/2092 [12:03<23:31,  1.02s/it]\u001b[A\n","Steps:  34% 710/2092 [12:04<23:26,  1.02s/it]\u001b[A\n","Steps:  34% 711/2092 [12:05<23:25,  1.02s/it]\u001b[A\n","Steps:  34% 712/2092 [12:06<23:25,  1.02s/it]\u001b[A\n","Steps:  34% 713/2092 [12:07<23:19,  1.01s/it]\u001b[A\n","Steps:  34% 714/2092 [12:08<23:24,  1.02s/it]\u001b[A\n","Steps:  34% 715/2092 [12:09<23:19,  1.02s/it]\u001b[A\n","Steps:  34% 716/2092 [12:10<23:15,  1.01s/it]\u001b[A\n","Steps:  34% 717/2092 [12:11<23:20,  1.02s/it]\u001b[A\n","Steps:  34% 718/2092 [12:12<23:24,  1.02s/it]\u001b[A\n","Steps:  34% 719/2092 [12:13<23:21,  1.02s/it]\u001b[A\n","Steps:  34% 720/2092 [12:14<23:16,  1.02s/it]\u001b[A\n","Steps:  34% 721/2092 [12:15<23:15,  1.02s/it]\u001b[A\n","Steps:  35% 722/2092 [12:16<23:14,  1.02s/it]\u001b[A\n","Steps:  35% 723/2092 [12:17<23:18,  1.02s/it]\u001b[A\n","Steps:  35% 724/2092 [12:18<23:18,  1.02s/it]\u001b[A\n","Steps:  35% 725/2092 [12:19<23:11,  1.02s/it]\u001b[A\n","Steps:  35% 726/2092 [12:20<23:06,  1.02s/it]\u001b[A\n","Steps:  35% 727/2092 [12:21<23:08,  1.02s/it]\u001b[A\n","Steps:  35% 728/2092 [12:22<23:04,  1.01s/it]\u001b[A\n","Steps:  35% 729/2092 [12:23<23:09,  1.02s/it]\u001b[A\n","Steps:  35% 730/2092 [12:24<23:11,  1.02s/it]\u001b[A\n","Steps:  35% 731/2092 [12:25<23:08,  1.02s/it]\u001b[A\n","Steps:  35% 732/2092 [12:26<23:07,  1.02s/it]\u001b[A\n","Steps:  35% 733/2092 [12:27<23:01,  1.02s/it]\u001b[A\n","Steps:  35% 734/2092 [12:28<23:01,  1.02s/it]\u001b[A\n","Steps:  35% 735/2092 [12:29<23:04,  1.02s/it]\u001b[A\n","Steps:  35% 736/2092 [12:30<23:00,  1.02s/it]\u001b[A\n","Steps:  35% 737/2092 [12:31<22:58,  1.02s/it]\u001b[A\n","Steps:  35% 738/2092 [12:32<23:02,  1.02s/it]\u001b[A\n","Steps:  35% 739/2092 [12:33<23:00,  1.02s/it]\u001b[A\n","Steps:  35% 740/2092 [12:34<22:55,  1.02s/it]\u001b[A\n","Steps:  35% 741/2092 [12:35<22:55,  1.02s/it]\u001b[A\n","Steps:  35% 742/2092 [12:36<22:59,  1.02s/it]\u001b[A\n","Steps:  36% 743/2092 [12:37<23:01,  1.02s/it]\u001b[A\n","Steps:  36% 744/2092 [12:38<22:54,  1.02s/it]\u001b[A\n","Steps:  36% 745/2092 [12:40<22:53,  1.02s/it]\u001b[A\n","Steps:  36% 746/2092 [12:41<22:49,  1.02s/it]\u001b[A\n","Steps:  36% 747/2092 [12:42<22:46,  1.02s/it]\u001b[A\n","Steps:  36% 748/2092 [12:43<22:46,  1.02s/it]\u001b[A\n","Steps:  36% 749/2092 [12:44<22:45,  1.02s/it]\u001b[A\n","Steps:  36% 750/2092 [12:45<22:44,  1.02s/it]\u001b[A\n","Steps:  36% 751/2092 [12:46<22:46,  1.02s/it]\u001b[A\n","Steps:  36% 752/2092 [12:47<22:41,  1.02s/it]\u001b[A\n","Steps:  36% 753/2092 [12:48<22:46,  1.02s/it]\u001b[A\n","Steps:  36% 754/2092 [12:49<22:47,  1.02s/it]\u001b[A\n","Steps:  36% 755/2092 [12:50<22:41,  1.02s/it]\u001b[A\n","Steps:  36% 756/2092 [12:51<22:33,  1.01s/it]\u001b[A\n","Steps:  36% 757/2092 [12:52<22:29,  1.01s/it]\u001b[A\n","Steps:  36% 758/2092 [12:53<22:30,  1.01s/it]\u001b[A\n","Steps:  36% 759/2092 [12:54<22:36,  1.02s/it]\u001b[A\n","Steps:  36% 760/2092 [12:55<22:38,  1.02s/it]\u001b[A\n","Steps:  36% 761/2092 [12:56<22:39,  1.02s/it]\u001b[A\n","Steps:  36% 762/2092 [12:57<22:37,  1.02s/it]\u001b[A\n","Steps:  36% 763/2092 [12:58<22:35,  1.02s/it]\u001b[A\n","Steps:  37% 764/2092 [12:59<22:34,  1.02s/it]\u001b[A\n","Steps:  37% 765/2092 [13:00<22:36,  1.02s/it]\u001b[A\n","Steps:  37% 766/2092 [13:01<22:38,  1.02s/it]\u001b[A\n","Steps:  37% 767/2092 [13:02<22:37,  1.02s/it]\u001b[A\n","Steps:  37% 768/2092 [13:03<22:33,  1.02s/it]\u001b[A\n","Steps:  37% 769/2092 [13:04<22:27,  1.02s/it]\u001b[A\n","Steps:  37% 770/2092 [13:05<22:22,  1.02s/it]\u001b[A\n","Steps:  37% 771/2092 [13:06<22:22,  1.02s/it]\u001b[A\n","Steps:  37% 772/2092 [13:07<22:24,  1.02s/it]\u001b[A\n","Steps:  37% 773/2092 [13:08<22:19,  1.02s/it]\u001b[A\n","Steps:  37% 774/2092 [13:09<22:15,  1.01s/it]\u001b[A\n","Steps:  37% 775/2092 [13:10<22:17,  1.02s/it]\u001b[A\n","Steps:  37% 776/2092 [13:11<22:18,  1.02s/it]\u001b[A\n","Steps:  37% 777/2092 [13:12<22:22,  1.02s/it]\u001b[A\n","Steps:  37% 778/2092 [13:13<22:14,  1.02s/it]\u001b[A\n","Steps:  37% 779/2092 [13:14<22:15,  1.02s/it]\u001b[A\n","Steps:  37% 780/2092 [13:15<22:18,  1.02s/it]\u001b[A\n","Steps:  37% 781/2092 [13:16<22:18,  1.02s/it]\u001b[A\n","Steps:  37% 782/2092 [13:17<22:20,  1.02s/it]\u001b[A\n","Steps:  37% 783/2092 [13:18<22:10,  1.02s/it]\u001b[A\n","Steps:  37% 784/2092 [13:19<22:06,  1.01s/it]\u001b[A\n","Steps:  38% 785/2092 [13:20<22:03,  1.01s/it]\u001b[A\n","Steps:  38% 786/2092 [13:21<22:08,  1.02s/it]\u001b[A\n","Steps:  38% 787/2092 [13:22<22:12,  1.02s/it]\u001b[A\n","Steps:  38% 788/2092 [13:23<22:10,  1.02s/it]\u001b[A\n","Steps:  38% 789/2092 [13:24<22:12,  1.02s/it]\u001b[A\n","Steps:  38% 790/2092 [13:25<22:10,  1.02s/it]\u001b[A\n","Steps:  38% 791/2092 [13:26<22:12,  1.02s/it]\u001b[A\n","Steps:  38% 792/2092 [13:27<22:10,  1.02s/it]\u001b[A\n","Steps:  38% 793/2092 [13:28<22:12,  1.03s/it]\u001b[A\n","Steps:  38% 794/2092 [13:29<22:11,  1.03s/it]\u001b[A\n","Steps:  38% 795/2092 [13:30<22:09,  1.03s/it]\u001b[A\n","Steps:  38% 796/2092 [13:31<22:05,  1.02s/it]\u001b[A\n","Steps:  38% 797/2092 [13:32<22:02,  1.02s/it]\u001b[A\n","Steps:  38% 798/2092 [13:34<22:05,  1.02s/it]\u001b[A\n","Steps:  38% 799/2092 [13:35<22:03,  1.02s/it]\u001b[A\n","Steps:  38% 800/2092 [13:36<22:03,  1.02s/it]\u001b[A\n","Steps:  38% 801/2092 [13:37<22:00,  1.02s/it]\u001b[A\n","Steps:  38% 802/2092 [13:38<22:01,  1.02s/it]\u001b[A\n","Steps:  38% 803/2092 [13:39<22:00,  1.02s/it]\u001b[A\n","Steps:  38% 804/2092 [13:40<21:59,  1.02s/it]\u001b[A\n","Steps:  38% 805/2092 [13:41<21:57,  1.02s/it]\u001b[A\n","Steps:  39% 806/2092 [13:42<21:55,  1.02s/it]\u001b[A\n","Steps:  39% 807/2092 [13:43<21:55,  1.02s/it]\u001b[A\n","Steps:  39% 808/2092 [13:44<21:55,  1.02s/it]\u001b[A\n","Steps:  39% 809/2092 [13:45<21:54,  1.02s/it]\u001b[A\n","Steps:  39% 810/2092 [13:46<21:54,  1.03s/it]\u001b[A\n","Steps:  39% 811/2092 [13:47<21:54,  1.03s/it]\u001b[A\n","Steps:  39% 812/2092 [13:48<21:50,  1.02s/it]\u001b[A\n","Steps:  39% 813/2092 [13:49<21:48,  1.02s/it]\u001b[A\n","Steps:  39% 814/2092 [13:50<21:48,  1.02s/it]\u001b[A\n","Steps:  39% 815/2092 [13:51<21:47,  1.02s/it]\u001b[A\n","Steps:  39% 816/2092 [13:52<21:46,  1.02s/it]\u001b[A\n","Steps:  39% 817/2092 [13:53<21:47,  1.03s/it]\u001b[A\n","Steps:  39% 818/2092 [13:54<21:46,  1.03s/it]\u001b[A\n","Steps:  39% 819/2092 [13:55<21:39,  1.02s/it]\u001b[A\n","Steps:  39% 820/2092 [13:56<21:40,  1.02s/it]\u001b[A\n","Steps:  39% 821/2092 [13:57<21:34,  1.02s/it]\u001b[A\n","Steps:  39% 822/2092 [13:58<21:37,  1.02s/it]\u001b[A\n","Steps:  39% 823/2092 [13:59<21:38,  1.02s/it]\u001b[A\n","Steps:  39% 824/2092 [14:00<21:36,  1.02s/it]\u001b[A\n","Steps:  39% 825/2092 [14:01<21:37,  1.02s/it]\u001b[A\n","Steps:  39% 826/2092 [14:02<21:35,  1.02s/it]\u001b[A\n","Steps:  40% 827/2092 [14:03<21:27,  1.02s/it]\u001b[A\n","Steps:  40% 828/2092 [14:04<21:29,  1.02s/it]\u001b[A\n","Steps:  40% 829/2092 [14:05<21:27,  1.02s/it]\u001b[A\n","Steps:  40% 830/2092 [14:06<21:26,  1.02s/it]\u001b[A\n","Steps:  40% 831/2092 [14:07<21:29,  1.02s/it]\u001b[A\n","Steps:  40% 832/2092 [14:08<21:22,  1.02s/it]\u001b[A\n","Steps:  40% 833/2092 [14:09<21:19,  1.02s/it]\u001b[A\n","Steps:  40% 834/2092 [14:10<21:22,  1.02s/it]\u001b[A\n","Steps:  40% 835/2092 [14:11<21:24,  1.02s/it]\u001b[A\n","Steps:  40% 836/2092 [14:12<21:19,  1.02s/it]\u001b[A\n","Steps:  40% 837/2092 [14:13<21:21,  1.02s/it]\u001b[A\n","Steps:  40% 838/2092 [14:14<21:23,  1.02s/it]\u001b[A\n","Steps:  40% 839/2092 [14:15<21:21,  1.02s/it]\u001b[A\n","Steps:  40% 840/2092 [14:16<21:19,  1.02s/it]\u001b[A\n","Steps:  40% 841/2092 [14:17<21:13,  1.02s/it]\u001b[A\n","Steps:  40% 842/2092 [14:18<21:10,  1.02s/it]\u001b[A\n","Steps:  40% 843/2092 [14:20<21:12,  1.02s/it]\u001b[A\n","Steps:  40% 844/2092 [14:21<21:11,  1.02s/it]\u001b[A\n","Steps:  40% 845/2092 [14:22<21:10,  1.02s/it]\u001b[A\n","Steps:  40% 846/2092 [14:23<21:11,  1.02s/it]\u001b[A\n","Steps:  40% 847/2092 [14:24<21:11,  1.02s/it]\u001b[A\n","Steps:  41% 848/2092 [14:25<21:09,  1.02s/it]\u001b[A\n","Steps:  41% 849/2092 [14:26<21:11,  1.02s/it]\u001b[A\n","Steps:  41% 850/2092 [14:27<21:09,  1.02s/it]\u001b[A\n","Steps:  41% 851/2092 [14:28<21:08,  1.02s/it]\u001b[A\n","Steps:  41% 852/2092 [14:29<21:09,  1.02s/it]\u001b[A\n","Steps:  41% 853/2092 [14:30<21:09,  1.02s/it]\u001b[A\n","Steps:  41% 854/2092 [14:31<21:08,  1.02s/it]\u001b[A\n","Steps:  41% 855/2092 [14:32<21:05,  1.02s/it]\u001b[A\n","Steps:  41% 856/2092 [14:33<21:03,  1.02s/it]\u001b[A\n","Steps:  41% 857/2092 [14:34<20:57,  1.02s/it]\u001b[A\n","Steps:  41% 858/2092 [14:35<21:00,  1.02s/it]\u001b[A\n","Steps:  41% 859/2092 [14:36<20:55,  1.02s/it]\u001b[A\n","Steps:  41% 860/2092 [14:37<20:54,  1.02s/it]\u001b[A\n","Steps:  41% 861/2092 [14:38<20:50,  1.02s/it]\u001b[A\n","Steps:  41% 862/2092 [14:39<20:50,  1.02s/it]\u001b[A\n","Steps:  41% 863/2092 [14:40<20:47,  1.01s/it]\u001b[A\n","Steps:  41% 864/2092 [14:41<20:43,  1.01s/it]\u001b[A\n","Steps:  41% 865/2092 [14:42<20:44,  1.01s/it]\u001b[A\n","Steps:  41% 866/2092 [14:43<20:44,  1.02s/it]\u001b[A\n","Steps:  41% 867/2092 [14:44<20:46,  1.02s/it]\u001b[A\n","Steps:  41% 868/2092 [14:45<20:45,  1.02s/it]\u001b[A\n","Steps:  42% 869/2092 [14:46<20:45,  1.02s/it]\u001b[A\n","Steps:  42% 870/2092 [14:47<20:47,  1.02s/it]\u001b[A\n","Steps:  42% 871/2092 [14:48<20:49,  1.02s/it]\u001b[A\n","Steps:  42% 872/2092 [14:49<20:49,  1.02s/it]\u001b[A\n","Steps:  42% 873/2092 [14:50<20:47,  1.02s/it]\u001b[A\n","Steps:  42% 874/2092 [14:51<20:41,  1.02s/it]\u001b[A\n","Steps:  42% 875/2092 [14:52<20:39,  1.02s/it]\u001b[A\n","Steps:  42% 876/2092 [14:53<20:39,  1.02s/it]\u001b[A\n","Steps:  42% 877/2092 [14:54<20:42,  1.02s/it]\u001b[A\n","Steps:  42% 878/2092 [14:55<20:39,  1.02s/it]\u001b[A\n","Steps:  42% 879/2092 [14:56<20:41,  1.02s/it]\u001b[A\n","Steps:  42% 880/2092 [14:57<20:41,  1.02s/it]\u001b[A\n","Steps:  42% 881/2092 [14:58<20:41,  1.03s/it]\u001b[A\n","Steps:  42% 882/2092 [14:59<20:35,  1.02s/it]\u001b[A\n","Steps:  42% 883/2092 [15:00<20:32,  1.02s/it]\u001b[A\n","Steps:  42% 884/2092 [15:01<20:31,  1.02s/it]\u001b[A\n","Steps:  42% 885/2092 [15:02<20:31,  1.02s/it]\u001b[A\n","Steps:  42% 886/2092 [15:03<20:26,  1.02s/it]\u001b[A\n","Steps:  42% 887/2092 [15:04<20:30,  1.02s/it]\u001b[A\n","Steps:  42% 888/2092 [15:05<20:30,  1.02s/it]\u001b[A\n","Steps:  42% 889/2092 [15:06<20:30,  1.02s/it]\u001b[A\n","Steps:  43% 890/2092 [15:07<20:30,  1.02s/it]\u001b[A\n","Steps:  43% 891/2092 [15:09<20:31,  1.03s/it]\u001b[A\n","Steps:  43% 892/2092 [15:10<20:28,  1.02s/it]\u001b[A\n","Steps:  43% 893/2092 [15:11<20:28,  1.02s/it]\u001b[A\n","Steps:  43% 894/2092 [15:12<20:24,  1.02s/it]\u001b[A\n","Steps:  43% 895/2092 [15:13<20:25,  1.02s/it]\u001b[A\n","Steps:  43% 896/2092 [15:14<20:26,  1.03s/it]\u001b[A\n","Steps:  43% 897/2092 [15:15<20:26,  1.03s/it]\u001b[A\n","Steps:  43% 898/2092 [15:16<20:26,  1.03s/it]\u001b[A\n","Steps:  43% 899/2092 [15:17<20:26,  1.03s/it]\u001b[A\n","Steps:  43% 900/2092 [15:18<20:23,  1.03s/it]\u001b[A\n","Steps:  43% 901/2092 [15:19<20:20,  1.02s/it]\u001b[A\n","Steps:  43% 902/2092 [15:20<20:16,  1.02s/it]\u001b[A\n","Steps:  43% 903/2092 [15:21<20:14,  1.02s/it]\u001b[A\n","Steps:  43% 904/2092 [15:22<20:12,  1.02s/it]\u001b[A\n","Steps:  43% 905/2092 [15:23<20:09,  1.02s/it]\u001b[A\n","Steps:  43% 906/2092 [15:24<20:03,  1.02s/it]\u001b[A\n","Steps:  43% 907/2092 [15:25<19:59,  1.01s/it]\u001b[A\n","Steps:  43% 908/2092 [15:26<20:03,  1.02s/it]\u001b[A\n","Steps:  43% 909/2092 [15:27<20:06,  1.02s/it]\u001b[A\n","Steps:  43% 910/2092 [15:28<20:08,  1.02s/it]\u001b[A\n","Steps:  44% 911/2092 [15:29<20:09,  1.02s/it]\u001b[A\n","Steps:  44% 912/2092 [15:30<20:07,  1.02s/it]\u001b[A\n","Steps:  44% 913/2092 [15:31<20:01,  1.02s/it]\u001b[A\n","Steps:  44% 914/2092 [15:32<20:03,  1.02s/it]\u001b[A\n","Steps:  44% 915/2092 [15:33<19:57,  1.02s/it]\u001b[A\n","Steps:  44% 916/2092 [15:34<19:52,  1.01s/it]\u001b[A\n","Steps:  44% 917/2092 [15:35<19:56,  1.02s/it]\u001b[A\n","Steps:  44% 918/2092 [15:36<19:58,  1.02s/it]\u001b[A\n","Steps:  44% 919/2092 [15:37<19:56,  1.02s/it]\u001b[A\n","Steps:  44% 920/2092 [15:38<19:59,  1.02s/it]\u001b[A\n","Steps:  44% 921/2092 [15:39<19:56,  1.02s/it]\u001b[A\n","Steps:  44% 922/2092 [15:40<19:54,  1.02s/it]\u001b[A\n","Steps:  44% 923/2092 [15:41<19:55,  1.02s/it]\u001b[A\n","Steps:  44% 924/2092 [15:42<19:52,  1.02s/it]\u001b[A\n","Steps:  44% 925/2092 [15:43<19:50,  1.02s/it]\u001b[A\n","Steps:  44% 926/2092 [15:44<19:52,  1.02s/it]\u001b[A\n","Steps:  44% 927/2092 [15:45<19:50,  1.02s/it]\u001b[A\n","Steps:  44% 928/2092 [15:46<19:51,  1.02s/it]\u001b[A\n","Steps:  44% 929/2092 [15:47<19:49,  1.02s/it]\u001b[A\n","Steps:  44% 930/2092 [15:48<19:47,  1.02s/it]\u001b[A\n","Steps:  45% 931/2092 [15:49<19:49,  1.02s/it]\u001b[A\n","Steps:  45% 932/2092 [15:50<19:49,  1.03s/it]\u001b[A\n","Steps:  45% 933/2092 [15:51<19:48,  1.03s/it]\u001b[A\n","Steps:  45% 934/2092 [15:52<19:44,  1.02s/it]\u001b[A\n","Steps:  45% 935/2092 [15:53<19:45,  1.02s/it]\u001b[A\n","Steps:  45% 936/2092 [15:54<19:40,  1.02s/it]\u001b[A\n","Steps:  45% 937/2092 [15:56<19:41,  1.02s/it]\u001b[A\n","Steps:  45% 938/2092 [15:57<19:38,  1.02s/it]\u001b[A\n","Steps:  45% 939/2092 [15:58<19:37,  1.02s/it]\u001b[A\n","Steps:  45% 940/2092 [15:59<19:35,  1.02s/it]\u001b[A\n","Steps:  45% 941/2092 [16:00<19:37,  1.02s/it]\u001b[A\n","Steps:  45% 942/2092 [16:01<19:38,  1.02s/it]\u001b[A\n","Steps:  45% 943/2092 [16:02<19:32,  1.02s/it]\u001b[A\n","Steps:  45% 944/2092 [16:03<19:34,  1.02s/it]\u001b[A\n","Steps:  45% 945/2092 [16:04<19:32,  1.02s/it]\u001b[A\n","Steps:  45% 946/2092 [16:05<19:29,  1.02s/it]\u001b[A\n","Steps:  45% 947/2092 [16:06<19:32,  1.02s/it]\u001b[A\n","Steps:  45% 948/2092 [16:07<19:29,  1.02s/it]\u001b[A\n","Steps:  45% 949/2092 [16:08<19:29,  1.02s/it]\u001b[A\n","Steps:  45% 950/2092 [16:09<19:26,  1.02s/it]\u001b[A\n","Steps:  45% 951/2092 [16:10<19:28,  1.02s/it]\u001b[A\n","Steps:  46% 952/2092 [16:11<19:29,  1.03s/it]\u001b[A\n","Steps:  46% 953/2092 [16:12<19:29,  1.03s/it]\u001b[A\n","Steps:  46% 954/2092 [16:13<19:22,  1.02s/it]\u001b[A\n","Steps:  46% 955/2092 [16:14<19:20,  1.02s/it]\u001b[A\n","Steps:  46% 956/2092 [16:15<19:18,  1.02s/it]\u001b[A\n","Steps:  46% 957/2092 [16:16<19:20,  1.02s/it]\u001b[A\n","Steps:  46% 958/2092 [16:17<19:21,  1.02s/it]\u001b[A\n","Steps:  46% 959/2092 [16:18<19:22,  1.03s/it]\u001b[A\n","Steps:  46% 960/2092 [16:19<19:15,  1.02s/it]\u001b[A\n","Steps:  46% 961/2092 [16:20<19:11,  1.02s/it]\u001b[A\n","Steps:  46% 962/2092 [16:21<19:14,  1.02s/it]\u001b[A\n","Steps:  46% 963/2092 [16:22<19:16,  1.02s/it]\u001b[A\n","Steps:  46% 964/2092 [16:23<19:13,  1.02s/it]\u001b[A\n","Steps:  46% 965/2092 [16:24<19:08,  1.02s/it]\u001b[A\n","Steps:  46% 966/2092 [16:25<19:04,  1.02s/it]\u001b[A\n","Steps:  46% 967/2092 [16:26<19:03,  1.02s/it]\u001b[A\n","Steps:  46% 968/2092 [16:27<19:06,  1.02s/it]\u001b[A\n","Steps:  46% 969/2092 [16:28<19:05,  1.02s/it]\u001b[A\n","Steps:  46% 970/2092 [16:29<19:05,  1.02s/it]\u001b[A\n","Steps:  46% 971/2092 [16:30<19:00,  1.02s/it]\u001b[A\n","Steps:  46% 972/2092 [16:31<19:00,  1.02s/it]\u001b[A\n","Steps:  47% 973/2092 [16:32<18:59,  1.02s/it]\u001b[A\n","Steps:  47% 974/2092 [16:33<19:00,  1.02s/it]\u001b[A\n","Steps:  47% 975/2092 [16:34<19:02,  1.02s/it]\u001b[A\n","Steps:  47% 976/2092 [16:35<18:59,  1.02s/it]\u001b[A\n","Steps:  47% 977/2092 [16:36<18:57,  1.02s/it]\u001b[A\n","Steps:  47% 978/2092 [16:37<18:53,  1.02s/it]\u001b[A\n","Steps:  47% 979/2092 [16:38<18:53,  1.02s/it]\u001b[A\n","Steps:  47% 980/2092 [16:39<18:52,  1.02s/it]\u001b[A\n","Steps:  47% 981/2092 [16:40<18:48,  1.02s/it]\u001b[A\n","Steps:  47% 982/2092 [16:41<18:48,  1.02s/it]\u001b[A\n","Steps:  47% 983/2092 [16:42<18:48,  1.02s/it]\u001b[A\n","Steps:  47% 984/2092 [16:43<18:49,  1.02s/it]\u001b[A\n","Steps:  47% 985/2092 [16:45<18:51,  1.02s/it]\u001b[A\n","Steps:  47% 986/2092 [16:46<18:46,  1.02s/it]\u001b[A\n","Steps:  47% 987/2092 [16:47<18:45,  1.02s/it]\u001b[A\n","Steps:  47% 988/2092 [16:48<18:45,  1.02s/it]\u001b[A\n","Steps:  47% 989/2092 [16:49<18:43,  1.02s/it]\u001b[A\n","Steps:  47% 990/2092 [16:50<18:45,  1.02s/it]\u001b[A\n","Steps:  47% 991/2092 [16:51<18:41,  1.02s/it]\u001b[A\n","Steps:  47% 992/2092 [16:52<18:43,  1.02s/it]\u001b[A\n","Steps:  47% 993/2092 [16:53<18:41,  1.02s/it]\u001b[A\n","Steps:  48% 994/2092 [16:54<18:43,  1.02s/it]\u001b[A\n","Steps:  48% 995/2092 [16:55<18:38,  1.02s/it]\u001b[A\n","Steps:  48% 996/2092 [16:56<18:34,  1.02s/it]\u001b[A\n","Steps:  48% 997/2092 [16:57<18:37,  1.02s/it]\u001b[A\n","Steps:  48% 998/2092 [16:58<18:32,  1.02s/it]\u001b[A\n","Steps:  48% 999/2092 [16:59<18:29,  1.02s/it]\u001b[A\n","Steps:  48% 1000/2092 [17:00<18:29,  1.02s/it]\u001b[A\n","Steps:  48% 1001/2092 [17:01<18:29,  1.02s/it]\u001b[A\n","Steps:  48% 1002/2092 [17:02<18:29,  1.02s/it]\u001b[A\n","Steps:  48% 1003/2092 [17:03<18:29,  1.02s/it]\u001b[A\n","Steps:  48% 1004/2092 [17:04<18:28,  1.02s/it]\u001b[A\n","Steps:  48% 1005/2092 [17:05<18:28,  1.02s/it]\u001b[A\n","Steps:  48% 1006/2092 [17:06<18:30,  1.02s/it]\u001b[A\n","Steps:  48% 1007/2092 [17:07<18:26,  1.02s/it]\u001b[A\n","Steps:  48% 1008/2092 [17:08<18:21,  1.02s/it]\u001b[A\n","Steps:  48% 1009/2092 [17:09<18:21,  1.02s/it]\u001b[A\n","Steps:  48% 1010/2092 [17:10<18:20,  1.02s/it]\u001b[A\n","Steps:  48% 1011/2092 [17:11<18:21,  1.02s/it]\u001b[A\n","Steps:  48% 1012/2092 [17:12<18:16,  1.02s/it]\u001b[A\n","Steps:  48% 1013/2092 [17:13<18:19,  1.02s/it]\u001b[A\n","Steps:  48% 1014/2092 [17:14<18:18,  1.02s/it]\u001b[A\n","Steps:  49% 1015/2092 [17:15<18:16,  1.02s/it]\u001b[A\n","Steps:  49% 1016/2092 [17:16<18:13,  1.02s/it]\u001b[A\n","Steps:  49% 1017/2092 [17:17<18:10,  1.01s/it]\u001b[A\n","Steps:  49% 1018/2092 [17:18<18:11,  1.02s/it]\u001b[A\n","Steps:  49% 1019/2092 [17:19<18:12,  1.02s/it]\u001b[A\n","Steps:  49% 1020/2092 [17:20<18:13,  1.02s/it]\u001b[A\n","Steps:  49% 1021/2092 [17:21<18:09,  1.02s/it]\u001b[A\n","Steps:  49% 1022/2092 [17:22<18:05,  1.01s/it]\u001b[A\n","Steps:  49% 1023/2092 [17:23<18:07,  1.02s/it]\u001b[A\n","Steps:  49% 1024/2092 [17:24<18:06,  1.02s/it]\u001b[A\n","Steps:  49% 1025/2092 [17:25<18:05,  1.02s/it]\u001b[A\n","Steps:  49% 1026/2092 [17:26<18:08,  1.02s/it]\u001b[A\n","Steps:  49% 1027/2092 [17:27<18:07,  1.02s/it]\u001b[A\n","Steps:  49% 1028/2092 [17:28<18:05,  1.02s/it]\u001b[A\n","Steps:  49% 1029/2092 [17:29<18:06,  1.02s/it]\u001b[A\n","Steps:  49% 1030/2092 [17:30<18:07,  1.02s/it]\u001b[A\n","Steps:  49% 1031/2092 [17:31<18:07,  1.02s/it]\u001b[A\n","Steps:  49% 1032/2092 [17:32<18:08,  1.03s/it]\u001b[A\n","Steps:  49% 1033/2092 [17:33<18:04,  1.02s/it]\u001b[A\n","Steps:  49% 1034/2092 [17:34<18:01,  1.02s/it]\u001b[A\n","Steps:  49% 1035/2092 [17:35<17:56,  1.02s/it]\u001b[A\n","Steps:  50% 1036/2092 [17:36<17:58,  1.02s/it]\u001b[A\n","Steps:  50% 1037/2092 [17:38<17:57,  1.02s/it]\u001b[A\n","Steps:  50% 1038/2092 [17:39<17:56,  1.02s/it]\u001b[A\n","Steps:  50% 1039/2092 [17:40<17:57,  1.02s/it]\u001b[A\n","Steps:  50% 1040/2092 [17:41<17:54,  1.02s/it]\u001b[A\n","Steps:  50% 1041/2092 [17:42<17:53,  1.02s/it]\u001b[A\n","Steps:  50% 1042/2092 [17:43<17:48,  1.02s/it]\u001b[A\n","Steps:  50% 1043/2092 [17:44<17:47,  1.02s/it]\u001b[A\n","Steps:  50% 1044/2092 [17:45<17:47,  1.02s/it]\u001b[A\n","Steps:  50% 1045/2092 [17:46<17:49,  1.02s/it]\u001b[A\n","Steps:  50% 1046/2092 [17:47<17:50,  1.02s/it]\u001b[A\n","Steps:  50% 1047/2092 [17:48<17:51,  1.02s/it]\u001b[A\n","Steps:  50% 1048/2092 [17:49<17:50,  1.03s/it]\u001b[A\n","Steps:  50% 1049/2092 [17:50<17:50,  1.03s/it]\u001b[A\n","Steps:  50% 1050/2092 [17:51<17:50,  1.03s/it]\u001b[A\n","Steps:  50% 1051/2092 [17:52<17:50,  1.03s/it]\u001b[A\n","Steps:  50% 1052/2092 [17:53<17:43,  1.02s/it]\u001b[A\n","Steps:  50% 1053/2092 [17:54<17:43,  1.02s/it]\u001b[A\n","Steps:  50% 1054/2092 [17:55<17:40,  1.02s/it]\u001b[A\n","Steps:  50% 1055/2092 [17:56<17:38,  1.02s/it]\u001b[A\n","Steps:  50% 1056/2092 [17:57<17:32,  1.02s/it]\u001b[A\n","Steps:  51% 1057/2092 [17:58<17:31,  1.02s/it]\u001b[A\n","Steps:  51% 1058/2092 [17:59<17:34,  1.02s/it]\u001b[A\n","Steps:  51% 1059/2092 [18:00<17:33,  1.02s/it]\u001b[A\n","Steps:  51% 1060/2092 [18:01<17:35,  1.02s/it]\u001b[A\n","Steps:  51% 1061/2092 [18:02<17:36,  1.02s/it]\u001b[A\n","Steps:  51% 1062/2092 [18:03<17:33,  1.02s/it]\u001b[A\n","Steps:  51% 1063/2092 [18:04<17:33,  1.02s/it]\u001b[A\n","Steps:  51% 1064/2092 [18:05<17:33,  1.02s/it]\u001b[A\n","Steps:  51% 1065/2092 [18:06<17:33,  1.03s/it]\u001b[A\n","Steps:  51% 1066/2092 [18:07<17:31,  1.03s/it]\u001b[A\n","Steps:  51% 1067/2092 [18:08<17:29,  1.02s/it]\u001b[A\n","Steps:  51% 1068/2092 [18:09<17:27,  1.02s/it]\u001b[A\n","Steps:  51% 1069/2092 [18:10<17:25,  1.02s/it]\u001b[A\n","Steps:  51% 1070/2092 [18:11<17:25,  1.02s/it]\u001b[A\n","Steps:  51% 1071/2092 [18:12<17:20,  1.02s/it]\u001b[A\n","Steps:  51% 1072/2092 [18:13<17:16,  1.02s/it]\u001b[A\n","Steps:  51% 1073/2092 [18:14<17:16,  1.02s/it]\u001b[A\n","Steps:  51% 1074/2092 [18:15<17:16,  1.02s/it]\u001b[A\n","Steps:  51% 1075/2092 [18:16<17:16,  1.02s/it]\u001b[A\n","Steps:  51% 1076/2092 [18:17<17:15,  1.02s/it]\u001b[A\n","Steps:  51% 1077/2092 [18:18<17:14,  1.02s/it]\u001b[A\n","Steps:  52% 1078/2092 [18:19<17:14,  1.02s/it]\u001b[A\n","Steps:  52% 1079/2092 [18:20<17:12,  1.02s/it]\u001b[A\n","Steps:  52% 1080/2092 [18:21<17:13,  1.02s/it]\u001b[A\n","Steps:  52% 1081/2092 [18:22<17:08,  1.02s/it]\u001b[A\n","Steps:  52% 1082/2092 [18:23<17:09,  1.02s/it]\u001b[A\n","Steps:  52% 1083/2092 [18:24<17:05,  1.02s/it]\u001b[A\n","Steps:  52% 1084/2092 [18:26<17:05,  1.02s/it]\u001b[A\n","Steps:  52% 1085/2092 [18:27<17:04,  1.02s/it]\u001b[A\n","Steps:  52% 1086/2092 [18:28<17:05,  1.02s/it]\u001b[A\n","Steps:  52% 1087/2092 [18:29<17:02,  1.02s/it]\u001b[A\n","Steps:  52% 1088/2092 [18:30<17:02,  1.02s/it]\u001b[A\n","Steps:  52% 1089/2092 [18:31<17:04,  1.02s/it]\u001b[A\n","Steps:  52% 1090/2092 [18:32<17:05,  1.02s/it]\u001b[A\n","Steps:  52% 1091/2092 [18:33<17:04,  1.02s/it]\u001b[A\n","Steps:  52% 1092/2092 [18:34<17:04,  1.02s/it]\u001b[A\n","Steps:  52% 1093/2092 [18:35<17:02,  1.02s/it]\u001b[A\n","Steps:  52% 1094/2092 [18:36<17:01,  1.02s/it]\u001b[A\n","Steps:  52% 1095/2092 [18:37<17:00,  1.02s/it]\u001b[A\n","Steps:  52% 1096/2092 [18:38<17:00,  1.02s/it]\u001b[A\n","Steps:  52% 1097/2092 [18:39<17:00,  1.03s/it]\u001b[A\n","Steps:  52% 1098/2092 [18:40<16:55,  1.02s/it]\u001b[A\n","Steps:  53% 1099/2092 [18:41<16:51,  1.02s/it]\u001b[A\n","Steps:  53% 1100/2092 [18:42<16:50,  1.02s/it]\u001b[A\n","Steps:  53% 1101/2092 [18:43<16:49,  1.02s/it]\u001b[A\n","Steps:  53% 1102/2092 [18:44<16:49,  1.02s/it]\u001b[A\n","Steps:  53% 1103/2092 [18:45<16:47,  1.02s/it]\u001b[A\n","Steps:  53% 1104/2092 [18:46<16:44,  1.02s/it]\u001b[A\n","Steps:  53% 1105/2092 [18:47<16:43,  1.02s/it]\u001b[A\n","Steps:  53% 1106/2092 [18:48<16:46,  1.02s/it]\u001b[A\n","Steps:  53% 1107/2092 [18:49<16:46,  1.02s/it]\u001b[A\n","Steps:  53% 1108/2092 [18:50<16:47,  1.02s/it]\u001b[A\n","Steps:  53% 1109/2092 [18:51<16:45,  1.02s/it]\u001b[A\n","Steps:  53% 1110/2092 [18:52<16:44,  1.02s/it]\u001b[A\n","Steps:  53% 1111/2092 [18:53<16:39,  1.02s/it]\u001b[A\n","Steps:  53% 1112/2092 [18:54<16:35,  1.02s/it]\u001b[A\n","Steps:  53% 1113/2092 [18:55<16:35,  1.02s/it]\u001b[A\n","Steps:  53% 1114/2092 [18:56<16:34,  1.02s/it]\u001b[A\n","Steps:  53% 1115/2092 [18:57<16:36,  1.02s/it]\u001b[A\n","Steps:  53% 1116/2092 [18:58<16:34,  1.02s/it]\u001b[A\n","Steps:  53% 1117/2092 [18:59<16:34,  1.02s/it]\u001b[A\n","Steps:  53% 1118/2092 [19:00<16:33,  1.02s/it]\u001b[A\n","Steps:  53% 1119/2092 [19:01<16:32,  1.02s/it]\u001b[A\n","Steps:  54% 1120/2092 [19:02<16:33,  1.02s/it]\u001b[A\n","Steps:  54% 1121/2092 [19:03<16:30,  1.02s/it]\u001b[A\n","Steps:  54% 1122/2092 [19:04<16:28,  1.02s/it]\u001b[A\n","Steps:  54% 1123/2092 [19:05<16:30,  1.02s/it]\u001b[A\n","Steps:  54% 1124/2092 [19:06<16:29,  1.02s/it]\u001b[A\n","Steps:  54% 1125/2092 [19:07<16:29,  1.02s/it]\u001b[A\n","Steps:  54% 1126/2092 [19:08<16:29,  1.02s/it]\u001b[A\n","Steps:  54% 1127/2092 [19:09<16:28,  1.02s/it]\u001b[A\n","Steps:  54% 1128/2092 [19:10<16:26,  1.02s/it]\u001b[A\n","Steps:  54% 1129/2092 [19:11<16:24,  1.02s/it]\u001b[A\n","Steps:  54% 1130/2092 [19:12<16:25,  1.02s/it]\u001b[A\n","Steps:  54% 1131/2092 [19:13<16:19,  1.02s/it]\u001b[A\n","Steps:  54% 1132/2092 [19:15<16:21,  1.02s/it]\u001b[A\n","Steps:  54% 1133/2092 [19:16<16:19,  1.02s/it]\u001b[A\n","Steps:  54% 1134/2092 [19:17<16:18,  1.02s/it]\u001b[A\n","Steps:  54% 1135/2092 [19:18<16:18,  1.02s/it]\u001b[A\n","Steps:  54% 1136/2092 [19:19<16:17,  1.02s/it]\u001b[A\n","Steps:  54% 1137/2092 [19:20<16:15,  1.02s/it]\u001b[A\n","Steps:  54% 1138/2092 [19:21<16:15,  1.02s/it]\u001b[A\n","Steps:  54% 1139/2092 [19:22<16:15,  1.02s/it]\u001b[A\n","Steps:  54% 1140/2092 [19:23<16:10,  1.02s/it]\u001b[A\n","Steps:  55% 1141/2092 [19:24<16:09,  1.02s/it]\u001b[A\n","Steps:  55% 1142/2092 [19:25<16:09,  1.02s/it]\u001b[A\n","Steps:  55% 1143/2092 [19:26<16:10,  1.02s/it]\u001b[A\n","Steps:  55% 1144/2092 [19:27<16:08,  1.02s/it]\u001b[A\n","Steps:  55% 1145/2092 [19:28<16:05,  1.02s/it]\u001b[A\n","Steps:  55% 1146/2092 [19:29<16:02,  1.02s/it]\u001b[A\n","Steps:  55% 1147/2092 [19:30<15:59,  1.02s/it]\u001b[A\n","Steps:  55% 1148/2092 [19:31<15:58,  1.02s/it]\u001b[A\n","Steps:  55% 1149/2092 [19:32<16:01,  1.02s/it]\u001b[A\n","Steps:  55% 1150/2092 [19:33<15:57,  1.02s/it]\u001b[A\n","Steps:  55% 1151/2092 [19:34<15:56,  1.02s/it]\u001b[A\n","Steps:  55% 1152/2092 [19:35<15:55,  1.02s/it]\u001b[A\n","Steps:  55% 1153/2092 [19:36<15:57,  1.02s/it]\u001b[A\n","Steps:  55% 1154/2092 [19:37<15:55,  1.02s/it]\u001b[A\n","Steps:  55% 1155/2092 [19:38<15:54,  1.02s/it]\u001b[A\n","Steps:  55% 1156/2092 [19:39<15:54,  1.02s/it]\u001b[A\n","Steps:  55% 1157/2092 [19:40<15:53,  1.02s/it]\u001b[A\n","Steps:  55% 1158/2092 [19:41<15:55,  1.02s/it]\u001b[A\n","Steps:  55% 1159/2092 [19:42<15:53,  1.02s/it]\u001b[A\n","Steps:  55% 1160/2092 [19:43<15:54,  1.02s/it]\u001b[A\n","Steps:  55% 1161/2092 [19:44<15:54,  1.03s/it]\u001b[A\n","Steps:  56% 1162/2092 [19:45<15:52,  1.02s/it]\u001b[A\n","Steps:  56% 1163/2092 [19:46<15:50,  1.02s/it]\u001b[A\n","Steps:  56% 1164/2092 [19:47<15:50,  1.02s/it]\u001b[A\n","Steps:  56% 1165/2092 [19:48<15:47,  1.02s/it]\u001b[A\n","Steps:  56% 1166/2092 [19:49<15:48,  1.02s/it]\u001b[A\n","Steps:  56% 1167/2092 [19:50<15:46,  1.02s/it]\u001b[A\n","Steps:  56% 1168/2092 [19:51<15:44,  1.02s/it]\u001b[A\n","Steps:  56% 1169/2092 [19:52<15:45,  1.02s/it]\u001b[A\n","Steps:  56% 1170/2092 [19:53<15:42,  1.02s/it]\u001b[A\n","Steps:  56% 1171/2092 [19:54<15:39,  1.02s/it]\u001b[A\n","Steps:  56% 1172/2092 [19:55<15:37,  1.02s/it]\u001b[A\n","Steps:  56% 1173/2092 [19:56<15:39,  1.02s/it]\u001b[A\n","Steps:  56% 1174/2092 [19:57<15:35,  1.02s/it]\u001b[A\n","Steps:  56% 1175/2092 [19:58<15:35,  1.02s/it]\u001b[A\n","Steps:  56% 1176/2092 [19:59<15:33,  1.02s/it]\u001b[A\n","Steps:  56% 1177/2092 [20:00<15:29,  1.02s/it]\u001b[A\n","Steps:  56% 1178/2092 [20:01<15:31,  1.02s/it]\u001b[A\n","Steps:  56% 1179/2092 [20:02<15:30,  1.02s/it]\u001b[A\n","Steps:  56% 1180/2092 [20:04<15:30,  1.02s/it]\u001b[A\n","Steps:  56% 1181/2092 [20:05<15:26,  1.02s/it]\u001b[A\n","Steps:  57% 1182/2092 [20:06<15:28,  1.02s/it]\u001b[A\n","Steps:  57% 1183/2092 [20:07<15:29,  1.02s/it]\u001b[A\n","Steps:  57% 1184/2092 [20:08<15:29,  1.02s/it]\u001b[A\n","Steps:  57% 1185/2092 [20:09<15:27,  1.02s/it]\u001b[A\n","Steps:  57% 1186/2092 [20:10<15:28,  1.02s/it]\u001b[A\n","Steps:  57% 1187/2092 [20:11<15:28,  1.03s/it]\u001b[A\n","Steps:  57% 1188/2092 [20:12<15:25,  1.02s/it]\u001b[A\n","Steps:  57% 1189/2092 [20:13<15:26,  1.03s/it]\u001b[A\n","Steps:  57% 1190/2092 [20:14<15:24,  1.03s/it]\u001b[A\n","Steps:  57% 1191/2092 [20:15<15:19,  1.02s/it]\u001b[A\n","Steps:  57% 1192/2092 [20:16<15:19,  1.02s/it]\u001b[A\n","Steps:  57% 1193/2092 [20:17<15:14,  1.02s/it]\u001b[A\n","Steps:  57% 1194/2092 [20:18<15:16,  1.02s/it]\u001b[A\n","Steps:  57% 1195/2092 [20:19<15:14,  1.02s/it]\u001b[A\n","Steps:  57% 1196/2092 [20:20<15:11,  1.02s/it]\u001b[A\n","Steps:  57% 1197/2092 [20:21<15:13,  1.02s/it]\u001b[A\n","Steps:  57% 1198/2092 [20:22<15:13,  1.02s/it]\u001b[A\n","Steps:  57% 1199/2092 [20:23<15:09,  1.02s/it]\u001b[A\n","Steps:  57% 1200/2092 [20:24<15:09,  1.02s/it]\u001b[A\n","Steps:  57% 1201/2092 [20:25<15:05,  1.02s/it]\u001b[A\n","Steps:  57% 1202/2092 [20:26<15:06,  1.02s/it]\u001b[A\n","Steps:  58% 1203/2092 [20:27<15:05,  1.02s/it]\u001b[A\n","Steps:  58% 1204/2092 [20:28<15:06,  1.02s/it]\u001b[A\n","Steps:  58% 1205/2092 [20:29<15:07,  1.02s/it]\u001b[A\n","Steps:  58% 1206/2092 [20:30<15:00,  1.02s/it]\u001b[A\n","Steps:  58% 1207/2092 [20:31<15:00,  1.02s/it]\u001b[A\n","Steps:  58% 1208/2092 [20:32<15:00,  1.02s/it]\u001b[A\n","Steps:  58% 1209/2092 [20:33<15:00,  1.02s/it]\u001b[A\n","Steps:  58% 1210/2092 [20:34<14:59,  1.02s/it]\u001b[A\n","Steps:  58% 1211/2092 [20:35<15:00,  1.02s/it]\u001b[A\n","Steps:  58% 1212/2092 [20:36<14:58,  1.02s/it]\u001b[A\n","Steps:  58% 1213/2092 [20:37<14:56,  1.02s/it]\u001b[A\n","Steps:  58% 1214/2092 [20:38<14:54,  1.02s/it]\u001b[A\n","Steps:  58% 1215/2092 [20:39<14:51,  1.02s/it]\u001b[A\n","Steps:  58% 1216/2092 [20:40<14:52,  1.02s/it]\u001b[A\n","Steps:  58% 1217/2092 [20:41<14:48,  1.02s/it]\u001b[A\n","Steps:  58% 1218/2092 [20:42<14:51,  1.02s/it]\u001b[A\n","Steps:  58% 1219/2092 [20:43<14:51,  1.02s/it]\u001b[A\n","Steps:  58% 1220/2092 [20:44<14:52,  1.02s/it]\u001b[A\n","Steps:  58% 1221/2092 [20:45<14:53,  1.03s/it]\u001b[A\n","Steps:  58% 1222/2092 [20:46<14:50,  1.02s/it]\u001b[A\n","Steps:  58% 1223/2092 [20:47<14:45,  1.02s/it]\u001b[A\n","Steps:  59% 1224/2092 [20:48<14:45,  1.02s/it]\u001b[A\n","Steps:  59% 1225/2092 [20:49<14:46,  1.02s/it]\u001b[A\n","Steps:  59% 1226/2092 [20:50<14:47,  1.02s/it]\u001b[A\n","Steps:  59% 1227/2092 [20:51<14:44,  1.02s/it]\u001b[A\n","Steps:  59% 1228/2092 [20:53<14:40,  1.02s/it]\u001b[A\n","Steps:  59% 1229/2092 [20:54<14:37,  1.02s/it]\u001b[A\n","Steps:  59% 1230/2092 [20:55<14:39,  1.02s/it]\u001b[A\n","Steps:  59% 1231/2092 [20:56<14:37,  1.02s/it]\u001b[A\n","Steps:  59% 1232/2092 [20:57<14:39,  1.02s/it]\u001b[A\n","Steps:  59% 1233/2092 [20:58<14:39,  1.02s/it]\u001b[A\n","Steps:  59% 1234/2092 [20:59<14:37,  1.02s/it]\u001b[A\n","Steps:  59% 1235/2092 [21:00<14:34,  1.02s/it]\u001b[A\n","Steps:  59% 1236/2092 [21:01<14:35,  1.02s/it]\u001b[A\n","Steps:  59% 1237/2092 [21:02<14:34,  1.02s/it]\u001b[A\n","Steps:  59% 1238/2092 [21:03<14:32,  1.02s/it]\u001b[A\n","Steps:  59% 1239/2092 [21:04<14:26,  1.02s/it]\u001b[A\n","Steps:  59% 1240/2092 [21:05<14:28,  1.02s/it]\u001b[A\n","Steps:  59% 1241/2092 [21:06<14:26,  1.02s/it]\u001b[A\n","Steps:  59% 1242/2092 [21:07<14:26,  1.02s/it]\u001b[A\n","Steps:  59% 1243/2092 [21:08<14:27,  1.02s/it]\u001b[A\n","Steps:  59% 1244/2092 [21:09<14:22,  1.02s/it]\u001b[A\n","Steps:  60% 1245/2092 [21:10<14:23,  1.02s/it]\u001b[A\n","Steps:  60% 1246/2092 [21:11<14:22,  1.02s/it]\u001b[A\n","Steps:  60% 1247/2092 [21:12<14:18,  1.02s/it]\u001b[A\n","Steps:  60% 1248/2092 [21:13<14:21,  1.02s/it]\u001b[A\n","Steps:  60% 1249/2092 [21:14<14:17,  1.02s/it]\u001b[A\n","Steps:  60% 1250/2092 [21:15<14:18,  1.02s/it]\u001b[A\n","Steps:  60% 1251/2092 [21:16<14:19,  1.02s/it]\u001b[A\n","Steps:  60% 1252/2092 [21:17<14:20,  1.02s/it]\u001b[A\n","Steps:  60% 1253/2092 [21:18<14:15,  1.02s/it]\u001b[A\n","Steps:  60% 1254/2092 [21:19<14:16,  1.02s/it]\u001b[A\n","Steps:  60% 1255/2092 [21:20<14:16,  1.02s/it]\u001b[A\n","Steps:  60% 1256/2092 [21:21<14:15,  1.02s/it]\u001b[A\n","Steps:  60% 1257/2092 [21:22<14:13,  1.02s/it]\u001b[A\n","Steps:  60% 1258/2092 [21:23<14:13,  1.02s/it]\u001b[A\n","Steps:  60% 1259/2092 [21:24<14:14,  1.03s/it]\u001b[A\n","Steps:  60% 1260/2092 [21:25<14:11,  1.02s/it]\u001b[A\n","Steps:  60% 1261/2092 [21:26<14:08,  1.02s/it]\u001b[A\n","Steps:  60% 1262/2092 [21:27<14:09,  1.02s/it]\u001b[A\n","Steps:  60% 1263/2092 [21:28<14:05,  1.02s/it]\u001b[A\n","Steps:  60% 1264/2092 [21:29<14:06,  1.02s/it]\u001b[A\n","Steps:  60% 1265/2092 [21:30<14:04,  1.02s/it]\u001b[A\n","Steps:  61% 1266/2092 [21:31<14:02,  1.02s/it]\u001b[A\n","Steps:  61% 1267/2092 [21:32<14:00,  1.02s/it]\u001b[A\n","Steps:  61% 1268/2092 [21:33<14:01,  1.02s/it]\u001b[A\n","Steps:  61% 1269/2092 [21:34<14:02,  1.02s/it]\u001b[A\n","Steps:  61% 1270/2092 [21:35<14:02,  1.02s/it]\u001b[A\n","Steps:  61% 1271/2092 [21:36<13:57,  1.02s/it]\u001b[A\n","Steps:  61% 1272/2092 [21:37<13:58,  1.02s/it]\u001b[A\n","Steps:  61% 1273/2092 [21:38<13:54,  1.02s/it]\u001b[A\n","Steps:  61% 1274/2092 [21:39<13:52,  1.02s/it]\u001b[A\n","Steps:  61% 1275/2092 [21:40<13:53,  1.02s/it]\u001b[A\n","Steps:  61% 1276/2092 [21:42<13:52,  1.02s/it]\u001b[A\n","Steps:  61% 1277/2092 [21:43<13:53,  1.02s/it]\u001b[A\n","Steps:  61% 1278/2092 [21:44<13:53,  1.02s/it]\u001b[A\n","Steps:  61% 1279/2092 [21:45<13:53,  1.03s/it]\u001b[A\n","Steps:  61% 1280/2092 [21:46<13:53,  1.03s/it]\u001b[A\n","Steps:  61% 1281/2092 [21:47<13:51,  1.03s/it]\u001b[A\n","Steps:  61% 1282/2092 [21:48<13:52,  1.03s/it]\u001b[A\n","Steps:  61% 1283/2092 [21:49<13:49,  1.03s/it]\u001b[A\n","Steps:  61% 1284/2092 [21:50<13:43,  1.02s/it]\u001b[A\n","Steps:  61% 1285/2092 [21:51<13:40,  1.02s/it]\u001b[A\n","Steps:  61% 1286/2092 [21:52<13:39,  1.02s/it]\u001b[A\n","Steps:  62% 1287/2092 [21:53<13:40,  1.02s/it]\u001b[A\n","Steps:  62% 1288/2092 [21:54<13:40,  1.02s/it]\u001b[A\n","Steps:  62% 1289/2092 [21:55<13:35,  1.02s/it]\u001b[A\n","Steps:  62% 1290/2092 [21:56<13:37,  1.02s/it]\u001b[A\n","Steps:  62% 1291/2092 [21:57<13:36,  1.02s/it]\u001b[A\n","Steps:  62% 1292/2092 [21:58<13:38,  1.02s/it]\u001b[A\n","Steps:  62% 1293/2092 [21:59<13:38,  1.02s/it]\u001b[A\n","Steps:  62% 1294/2092 [22:00<13:34,  1.02s/it]\u001b[A\n","Steps:  62% 1295/2092 [22:01<13:35,  1.02s/it]\u001b[A\n","Steps:  62% 1296/2092 [22:02<13:33,  1.02s/it]\u001b[A\n","Steps:  62% 1297/2092 [22:03<13:34,  1.02s/it]\u001b[A\n","Steps:  62% 1298/2092 [22:04<13:29,  1.02s/it]\u001b[A\n","Steps:  62% 1299/2092 [22:05<13:24,  1.01s/it]\u001b[A\n","Steps:  62% 1300/2092 [22:06<13:25,  1.02s/it]\u001b[A\n","Steps:  62% 1301/2092 [22:07<13:26,  1.02s/it]\u001b[A\n","Steps:  62% 1302/2092 [22:08<13:23,  1.02s/it]\u001b[A\n","Steps:  62% 1303/2092 [22:09<13:20,  1.01s/it]\u001b[A\n","Steps:  62% 1304/2092 [22:10<13:21,  1.02s/it]\u001b[A\n","Steps:  62% 1305/2092 [22:11<13:21,  1.02s/it]\u001b[A\n","Steps:  62% 1306/2092 [22:12<13:20,  1.02s/it]\u001b[A\n","Steps:  62% 1307/2092 [22:13<13:21,  1.02s/it]\u001b[A\n","Steps:  63% 1308/2092 [22:14<13:18,  1.02s/it]\u001b[A\n","Steps:  63% 1309/2092 [22:15<13:20,  1.02s/it]\u001b[A\n","Steps:  63% 1310/2092 [22:16<13:19,  1.02s/it]\u001b[A\n","Steps:  63% 1311/2092 [22:17<13:19,  1.02s/it]\u001b[A\n","Steps:  63% 1312/2092 [22:18<13:15,  1.02s/it]\u001b[A\n","Steps:  63% 1313/2092 [22:19<13:15,  1.02s/it]\u001b[A\n","Steps:  63% 1314/2092 [22:20<13:12,  1.02s/it]\u001b[A\n","Steps:  63% 1315/2092 [22:21<13:13,  1.02s/it]\u001b[A\n","Steps:  63% 1316/2092 [22:22<13:11,  1.02s/it]\u001b[A\n","Steps:  63% 1317/2092 [22:23<13:12,  1.02s/it]\u001b[A\n","Steps:  63% 1318/2092 [22:24<13:10,  1.02s/it]\u001b[A\n","Steps:  63% 1319/2092 [22:25<13:10,  1.02s/it]\u001b[A\n","Steps:  63% 1320/2092 [22:26<13:11,  1.03s/it]\u001b[A\n","Steps:  63% 1321/2092 [22:27<13:07,  1.02s/it]\u001b[A\n","Steps:  63% 1322/2092 [22:28<13:07,  1.02s/it]\u001b[A\n","Steps:  63% 1323/2092 [22:29<13:03,  1.02s/it]\u001b[A\n","Steps:  63% 1324/2092 [22:30<13:02,  1.02s/it]\u001b[A\n","Steps:  63% 1325/2092 [22:32<13:04,  1.02s/it]\u001b[A\n","Steps:  63% 1326/2092 [22:33<13:00,  1.02s/it]\u001b[A\n","Steps:  63% 1327/2092 [22:34<13:01,  1.02s/it]\u001b[A\n","Steps:  63% 1328/2092 [22:35<13:02,  1.02s/it]\u001b[A\n","Steps:  64% 1329/2092 [22:36<12:57,  1.02s/it]\u001b[A\n","Steps:  64% 1330/2092 [22:37<12:57,  1.02s/it]\u001b[A\n","Steps:  64% 1331/2092 [22:38<12:55,  1.02s/it]\u001b[A\n","Steps:  64% 1332/2092 [22:39<12:56,  1.02s/it]\u001b[A\n","Steps:  64% 1333/2092 [22:40<12:54,  1.02s/it]\u001b[A\n","Steps:  64% 1334/2092 [22:41<12:52,  1.02s/it]\u001b[A\n","Steps:  64% 1335/2092 [22:42<12:53,  1.02s/it]\u001b[A\n","Steps:  64% 1336/2092 [22:43<12:51,  1.02s/it]\u001b[A\n","Steps:  64% 1337/2092 [22:44<12:48,  1.02s/it]\u001b[A\n","Steps:  64% 1338/2092 [22:45<12:48,  1.02s/it]\u001b[A\n","Steps:  64% 1339/2092 [22:46<12:49,  1.02s/it]\u001b[A\n","Steps:  64% 1340/2092 [22:47<12:45,  1.02s/it]\u001b[A\n","Steps:  64% 1341/2092 [22:48<12:42,  1.02s/it]\u001b[A\n","Steps:  64% 1342/2092 [22:49<12:42,  1.02s/it]\u001b[A\n","Steps:  64% 1343/2092 [22:50<12:43,  1.02s/it]\u001b[A\n","Steps:  64% 1344/2092 [22:51<12:42,  1.02s/it]\u001b[A\n","Steps:  64% 1345/2092 [22:52<12:41,  1.02s/it]\u001b[A\n","Steps:  64% 1346/2092 [22:53<12:42,  1.02s/it]\u001b[A\n","Steps:  64% 1347/2092 [22:54<12:38,  1.02s/it]\u001b[A\n","Steps:  64% 1348/2092 [22:55<12:37,  1.02s/it]\u001b[A\n","Steps:  64% 1349/2092 [22:56<12:36,  1.02s/it]\u001b[A\n","Steps:  65% 1350/2092 [22:57<12:36,  1.02s/it]\u001b[A\n","Steps:  65% 1351/2092 [22:58<12:38,  1.02s/it]\u001b[A\n","Steps:  65% 1352/2092 [22:59<12:35,  1.02s/it]\u001b[A\n","Steps:  65% 1353/2092 [23:00<12:34,  1.02s/it]\u001b[A\n","Steps:  65% 1354/2092 [23:01<12:34,  1.02s/it]\u001b[A\n","Steps:  65% 1355/2092 [23:02<12:34,  1.02s/it]\u001b[A\n","Steps:  65% 1356/2092 [23:03<12:33,  1.02s/it]\u001b[A\n","Steps:  65% 1357/2092 [23:04<12:28,  1.02s/it]\u001b[A\n","Steps:  65% 1358/2092 [23:05<12:25,  1.02s/it]\u001b[A\n","Steps:  65% 1359/2092 [23:06<12:25,  1.02s/it]\u001b[A\n","Steps:  65% 1360/2092 [23:07<12:25,  1.02s/it]\u001b[A\n","Steps:  65% 1361/2092 [23:08<12:25,  1.02s/it]\u001b[A\n","Steps:  65% 1362/2092 [23:09<12:23,  1.02s/it]\u001b[A\n","Steps:  65% 1363/2092 [23:10<12:21,  1.02s/it]\u001b[A\n","Steps:  65% 1364/2092 [23:11<12:22,  1.02s/it]\u001b[A\n","Steps:  65% 1365/2092 [23:12<12:21,  1.02s/it]\u001b[A\n","Steps:  65% 1366/2092 [23:13<12:20,  1.02s/it]\u001b[A\n","Steps:  65% 1367/2092 [23:14<12:21,  1.02s/it]\u001b[A\n","Steps:  65% 1368/2092 [23:15<12:19,  1.02s/it]\u001b[A\n","Steps:  65% 1369/2092 [23:16<12:17,  1.02s/it]\u001b[A\n","Steps:  65% 1370/2092 [23:17<12:17,  1.02s/it]\u001b[A\n","Steps:  66% 1371/2092 [23:18<12:17,  1.02s/it]\u001b[A\n","Steps:  66% 1372/2092 [23:19<12:14,  1.02s/it]\u001b[A\n","Steps:  66% 1373/2092 [23:20<12:13,  1.02s/it]\u001b[A\n","Steps:  66% 1374/2092 [23:22<12:12,  1.02s/it]\u001b[A\n","Steps:  66% 1375/2092 [23:23<12:12,  1.02s/it]\u001b[A\n","Steps:  66% 1376/2092 [23:24<12:08,  1.02s/it]\u001b[A\n","Steps:  66% 1377/2092 [23:25<12:09,  1.02s/it]\u001b[A\n","Steps:  66% 1378/2092 [23:26<12:09,  1.02s/it]\u001b[A\n","Steps:  66% 1379/2092 [23:27<12:08,  1.02s/it]\u001b[A\n","Steps:  66% 1380/2092 [23:28<12:08,  1.02s/it]\u001b[A\n","Steps:  66% 1381/2092 [23:29<12:06,  1.02s/it]\u001b[A\n","Steps:  66% 1382/2092 [23:30<12:04,  1.02s/it]\u001b[A\n","Steps:  66% 1383/2092 [23:31<12:04,  1.02s/it]\u001b[A\n","Steps:  66% 1384/2092 [23:32<12:05,  1.02s/it]\u001b[A\n","Steps:  66% 1385/2092 [23:33<12:03,  1.02s/it]\u001b[A\n","Steps:  66% 1386/2092 [23:34<12:03,  1.02s/it]\u001b[A\n","Steps:  66% 1387/2092 [23:35<12:03,  1.03s/it]\u001b[A\n","Steps:  66% 1388/2092 [23:36<12:01,  1.02s/it]\u001b[A\n","Steps:  66% 1389/2092 [23:37<11:59,  1.02s/it]\u001b[A\n","Steps:  66% 1390/2092 [23:38<11:58,  1.02s/it]\u001b[A\n","Steps:  66% 1391/2092 [23:39<11:57,  1.02s/it]\u001b[A\n","Steps:  67% 1392/2092 [23:40<11:54,  1.02s/it]\u001b[A\n","Steps:  67% 1393/2092 [23:41<11:53,  1.02s/it]\u001b[A\n","Steps:  67% 1394/2092 [23:42<11:51,  1.02s/it]\u001b[A\n","Steps:  67% 1395/2092 [23:43<11:52,  1.02s/it]\u001b[A\n","Steps:  67% 1396/2092 [23:44<11:52,  1.02s/it]\u001b[A\n","Steps:  67% 1397/2092 [23:45<11:52,  1.03s/it]\u001b[A\n","Steps:  67% 1398/2092 [23:46<11:51,  1.02s/it]\u001b[A\n","Steps:  67% 1399/2092 [23:47<11:47,  1.02s/it]\u001b[A\n","Steps:  67% 1400/2092 [23:48<11:47,  1.02s/it]\u001b[A\n","Steps:  67% 1401/2092 [23:49<11:47,  1.02s/it]\u001b[A\n","Steps:  67% 1402/2092 [23:50<11:45,  1.02s/it]\u001b[A\n","Steps:  67% 1403/2092 [23:51<11:43,  1.02s/it]\u001b[A\n","Steps:  67% 1404/2092 [23:52<11:41,  1.02s/it]\u001b[A\n","Steps:  67% 1405/2092 [23:53<11:40,  1.02s/it]\u001b[A\n","Steps:  67% 1406/2092 [23:54<11:39,  1.02s/it]\u001b[A\n","Steps:  67% 1407/2092 [23:55<11:40,  1.02s/it]\u001b[A\n","Steps:  67% 1408/2092 [23:56<11:38,  1.02s/it]\u001b[A\n","Steps:  67% 1409/2092 [23:57<11:37,  1.02s/it]\u001b[A\n","Steps:  67% 1410/2092 [23:58<11:35,  1.02s/it]\u001b[A\n","Steps:  67% 1411/2092 [23:59<11:34,  1.02s/it]\u001b[A\n","Steps:  67% 1412/2092 [24:00<11:33,  1.02s/it]\u001b[A\n","Steps:  68% 1413/2092 [24:01<11:32,  1.02s/it]\u001b[A\n","Steps:  68% 1414/2092 [24:02<11:32,  1.02s/it]\u001b[A\n","Steps:  68% 1415/2092 [24:03<11:31,  1.02s/it]\u001b[A\n","Steps:  68% 1416/2092 [24:04<11:28,  1.02s/it]\u001b[A\n","Steps:  68% 1417/2092 [24:05<11:27,  1.02s/it]\u001b[A\n","Steps:  68% 1418/2092 [24:06<11:26,  1.02s/it]\u001b[A\n","Steps:  68% 1419/2092 [24:07<11:27,  1.02s/it]\u001b[A\n","Steps:  68% 1420/2092 [24:09<11:25,  1.02s/it]\u001b[A\n","Steps:  68% 1421/2092 [24:10<11:25,  1.02s/it]\u001b[A\n","Steps:  68% 1422/2092 [24:11<11:23,  1.02s/it]\u001b[A\n","Steps:  68% 1423/2092 [24:12<11:24,  1.02s/it]\u001b[A\n","Steps:  68% 1424/2092 [24:13<11:24,  1.03s/it]\u001b[A\n","Steps:  68% 1425/2092 [24:14<11:24,  1.03s/it]\u001b[A\n","Steps:  68% 1426/2092 [24:15<11:22,  1.02s/it]\u001b[A\n","Steps:  68% 1427/2092 [24:16<11:17,  1.02s/it]\u001b[A\n","Steps:  68% 1428/2092 [24:17<11:16,  1.02s/it]\u001b[A\n","Steps:  68% 1429/2092 [24:18<11:17,  1.02s/it]\u001b[A\n","Steps:  68% 1430/2092 [24:19<11:15,  1.02s/it]\u001b[A\n","Steps:  68% 1431/2092 [24:20<11:13,  1.02s/it]\u001b[A\n","Steps:  68% 1432/2092 [24:21<11:11,  1.02s/it]\u001b[A\n","Steps:  68% 1433/2092 [24:22<11:11,  1.02s/it]\u001b[A\n","Steps:  69% 1434/2092 [24:23<11:10,  1.02s/it]\u001b[A\n","Steps:  69% 1435/2092 [24:24<11:07,  1.02s/it]\u001b[A\n","Steps:  69% 1436/2092 [24:25<11:06,  1.02s/it]\u001b[A\n","Steps:  69% 1437/2092 [24:26<11:07,  1.02s/it]\u001b[A\n","Steps:  69% 1438/2092 [24:27<11:08,  1.02s/it]\u001b[A\n","Steps:  69% 1439/2092 [24:28<11:05,  1.02s/it]\u001b[A\n","Steps:  69% 1440/2092 [24:29<11:05,  1.02s/it]\u001b[A\n","Steps:  69% 1441/2092 [24:30<11:05,  1.02s/it]\u001b[A\n","Steps:  69% 1442/2092 [24:31<11:01,  1.02s/it]\u001b[A\n","Steps:  69% 1443/2092 [24:32<10:59,  1.02s/it]\u001b[A\n","Steps:  69% 1444/2092 [24:33<10:58,  1.02s/it]\u001b[A\n","Steps:  69% 1445/2092 [24:34<10:58,  1.02s/it]\u001b[A\n","Steps:  69% 1446/2092 [24:35<10:56,  1.02s/it]\u001b[A\n","Steps:  69% 1447/2092 [24:36<10:56,  1.02s/it]\u001b[A\n","Steps:  69% 1448/2092 [24:37<10:54,  1.02s/it]\u001b[A\n","Steps:  69% 1449/2092 [24:38<10:53,  1.02s/it]\u001b[A\n","Steps:  69% 1450/2092 [24:39<10:52,  1.02s/it]\u001b[A\n","Steps:  69% 1451/2092 [24:40<10:54,  1.02s/it]\u001b[A\n","Steps:  69% 1452/2092 [24:41<10:52,  1.02s/it]\u001b[A\n","Steps:  69% 1453/2092 [24:42<10:53,  1.02s/it]\u001b[A\n","Steps:  70% 1454/2092 [24:43<10:49,  1.02s/it]\u001b[A\n","Steps:  70% 1455/2092 [24:44<10:50,  1.02s/it]\u001b[A\n","Steps:  70% 1456/2092 [24:45<10:51,  1.02s/it]\u001b[A\n","Steps:  70% 1457/2092 [24:46<10:49,  1.02s/it]\u001b[A\n","Steps:  70% 1458/2092 [24:47<10:48,  1.02s/it]\u001b[A\n","Steps:  70% 1459/2092 [24:48<10:46,  1.02s/it]\u001b[A\n","Steps:  70% 1460/2092 [24:49<10:44,  1.02s/it]\u001b[A\n","Steps:  70% 1461/2092 [24:50<10:43,  1.02s/it]\u001b[A\n","Steps:  70% 1462/2092 [24:51<10:42,  1.02s/it]\u001b[A\n","Steps:  70% 1463/2092 [24:52<10:40,  1.02s/it]\u001b[A\n","Steps:  70% 1464/2092 [24:53<10:39,  1.02s/it]\u001b[A\n","Steps:  70% 1465/2092 [24:54<10:36,  1.02s/it]\u001b[A\n","Steps:  70% 1466/2092 [24:55<10:36,  1.02s/it]\u001b[A\n","Steps:  70% 1467/2092 [24:56<10:34,  1.01s/it]\u001b[A\n","Steps:  70% 1468/2092 [24:57<10:32,  1.01s/it]\u001b[A\n","Steps:  70% 1469/2092 [24:58<10:33,  1.02s/it]\u001b[A\n","Steps:  70% 1470/2092 [24:59<10:33,  1.02s/it]\u001b[A\n","Steps:  70% 1471/2092 [25:00<10:32,  1.02s/it]\u001b[A\n","Steps:  70% 1472/2092 [25:02<10:33,  1.02s/it]\u001b[A\n","Steps:  70% 1473/2092 [25:03<10:32,  1.02s/it]\u001b[A\n","Steps:  70% 1474/2092 [25:04<10:32,  1.02s/it]\u001b[A\n","Steps:  71% 1475/2092 [25:05<10:31,  1.02s/it]\u001b[A\n","Steps:  71% 1476/2092 [25:06<10:31,  1.02s/it]\u001b[A\n","Steps:  71% 1477/2092 [25:07<10:31,  1.03s/it]\u001b[A\n","Steps:  71% 1478/2092 [25:08<10:29,  1.03s/it]\u001b[A\n","Steps:  71% 1479/2092 [25:09<10:28,  1.02s/it]\u001b[A\n","Steps:  71% 1480/2092 [25:10<10:24,  1.02s/it]\u001b[A\n","Steps:  71% 1481/2092 [25:11<10:23,  1.02s/it]\u001b[A\n","Steps:  71% 1482/2092 [25:12<10:24,  1.02s/it]\u001b[A\n","Steps:  71% 1483/2092 [25:13<10:20,  1.02s/it]\u001b[A\n","Steps:  71% 1484/2092 [25:14<10:20,  1.02s/it]\u001b[A\n","Steps:  71% 1485/2092 [25:15<10:18,  1.02s/it]\u001b[A\n","Steps:  71% 1486/2092 [25:16<10:17,  1.02s/it]\u001b[A\n","Steps:  71% 1487/2092 [25:17<10:17,  1.02s/it]\u001b[A\n","Steps:  71% 1488/2092 [25:18<10:16,  1.02s/it]\u001b[A\n","Steps:  71% 1489/2092 [25:19<10:13,  1.02s/it]\u001b[A\n","Steps:  71% 1490/2092 [25:20<10:12,  1.02s/it]\u001b[A\n","Steps:  71% 1491/2092 [25:21<10:12,  1.02s/it]\u001b[A\n","Steps:  71% 1492/2092 [25:22<10:12,  1.02s/it]\u001b[A\n","Steps:  71% 1493/2092 [25:23<10:12,  1.02s/it]\u001b[A\n","Steps:  71% 1494/2092 [25:24<10:12,  1.02s/it]\u001b[A\n","Steps:  71% 1495/2092 [25:25<10:10,  1.02s/it]\u001b[A\n","Steps:  72% 1496/2092 [25:26<10:08,  1.02s/it]\u001b[A\n","Steps:  72% 1497/2092 [25:27<10:05,  1.02s/it]\u001b[A\n","Steps:  72% 1498/2092 [25:28<10:06,  1.02s/it]\u001b[A\n","Steps:  72% 1499/2092 [25:29<10:03,  1.02s/it]\u001b[A\n","Steps:  72% 1500/2092 [25:30<10:01,  1.02s/it]\u001b[A\n","Steps:  72% 1501/2092 [25:31<10:00,  1.02s/it]\u001b[A\n","Steps:  72% 1502/2092 [25:32<10:01,  1.02s/it]\u001b[A\n","Steps:  72% 1503/2092 [25:33<10:00,  1.02s/it]\u001b[A\n","Steps:  72% 1504/2092 [25:34<09:59,  1.02s/it]\u001b[A\n","Steps:  72% 1505/2092 [25:35<10:00,  1.02s/it]\u001b[A\n","Steps:  72% 1506/2092 [25:36<09:56,  1.02s/it]\u001b[A\n","Steps:  72% 1507/2092 [25:37<09:54,  1.02s/it]\u001b[A\n","Steps:  72% 1508/2092 [25:38<09:52,  1.01s/it]\u001b[A\n","Steps:  72% 1509/2092 [25:39<09:51,  1.02s/it]\u001b[A\n","Steps:  72% 1510/2092 [25:40<09:51,  1.02s/it]\u001b[A\n","Steps:  72% 1511/2092 [25:41<09:51,  1.02s/it]\u001b[A\n","Steps:  72% 1512/2092 [25:42<09:49,  1.02s/it]\u001b[A\n","Steps:  72% 1513/2092 [25:43<09:50,  1.02s/it]\u001b[A\n","Steps:  72% 1514/2092 [25:44<09:49,  1.02s/it]\u001b[A\n","Steps:  72% 1515/2092 [25:45<09:48,  1.02s/it]\u001b[A\n","Steps:  72% 1516/2092 [25:46<09:47,  1.02s/it]\u001b[A\n","Steps:  73% 1517/2092 [25:47<09:45,  1.02s/it]\u001b[A\n","Steps:  73% 1518/2092 [25:48<09:46,  1.02s/it]\u001b[A\n","Steps:  73% 1519/2092 [25:49<09:45,  1.02s/it]\u001b[A\n","Steps:  73% 1520/2092 [25:50<09:42,  1.02s/it]\u001b[A\n","Steps:  73% 1521/2092 [25:51<09:41,  1.02s/it]\u001b[A\n","Steps:  73% 1522/2092 [25:53<09:40,  1.02s/it]\u001b[A\n","Steps:  73% 1523/2092 [25:54<09:39,  1.02s/it]\u001b[A\n","Steps:  73% 1524/2092 [25:55<09:39,  1.02s/it]\u001b[A\n","Steps:  73% 1525/2092 [25:56<09:40,  1.02s/it]\u001b[A\n","Steps:  73% 1526/2092 [25:57<09:39,  1.02s/it]\u001b[A\n","Steps:  73% 1527/2092 [25:58<09:36,  1.02s/it]\u001b[A\n","Steps:  73% 1528/2092 [25:59<09:35,  1.02s/it]\u001b[A\n","Steps:  73% 1529/2092 [26:00<09:32,  1.02s/it]\u001b[A\n","Steps:  73% 1530/2092 [26:01<09:31,  1.02s/it]\u001b[A\n","Steps:  73% 1531/2092 [26:02<09:31,  1.02s/it]\u001b[A\n","Steps:  73% 1532/2092 [26:03<09:31,  1.02s/it]\u001b[A\n","Steps:  73% 1533/2092 [26:04<09:32,  1.02s/it]\u001b[A\n","Steps:  73% 1534/2092 [26:05<09:29,  1.02s/it]\u001b[A\n","Steps:  73% 1535/2092 [26:06<09:27,  1.02s/it]\u001b[A\n","Steps:  73% 1536/2092 [26:07<09:25,  1.02s/it]\u001b[A\n","Steps:  73% 1537/2092 [26:08<09:22,  1.01s/it]\u001b[A\n","Steps:  74% 1538/2092 [26:09<09:22,  1.02s/it]\u001b[A\n","Steps:  74% 1539/2092 [26:10<09:22,  1.02s/it]\u001b[A\n","Steps:  74% 1540/2092 [26:11<09:23,  1.02s/it]\u001b[A\n","Steps:  74% 1541/2092 [26:12<09:20,  1.02s/it]\u001b[A\n","Steps:  74% 1542/2092 [26:13<09:19,  1.02s/it]\u001b[A\n","Steps:  74% 1543/2092 [26:14<09:17,  1.02s/it]\u001b[A\n","Steps:  74% 1544/2092 [26:15<09:17,  1.02s/it]\u001b[A\n","Steps:  74% 1545/2092 [26:16<09:16,  1.02s/it]\u001b[A\n","Steps:  74% 1546/2092 [26:17<09:15,  1.02s/it]\u001b[A\n","Steps:  74% 1547/2092 [26:18<09:14,  1.02s/it]\u001b[A\n","Steps:  74% 1548/2092 [26:19<09:15,  1.02s/it]\u001b[A\n","Steps:  74% 1549/2092 [26:20<09:15,  1.02s/it]\u001b[A\n","Steps:  74% 1550/2092 [26:21<09:12,  1.02s/it]\u001b[A\n","Steps:  74% 1551/2092 [26:22<09:11,  1.02s/it]\u001b[A\n","Steps:  74% 1552/2092 [26:23<09:09,  1.02s/it]\u001b[A\n","Steps:  74% 1553/2092 [26:24<09:09,  1.02s/it]\u001b[A\n","Steps:  74% 1554/2092 [26:25<09:07,  1.02s/it]\u001b[A\n","Steps:  74% 1555/2092 [26:26<09:08,  1.02s/it]\u001b[A\n","Steps:  74% 1556/2092 [26:27<09:08,  1.02s/it]\u001b[A\n","Steps:  74% 1557/2092 [26:28<09:08,  1.02s/it]\u001b[A\n","Steps:  74% 1558/2092 [26:29<09:04,  1.02s/it]\u001b[A\n","Steps:  75% 1559/2092 [26:30<09:05,  1.02s/it]\u001b[A\n","Steps:  75% 1560/2092 [26:31<09:04,  1.02s/it]\u001b[A\n","Steps:  75% 1561/2092 [26:32<09:04,  1.03s/it]\u001b[A\n","Steps:  75% 1562/2092 [26:33<09:03,  1.03s/it]\u001b[A\n","Steps:  75% 1563/2092 [26:34<09:01,  1.02s/it]\u001b[A\n","Steps:  75% 1564/2092 [26:35<08:59,  1.02s/it]\u001b[A\n","Steps:  75% 1565/2092 [26:36<08:58,  1.02s/it]\u001b[A\n","Steps:  75% 1566/2092 [26:37<08:58,  1.02s/it]\u001b[A\n","Steps:  75% 1567/2092 [26:38<08:56,  1.02s/it]\u001b[A\n","Steps:  75% 1568/2092 [26:39<08:56,  1.02s/it]\u001b[A\n","Steps:  75% 1569/2092 [26:40<08:55,  1.02s/it]\u001b[A\n","Steps:  75% 1570/2092 [26:42<08:55,  1.03s/it]\u001b[A\n","Steps:  75% 1571/2092 [26:43<08:54,  1.03s/it]\u001b[A\n","Steps:  75% 1572/2092 [26:44<08:54,  1.03s/it]\u001b[A\n","Steps:  75% 1573/2092 [26:45<08:52,  1.03s/it]\u001b[A\n","Steps:  75% 1574/2092 [26:46<08:50,  1.02s/it]\u001b[A\n","Steps:  75% 1575/2092 [26:47<08:47,  1.02s/it]\u001b[A\n","Steps:  75% 1576/2092 [26:48<08:46,  1.02s/it]\u001b[A\n","Steps:  75% 1577/2092 [26:49<08:46,  1.02s/it]\u001b[A\n","Steps:  75% 1578/2092 [26:50<08:44,  1.02s/it]\u001b[A\n","Steps:  75% 1579/2092 [26:51<08:42,  1.02s/it]\u001b[A\n","Steps:  76% 1580/2092 [26:52<08:41,  1.02s/it]\u001b[A\n","Steps:  76% 1581/2092 [26:53<08:42,  1.02s/it]\u001b[A\n","Steps:  76% 1582/2092 [26:54<08:40,  1.02s/it]\u001b[A\n","Steps:  76% 1583/2092 [26:55<08:40,  1.02s/it]\u001b[A\n","Steps:  76% 1584/2092 [26:56<08:40,  1.03s/it]\u001b[A\n","Steps:  76% 1585/2092 [26:57<08:39,  1.03s/it]\u001b[A\n","Steps:  76% 1586/2092 [26:58<08:38,  1.03s/it]\u001b[A\n","Steps:  76% 1587/2092 [26:59<08:38,  1.03s/it]\u001b[A\n","Steps:  76% 1588/2092 [27:00<08:35,  1.02s/it]\u001b[A\n","Steps:  76% 1589/2092 [27:01<08:35,  1.02s/it]\u001b[A\n","Steps:  76% 1590/2092 [27:02<08:33,  1.02s/it]\u001b[A\n","Steps:  76% 1591/2092 [27:03<08:31,  1.02s/it]\u001b[A\n","Steps:  76% 1592/2092 [27:04<08:30,  1.02s/it]\u001b[A\n","Steps:  76% 1593/2092 [27:05<08:29,  1.02s/it]\u001b[A\n","Steps:  76% 1594/2092 [27:06<08:28,  1.02s/it]\u001b[A\n","Steps:  76% 1595/2092 [27:07<08:28,  1.02s/it]\u001b[A\n","Steps:  76% 1596/2092 [27:08<08:28,  1.02s/it]\u001b[A\n","Steps:  76% 1597/2092 [27:09<08:27,  1.02s/it]\u001b[A\n","Steps:  76% 1598/2092 [27:10<08:25,  1.02s/it]\u001b[A\n","Steps:  76% 1599/2092 [27:11<08:22,  1.02s/it]\u001b[A\n","Steps:  76% 1600/2092 [27:12<08:22,  1.02s/it]\u001b[A\n","Steps:  77% 1601/2092 [27:13<08:22,  1.02s/it]\u001b[A\n","Steps:  77% 1602/2092 [27:14<08:22,  1.03s/it]\u001b[A\n","Steps:  77% 1603/2092 [27:15<08:20,  1.02s/it]\u001b[A\n","Steps:  77% 1604/2092 [27:16<08:18,  1.02s/it]\u001b[A\n","Steps:  77% 1605/2092 [27:17<08:17,  1.02s/it]\u001b[A\n","Steps:  77% 1606/2092 [27:18<08:17,  1.02s/it]\u001b[A\n","Steps:  77% 1607/2092 [27:19<08:17,  1.03s/it]\u001b[A\n","Steps:  77% 1608/2092 [27:20<08:16,  1.03s/it]\u001b[A\n","Steps:  77% 1609/2092 [27:21<08:14,  1.02s/it]\u001b[A\n","Steps:  77% 1610/2092 [27:22<08:14,  1.03s/it]\u001b[A\n","Steps:  77% 1611/2092 [27:23<08:10,  1.02s/it]\u001b[A\n","Steps:  77% 1612/2092 [27:24<08:09,  1.02s/it]\u001b[A\n","Steps:  77% 1613/2092 [27:25<08:08,  1.02s/it]\u001b[A\n","Steps:  77% 1614/2092 [27:26<08:07,  1.02s/it]\u001b[A\n","Steps:  77% 1615/2092 [27:28<08:05,  1.02s/it]\u001b[A\n","Steps:  77% 1616/2092 [27:29<08:03,  1.02s/it]\u001b[A\n","Steps:  77% 1617/2092 [27:30<08:01,  1.01s/it]\u001b[A\n","Steps:  77% 1618/2092 [27:31<08:01,  1.02s/it]\u001b[A\n","Steps:  77% 1619/2092 [27:32<08:00,  1.02s/it]\u001b[A\n","Steps:  77% 1620/2092 [27:33<07:59,  1.02s/it]\u001b[A\n","Steps:  77% 1621/2092 [27:34<07:57,  1.01s/it]\u001b[A\n","Steps:  78% 1622/2092 [27:35<07:57,  1.02s/it]\u001b[A\n","Steps:  78% 1623/2092 [27:36<07:56,  1.02s/it]\u001b[A\n","Steps:  78% 1624/2092 [27:37<07:57,  1.02s/it]\u001b[A\n","Steps:  78% 1625/2092 [27:38<07:57,  1.02s/it]\u001b[A\n","Steps:  78% 1626/2092 [27:39<07:54,  1.02s/it]\u001b[A\n","Steps:  78% 1627/2092 [27:40<07:54,  1.02s/it]\u001b[A\n","Steps:  78% 1628/2092 [27:41<07:54,  1.02s/it]\u001b[A\n","Steps:  78% 1629/2092 [27:42<07:51,  1.02s/it]\u001b[A\n","Steps:  78% 1630/2092 [27:43<07:52,  1.02s/it]\u001b[A\n","Steps:  78% 1631/2092 [27:44<07:50,  1.02s/it]\u001b[A\n","Steps:  78% 1632/2092 [27:45<07:50,  1.02s/it]\u001b[A\n","Steps:  78% 1633/2092 [27:46<07:50,  1.02s/it]\u001b[A\n","Steps:  78% 1634/2092 [27:47<07:48,  1.02s/it]\u001b[A\n","Steps:  78% 1635/2092 [27:48<07:46,  1.02s/it]\u001b[A\n","Steps:  78% 1636/2092 [27:49<07:45,  1.02s/it]\u001b[A\n","Steps:  78% 1637/2092 [27:50<07:44,  1.02s/it]\u001b[A\n","Steps:  78% 1638/2092 [27:51<07:42,  1.02s/it]\u001b[A\n","Steps:  78% 1639/2092 [27:52<07:41,  1.02s/it]\u001b[A\n","Steps:  78% 1640/2092 [27:53<07:41,  1.02s/it]\u001b[A\n","Steps:  78% 1641/2092 [27:54<07:41,  1.02s/it]\u001b[A\n","Steps:  78% 1642/2092 [27:55<07:40,  1.02s/it]\u001b[A\n","Steps:  79% 1643/2092 [27:56<07:39,  1.02s/it]\u001b[A\n","Steps:  79% 1644/2092 [27:57<07:38,  1.02s/it]\u001b[A\n","Steps:  79% 1645/2092 [27:58<07:37,  1.02s/it]\u001b[A\n","Steps:  79% 1646/2092 [27:59<07:35,  1.02s/it]\u001b[A\n","Steps:  79% 1647/2092 [28:00<07:35,  1.02s/it]\u001b[A\n","Steps:  79% 1648/2092 [28:01<07:35,  1.03s/it]\u001b[A\n","Steps:  79% 1649/2092 [28:02<07:33,  1.02s/it]\u001b[A\n","Steps:  79% 1650/2092 [28:03<07:32,  1.02s/it]\u001b[A\n","Steps:  79% 1651/2092 [28:04<07:30,  1.02s/it]\u001b[A\n","Steps:  79% 1652/2092 [28:05<07:30,  1.02s/it]\u001b[A\n","Steps:  79% 1653/2092 [28:06<07:28,  1.02s/it]\u001b[A\n","Steps:  79% 1654/2092 [28:07<07:28,  1.02s/it]\u001b[A\n","Steps:  79% 1655/2092 [28:08<07:27,  1.03s/it]\u001b[A\n","Steps:  79% 1656/2092 [28:09<07:23,  1.02s/it]\u001b[A\n","Steps:  79% 1657/2092 [28:10<07:23,  1.02s/it]\u001b[A\n","Steps:  79% 1658/2092 [28:11<07:22,  1.02s/it]\u001b[A\n","Steps:  79% 1659/2092 [28:12<07:21,  1.02s/it]\u001b[A\n","Steps:  79% 1660/2092 [28:13<07:18,  1.02s/it]\u001b[A\n","Steps:  79% 1661/2092 [28:14<07:17,  1.02s/it]\u001b[A\n","Steps:  79% 1662/2092 [28:15<07:17,  1.02s/it]\u001b[A\n","Steps:  79% 1663/2092 [28:16<07:17,  1.02s/it]\u001b[A\n","Steps:  80% 1664/2092 [28:17<07:15,  1.02s/it]\u001b[A\n","Steps:  80% 1665/2092 [28:19<07:15,  1.02s/it]\u001b[A\n","Steps:  80% 1666/2092 [28:20<07:13,  1.02s/it]\u001b[A\n","Steps:  80% 1667/2092 [28:21<07:14,  1.02s/it]\u001b[A\n","Steps:  80% 1668/2092 [28:22<07:13,  1.02s/it]\u001b[A\n","Steps:  80% 1669/2092 [28:23<07:13,  1.02s/it]\u001b[A\n","Steps:  80% 1670/2092 [28:24<07:11,  1.02s/it]\u001b[A\n","Steps:  80% 1671/2092 [28:25<07:09,  1.02s/it]\u001b[A\n","Steps:  80% 1672/2092 [28:26<07:08,  1.02s/it]\u001b[A\n","Steps:  80% 1673/2092 [28:27<07:07,  1.02s/it]\u001b[A\n","Steps:  80% 1674/2092 [28:28<07:07,  1.02s/it]\u001b[A\n","Steps:  80% 1675/2092 [28:29<07:05,  1.02s/it]\u001b[A\n","Steps:  80% 1676/2092 [28:30<07:03,  1.02s/it]\u001b[A\n","Steps:  80% 1677/2092 [28:31<07:01,  1.02s/it]\u001b[A\n","Steps:  80% 1678/2092 [28:32<07:01,  1.02s/it]\u001b[A\n","Steps:  80% 1679/2092 [28:33<07:01,  1.02s/it]\u001b[A\n","Steps:  80% 1680/2092 [28:34<06:59,  1.02s/it]\u001b[A\n","Steps:  80% 1681/2092 [28:35<06:57,  1.02s/it]\u001b[A\n","Steps:  80% 1682/2092 [28:36<06:56,  1.02s/it]\u001b[A\n","Steps:  80% 1683/2092 [28:37<06:54,  1.01s/it]\u001b[A\n","Steps:  80% 1684/2092 [28:38<06:55,  1.02s/it]\u001b[A\n","Steps:  81% 1685/2092 [28:39<06:54,  1.02s/it]\u001b[A\n","Steps:  81% 1686/2092 [28:40<06:52,  1.02s/it]\u001b[A\n","Steps:  81% 1687/2092 [28:41<06:50,  1.01s/it]\u001b[A\n","Steps:  81% 1688/2092 [28:42<06:50,  1.02s/it]\u001b[A\n","Steps:  81% 1689/2092 [28:43<06:50,  1.02s/it]\u001b[A\n","Steps:  81% 1690/2092 [28:44<06:50,  1.02s/it]\u001b[A\n","Steps:  81% 1691/2092 [28:45<06:50,  1.02s/it]\u001b[A\n","Steps:  81% 1692/2092 [28:46<06:49,  1.02s/it]\u001b[A\n","Steps:  81% 1693/2092 [28:47<06:47,  1.02s/it]\u001b[A\n","Steps:  81% 1694/2092 [28:48<06:47,  1.02s/it]\u001b[A\n","Steps:  81% 1695/2092 [28:49<06:46,  1.03s/it]\u001b[A\n","Steps:  81% 1696/2092 [28:50<06:46,  1.03s/it]\u001b[A\n","Steps:  81% 1697/2092 [28:51<06:44,  1.02s/it]\u001b[A\n","Steps:  81% 1698/2092 [28:52<06:44,  1.03s/it]\u001b[A\n","Steps:  81% 1699/2092 [28:53<06:41,  1.02s/it]\u001b[A\n","Steps:  81% 1700/2092 [28:54<06:40,  1.02s/it]\u001b[A\n","Steps:  81% 1701/2092 [28:55<06:39,  1.02s/it]\u001b[A\n","Steps:  81% 1702/2092 [28:56<06:37,  1.02s/it]\u001b[A\n","Steps:  81% 1703/2092 [28:57<06:37,  1.02s/it]\u001b[A\n","Steps:  81% 1704/2092 [28:58<06:36,  1.02s/it]\u001b[A\n","Steps:  82% 1705/2092 [28:59<06:35,  1.02s/it]\u001b[A\n","Steps:  82% 1706/2092 [29:00<06:35,  1.02s/it]\u001b[A\n","Steps:  82% 1707/2092 [29:01<06:34,  1.02s/it]\u001b[A\n","Steps:  82% 1708/2092 [29:02<06:32,  1.02s/it]\u001b[A\n","Steps:  82% 1709/2092 [29:03<06:30,  1.02s/it]\u001b[A\n","Steps:  82% 1710/2092 [29:04<06:29,  1.02s/it]\u001b[A\n","Steps:  82% 1711/2092 [29:05<06:29,  1.02s/it]\u001b[A\n","Steps:  82% 1712/2092 [29:06<06:26,  1.02s/it]\u001b[A\n","Steps:  82% 1713/2092 [29:08<06:25,  1.02s/it]\u001b[A\n","Steps:  82% 1714/2092 [29:09<06:24,  1.02s/it]\u001b[A\n","Steps:  82% 1715/2092 [29:10<06:23,  1.02s/it]\u001b[A\n","Steps:  82% 1716/2092 [29:11<06:23,  1.02s/it]\u001b[A\n","Steps:  82% 1717/2092 [29:12<06:21,  1.02s/it]\u001b[A\n","Steps:  82% 1718/2092 [29:13<06:21,  1.02s/it]\u001b[A\n","Steps:  82% 1719/2092 [29:14<06:19,  1.02s/it]\u001b[A\n","Steps:  82% 1720/2092 [29:15<06:18,  1.02s/it]\u001b[A\n","Steps:  82% 1721/2092 [29:16<06:18,  1.02s/it]\u001b[A\n","Steps:  82% 1722/2092 [29:17<06:17,  1.02s/it]\u001b[A\n","Steps:  82% 1723/2092 [29:18<06:16,  1.02s/it]\u001b[A\n","Steps:  82% 1724/2092 [29:19<06:14,  1.02s/it]\u001b[A\n","Steps:  82% 1725/2092 [29:20<06:12,  1.02s/it]\u001b[A\n","Steps:  83% 1726/2092 [29:21<06:13,  1.02s/it]\u001b[A\n","Steps:  83% 1727/2092 [29:22<06:12,  1.02s/it]\u001b[A\n","Steps:  83% 1728/2092 [29:23<06:12,  1.02s/it]\u001b[A\n","Steps:  83% 1729/2092 [29:24<06:09,  1.02s/it]\u001b[A\n","Steps:  83% 1730/2092 [29:25<06:08,  1.02s/it]\u001b[A\n","Steps:  83% 1731/2092 [29:26<06:07,  1.02s/it]\u001b[A\n","Steps:  83% 1732/2092 [29:27<06:06,  1.02s/it]\u001b[A\n","Steps:  83% 1733/2092 [29:28<06:05,  1.02s/it]\u001b[A\n","Steps:  83% 1734/2092 [29:29<06:04,  1.02s/it]\u001b[A\n","Steps:  83% 1735/2092 [29:30<06:03,  1.02s/it]\u001b[A\n","Steps:  83% 1736/2092 [29:31<06:03,  1.02s/it]\u001b[A\n","Steps:  83% 1737/2092 [29:32<06:03,  1.02s/it]\u001b[A\n","Steps:  83% 1738/2092 [29:33<06:01,  1.02s/it]\u001b[A\n","Steps:  83% 1739/2092 [29:34<06:00,  1.02s/it]\u001b[A\n","Steps:  83% 1740/2092 [29:35<05:58,  1.02s/it]\u001b[A\n","Steps:  83% 1741/2092 [29:36<05:57,  1.02s/it]\u001b[A\n","Steps:  83% 1742/2092 [29:37<05:55,  1.02s/it]\u001b[A\n","Steps:  83% 1743/2092 [29:38<05:54,  1.02s/it]\u001b[A\n","Steps:  83% 1744/2092 [29:39<05:55,  1.02s/it]\u001b[A\n","Steps:  83% 1745/2092 [29:40<05:53,  1.02s/it]\u001b[A\n","Steps:  83% 1746/2092 [29:41<05:52,  1.02s/it]\u001b[A\n","Steps:  84% 1747/2092 [29:42<05:50,  1.02s/it]\u001b[A\n","Steps:  84% 1748/2092 [29:43<05:50,  1.02s/it]\u001b[A\n","Steps:  84% 1749/2092 [29:44<05:48,  1.02s/it]\u001b[A\n","Steps:  84% 1750/2092 [29:45<05:47,  1.02s/it]\u001b[A\n","Steps:  84% 1751/2092 [29:46<05:46,  1.02s/it]\u001b[A\n","Steps:  84% 1752/2092 [29:47<05:45,  1.02s/it]\u001b[A\n","Steps:  84% 1753/2092 [29:48<05:45,  1.02s/it]\u001b[A\n","Steps:  84% 1754/2092 [29:49<05:44,  1.02s/it]\u001b[A\n","Steps:  84% 1755/2092 [29:50<05:44,  1.02s/it]\u001b[A\n","Steps:  84% 1756/2092 [29:51<05:44,  1.02s/it]\u001b[A\n","Steps:  84% 1757/2092 [29:52<05:41,  1.02s/it]\u001b[A\n","Steps:  84% 1758/2092 [29:53<05:40,  1.02s/it]\u001b[A\n","Steps:  84% 1759/2092 [29:54<05:39,  1.02s/it]\u001b[A\n","Steps:  84% 1760/2092 [29:55<05:38,  1.02s/it]\u001b[A\n","Steps:  84% 1761/2092 [29:56<05:38,  1.02s/it]\u001b[A\n","Steps:  84% 1762/2092 [29:57<05:37,  1.02s/it]\u001b[A\n","Steps:  84% 1763/2092 [29:58<05:35,  1.02s/it]\u001b[A\n","Steps:  84% 1764/2092 [29:59<05:34,  1.02s/it]\u001b[A\n","Steps:  84% 1765/2092 [30:01<05:34,  1.02s/it]\u001b[A\n","Steps:  84% 1766/2092 [30:02<05:33,  1.02s/it]\u001b[A\n","Steps:  84% 1767/2092 [30:03<05:33,  1.02s/it]\u001b[A\n","Steps:  85% 1768/2092 [30:04<05:30,  1.02s/it]\u001b[A\n","Steps:  85% 1769/2092 [30:05<05:29,  1.02s/it]\u001b[A\n","Steps:  85% 1770/2092 [30:06<05:28,  1.02s/it]\u001b[A\n","Steps:  85% 1771/2092 [30:07<05:26,  1.02s/it]\u001b[A\n","Steps:  85% 1772/2092 [30:08<05:25,  1.02s/it]\u001b[A\n","Steps:  85% 1773/2092 [30:09<05:22,  1.01s/it]\u001b[A\n","Steps:  85% 1774/2092 [30:10<05:23,  1.02s/it]\u001b[A\n","Steps:  85% 1775/2092 [30:11<05:23,  1.02s/it]\u001b[A\n","Steps:  85% 1776/2092 [30:12<05:22,  1.02s/it]\u001b[A\n","Steps:  85% 1777/2092 [30:13<05:21,  1.02s/it]\u001b[A\n","Steps:  85% 1778/2092 [30:14<05:19,  1.02s/it]\u001b[A\n","Steps:  85% 1779/2092 [30:15<05:18,  1.02s/it]\u001b[A\n","Steps:  85% 1780/2092 [30:16<05:17,  1.02s/it]\u001b[A\n","Steps:  85% 1781/2092 [30:17<05:17,  1.02s/it]\u001b[A\n","Steps:  85% 1782/2092 [30:18<05:16,  1.02s/it]\u001b[A\n","Steps:  85% 1783/2092 [30:19<05:14,  1.02s/it]\u001b[A\n","Steps:  85% 1784/2092 [30:20<05:14,  1.02s/it]\u001b[A\n","Steps:  85% 1785/2092 [30:21<05:13,  1.02s/it]\u001b[A\n","Steps:  85% 1786/2092 [30:22<05:13,  1.02s/it]\u001b[A\n","Steps:  85% 1787/2092 [30:23<05:11,  1.02s/it]\u001b[A\n","Steps:  85% 1788/2092 [30:24<05:10,  1.02s/it]\u001b[A\n","Steps:  86% 1789/2092 [30:25<05:09,  1.02s/it]\u001b[A\n","Steps:  86% 1790/2092 [30:26<05:08,  1.02s/it]\u001b[A\n","Steps:  86% 1791/2092 [30:27<05:07,  1.02s/it]\u001b[A\n","Steps:  86% 1792/2092 [30:28<05:07,  1.02s/it]\u001b[A\n","Steps:  86% 1793/2092 [30:29<05:05,  1.02s/it]\u001b[A\n","Steps:  86% 1794/2092 [30:30<05:03,  1.02s/it]\u001b[A\n","Steps:  86% 1795/2092 [30:31<05:02,  1.02s/it]\u001b[A\n","Steps:  86% 1796/2092 [30:32<05:02,  1.02s/it]\u001b[A\n","Steps:  86% 1797/2092 [30:33<05:00,  1.02s/it]\u001b[A\n","Steps:  86% 1798/2092 [30:34<04:59,  1.02s/it]\u001b[A\n","Steps:  86% 1799/2092 [30:35<04:59,  1.02s/it]\u001b[A\n","Steps:  86% 1800/2092 [30:36<04:58,  1.02s/it]\u001b[A\n","Steps:  86% 1801/2092 [30:37<04:56,  1.02s/it]\u001b[A\n","Steps:  86% 1802/2092 [30:38<04:56,  1.02s/it]\u001b[A\n","Steps:  86% 1803/2092 [30:39<04:55,  1.02s/it]\u001b[A\n","Steps:  86% 1804/2092 [30:40<04:53,  1.02s/it]\u001b[A\n","Steps:  86% 1805/2092 [30:41<04:53,  1.02s/it]\u001b[A\n","Steps:  86% 1806/2092 [30:42<04:51,  1.02s/it]\u001b[A\n","Steps:  86% 1807/2092 [30:43<04:49,  1.02s/it]\u001b[A\n","Steps:  86% 1808/2092 [30:44<04:49,  1.02s/it]\u001b[A\n","Steps:  86% 1809/2092 [30:45<04:47,  1.02s/it]\u001b[A\n","Steps:  87% 1810/2092 [30:46<04:46,  1.02s/it]\u001b[A\n","Steps:  87% 1811/2092 [30:47<04:46,  1.02s/it]\u001b[A\n","Steps:  87% 1812/2092 [30:48<04:45,  1.02s/it]\u001b[A\n","Steps:  87% 1813/2092 [30:49<04:44,  1.02s/it]\u001b[A\n","Steps:  87% 1814/2092 [30:50<04:42,  1.02s/it]\u001b[A\n","Steps:  87% 1815/2092 [30:52<04:42,  1.02s/it]\u001b[A\n","Steps:  87% 1816/2092 [30:53<04:42,  1.02s/it]\u001b[A\n","Steps:  87% 1817/2092 [30:54<04:40,  1.02s/it]\u001b[A\n","Steps:  87% 1818/2092 [30:55<04:39,  1.02s/it]\u001b[A\n","Steps:  87% 1819/2092 [30:56<04:38,  1.02s/it]\u001b[A\n","Steps:  87% 1820/2092 [30:57<04:38,  1.02s/it]\u001b[A\n","Steps:  87% 1821/2092 [30:58<04:36,  1.02s/it]\u001b[A\n","Steps:  87% 1822/2092 [30:59<04:34,  1.02s/it]\u001b[A\n","Steps:  87% 1823/2092 [31:00<04:33,  1.02s/it]\u001b[A\n","Steps:  87% 1824/2092 [31:01<04:32,  1.02s/it]\u001b[A\n","Steps:  87% 1825/2092 [31:02<04:31,  1.02s/it]\u001b[A\n","Steps:  87% 1826/2092 [31:03<04:29,  1.01s/it]\u001b[A\n","Steps:  87% 1827/2092 [31:04<04:29,  1.02s/it]\u001b[A\n","Steps:  87% 1828/2092 [31:05<04:27,  1.01s/it]\u001b[A\n","Steps:  87% 1829/2092 [31:06<04:26,  1.01s/it]\u001b[A\n","Steps:  87% 1830/2092 [31:07<04:25,  1.01s/it]\u001b[A\n","Steps:  88% 1831/2092 [31:08<04:24,  1.01s/it]\u001b[A\n","Steps:  88% 1832/2092 [31:09<04:23,  1.01s/it]\u001b[A\n","Steps:  88% 1833/2092 [31:10<04:22,  1.01s/it]\u001b[A\n","Steps:  88% 1834/2092 [31:11<04:22,  1.02s/it]\u001b[A\n","Steps:  88% 1835/2092 [31:12<04:22,  1.02s/it]\u001b[A\n","Steps:  88% 1836/2092 [31:13<04:20,  1.02s/it]\u001b[A\n","Steps:  88% 1837/2092 [31:14<04:19,  1.02s/it]\u001b[A\n","Steps:  88% 1838/2092 [31:15<04:18,  1.02s/it]\u001b[A\n","Steps:  88% 1839/2092 [31:16<04:18,  1.02s/it]\u001b[A\n","Steps:  88% 1840/2092 [31:17<04:17,  1.02s/it]\u001b[A\n","Steps:  88% 1841/2092 [31:18<04:15,  1.02s/it]\u001b[A\n","Steps:  88% 1842/2092 [31:19<04:14,  1.02s/it]\u001b[A\n","Steps:  88% 1843/2092 [31:20<04:14,  1.02s/it]\u001b[A\n","Steps:  88% 1844/2092 [31:21<04:13,  1.02s/it]\u001b[A\n","Steps:  88% 1845/2092 [31:22<04:12,  1.02s/it]\u001b[A\n","Steps:  88% 1846/2092 [31:23<04:11,  1.02s/it]\u001b[A\n","Steps:  88% 1847/2092 [31:24<04:10,  1.02s/it]\u001b[A\n","Steps:  88% 1848/2092 [31:25<04:09,  1.02s/it]\u001b[A\n","Steps:  88% 1849/2092 [31:26<04:08,  1.02s/it]\u001b[A\n","Steps:  88% 1850/2092 [31:27<04:07,  1.02s/it]\u001b[A\n","Steps:  88% 1851/2092 [31:28<04:06,  1.02s/it]\u001b[A\n","Steps:  89% 1852/2092 [31:29<04:05,  1.02s/it]\u001b[A\n","Steps:  89% 1853/2092 [31:30<04:04,  1.02s/it]\u001b[A\n","Steps:  89% 1854/2092 [31:31<04:03,  1.02s/it]\u001b[A\n","Steps:  89% 1855/2092 [31:32<04:02,  1.02s/it]\u001b[A\n","Steps:  89% 1856/2092 [31:33<04:01,  1.02s/it]\u001b[A\n","Steps:  89% 1857/2092 [31:34<03:59,  1.02s/it]\u001b[A\n","Steps:  89% 1858/2092 [31:35<03:58,  1.02s/it]\u001b[A\n","Steps:  89% 1859/2092 [31:36<03:57,  1.02s/it]\u001b[A\n","Steps:  89% 1860/2092 [31:37<03:57,  1.02s/it]\u001b[A\n","Steps:  89% 1861/2092 [31:38<03:56,  1.02s/it]\u001b[A\n","Steps:  89% 1862/2092 [31:39<03:55,  1.02s/it]\u001b[A\n","Steps:  89% 1863/2092 [31:40<03:54,  1.02s/it]\u001b[A\n","Steps:  89% 1864/2092 [31:41<03:53,  1.02s/it]\u001b[A\n","Steps:  89% 1865/2092 [31:42<03:51,  1.02s/it]\u001b[A\n","Steps:  89% 1866/2092 [31:44<03:50,  1.02s/it]\u001b[A\n","Steps:  89% 1867/2092 [31:45<03:49,  1.02s/it]\u001b[A\n","Steps:  89% 1868/2092 [31:46<03:49,  1.02s/it]\u001b[A\n","Steps:  89% 1869/2092 [31:47<03:48,  1.03s/it]\u001b[A\n","Steps:  89% 1870/2092 [31:48<03:48,  1.03s/it]\u001b[A\n","Steps:  89% 1871/2092 [31:49<03:46,  1.02s/it]\u001b[A\n","Steps:  89% 1872/2092 [31:50<03:44,  1.02s/it]\u001b[A\n","Steps:  90% 1873/2092 [31:51<03:43,  1.02s/it]\u001b[A\n","Steps:  90% 1874/2092 [31:52<03:43,  1.02s/it]\u001b[A\n","Steps:  90% 1875/2092 [31:53<03:42,  1.02s/it]\u001b[A\n","Steps:  90% 1876/2092 [31:54<03:40,  1.02s/it]\u001b[A\n","Steps:  90% 1877/2092 [31:55<03:39,  1.02s/it]\u001b[A\n","Steps:  90% 1878/2092 [31:56<03:38,  1.02s/it]\u001b[A\n","Steps:  90% 1879/2092 [31:57<03:37,  1.02s/it]\u001b[A\n","Steps:  90% 1880/2092 [31:58<03:36,  1.02s/it]\u001b[A\n","Steps:  90% 1881/2092 [31:59<03:35,  1.02s/it]\u001b[A\n","Steps:  90% 1882/2092 [32:00<03:34,  1.02s/it]\u001b[A\n","Steps:  90% 1883/2092 [32:01<03:33,  1.02s/it]\u001b[A\n","Steps:  90% 1884/2092 [32:02<03:32,  1.02s/it]\u001b[A\n","Steps:  90% 1885/2092 [32:03<03:31,  1.02s/it]\u001b[A\n","Steps:  90% 1886/2092 [32:04<03:30,  1.02s/it]\u001b[A\n","Steps:  90% 1887/2092 [32:05<03:28,  1.02s/it]\u001b[A\n","Steps:  90% 1888/2092 [32:06<03:28,  1.02s/it]\u001b[A\n","Steps:  90% 1889/2092 [32:07<03:27,  1.02s/it]\u001b[A\n","Steps:  90% 1890/2092 [32:08<03:26,  1.02s/it]\u001b[A\n","Steps:  90% 1891/2092 [32:09<03:25,  1.02s/it]\u001b[A\n","Steps:  90% 1892/2092 [32:10<03:23,  1.02s/it]\u001b[A\n","Steps:  90% 1893/2092 [32:11<03:22,  1.02s/it]\u001b[A\n","Steps:  91% 1894/2092 [32:12<03:22,  1.02s/it]\u001b[A\n","Steps:  91% 1895/2092 [32:13<03:20,  1.02s/it]\u001b[A\n","Steps:  91% 1896/2092 [32:14<03:20,  1.02s/it]\u001b[A\n","Steps:  91% 1897/2092 [32:15<03:19,  1.02s/it]\u001b[A\n","Steps:  91% 1898/2092 [32:16<03:18,  1.02s/it]\u001b[A\n","Steps:  91% 1899/2092 [32:17<03:17,  1.02s/it]\u001b[A\n","Steps:  91% 1900/2092 [32:18<03:16,  1.02s/it]\u001b[A\n","Steps:  91% 1901/2092 [32:19<03:15,  1.02s/it]\u001b[A\n","Steps:  91% 1902/2092 [32:20<03:14,  1.03s/it]\u001b[A\n","Steps:  91% 1903/2092 [32:21<03:13,  1.03s/it]\u001b[A\n","Steps:  91% 1904/2092 [32:22<03:12,  1.02s/it]\u001b[A\n","Steps:  91% 1905/2092 [32:23<03:10,  1.02s/it]\u001b[A\n","Steps:  91% 1906/2092 [32:24<03:10,  1.02s/it]\u001b[A\n","Steps:  91% 1907/2092 [32:25<03:08,  1.02s/it]\u001b[A\n","Steps:  91% 1908/2092 [32:26<03:07,  1.02s/it]\u001b[A11/24/2024 18:14:31 - INFO - train_eval -   *****1 - 1908: loss: 1.663497805595398 *****\n","\n","Steps:  91% 1909/2092 [32:27<03:06,  1.02s/it]\u001b[A\n","Steps:  91% 1910/2092 [32:28<03:05,  1.02s/it]\u001b[A\n","Steps:  91% 1911/2092 [32:30<03:04,  1.02s/it]\u001b[A\n","Steps:  91% 1912/2092 [32:31<03:04,  1.02s/it]\u001b[A\n","Steps:  91% 1913/2092 [32:32<03:02,  1.02s/it]\u001b[A\n","Steps:  91% 1914/2092 [32:33<03:01,  1.02s/it]\u001b[A\n","Steps:  92% 1915/2092 [32:34<02:59,  1.02s/it]\u001b[A\n","Steps:  92% 1916/2092 [32:35<02:58,  1.01s/it]\u001b[A\n","Steps:  92% 1917/2092 [32:36<02:58,  1.02s/it]\u001b[A\n","Steps:  92% 1918/2092 [32:37<02:56,  1.02s/it]\u001b[A\n","Steps:  92% 1919/2092 [32:38<02:54,  1.01s/it]\u001b[A\n","Steps:  92% 1920/2092 [32:39<02:54,  1.02s/it]\u001b[A\n","Steps:  92% 1921/2092 [32:40<02:54,  1.02s/it]\u001b[A\n","Steps:  92% 1922/2092 [32:41<02:52,  1.02s/it]\u001b[A\n","Steps:  92% 1923/2092 [32:42<02:51,  1.02s/it]\u001b[A\n","Steps:  92% 1924/2092 [32:43<02:51,  1.02s/it]\u001b[A\n","Steps:  92% 1925/2092 [32:44<02:49,  1.02s/it]\u001b[A\n","Steps:  92% 1926/2092 [32:45<02:48,  1.02s/it]\u001b[A\n","Steps:  92% 1927/2092 [32:46<02:47,  1.02s/it]\u001b[A\n","Steps:  92% 1928/2092 [32:47<02:46,  1.01s/it]\u001b[A\n","Steps:  92% 1929/2092 [32:48<02:45,  1.02s/it]\u001b[A\n","Steps:  92% 1930/2092 [32:49<02:44,  1.02s/it]\u001b[A\n","Steps:  92% 1931/2092 [32:50<02:43,  1.02s/it]\u001b[A\n","Steps:  92% 1932/2092 [32:51<02:43,  1.02s/it]\u001b[A\n","Steps:  92% 1933/2092 [32:52<02:42,  1.02s/it]\u001b[A\n","Steps:  92% 1934/2092 [32:53<02:41,  1.02s/it]\u001b[A\n","Steps:  92% 1935/2092 [32:54<02:40,  1.02s/it]\u001b[A\n","Steps:  93% 1936/2092 [32:55<02:39,  1.02s/it]\u001b[A\n","Steps:  93% 1937/2092 [32:56<02:38,  1.02s/it]\u001b[A\n","Steps:  93% 1938/2092 [32:57<02:37,  1.02s/it]\u001b[A\n","Steps:  93% 1939/2092 [32:58<02:36,  1.02s/it]\u001b[A\n","Steps:  93% 1940/2092 [32:59<02:35,  1.02s/it]\u001b[A\n","Steps:  93% 1941/2092 [33:00<02:34,  1.02s/it]\u001b[A\n","Steps:  93% 1942/2092 [33:01<02:33,  1.02s/it]\u001b[A\n","Steps:  93% 1943/2092 [33:02<02:32,  1.02s/it]\u001b[A\n","Steps:  93% 1944/2092 [33:03<02:31,  1.02s/it]\u001b[A\n","Steps:  93% 1945/2092 [33:04<02:30,  1.02s/it]\u001b[A\n","Steps:  93% 1946/2092 [33:05<02:29,  1.02s/it]\u001b[A\n","Steps:  93% 1947/2092 [33:06<02:27,  1.02s/it]\u001b[A\n","Steps:  93% 1948/2092 [33:07<02:27,  1.02s/it]\u001b[A\n","Steps:  93% 1949/2092 [33:08<02:26,  1.02s/it]\u001b[A\n","Steps:  93% 1950/2092 [33:09<02:25,  1.02s/it]\u001b[A\n","Steps:  93% 1951/2092 [33:10<02:24,  1.02s/it]\u001b[A\n","Steps:  93% 1952/2092 [33:11<02:23,  1.02s/it]\u001b[A\n","Steps:  93% 1953/2092 [33:12<02:21,  1.02s/it]\u001b[A\n","Steps:  93% 1954/2092 [33:13<02:21,  1.02s/it]\u001b[A\n","Steps:  93% 1955/2092 [33:14<02:19,  1.02s/it]\u001b[A\n","Steps:  93% 1956/2092 [33:15<02:18,  1.02s/it]\u001b[A\n","Steps:  94% 1957/2092 [33:16<02:18,  1.02s/it]\u001b[A\n","Steps:  94% 1958/2092 [33:17<02:16,  1.02s/it]\u001b[A\n","Steps:  94% 1959/2092 [33:18<02:15,  1.02s/it]\u001b[A\n","Steps:  94% 1960/2092 [33:19<02:14,  1.02s/it]\u001b[A\n","Steps:  94% 1961/2092 [33:20<02:13,  1.02s/it]\u001b[A\n","Steps:  94% 1962/2092 [33:22<02:13,  1.02s/it]\u001b[A\n","Steps:  94% 1963/2092 [33:23<02:12,  1.03s/it]\u001b[A\n","Steps:  94% 1964/2092 [33:24<02:10,  1.02s/it]\u001b[A\n","Steps:  94% 1965/2092 [33:25<02:09,  1.02s/it]\u001b[A\n","Steps:  94% 1966/2092 [33:26<02:08,  1.02s/it]\u001b[A\n","Steps:  94% 1967/2092 [33:27<02:06,  1.01s/it]\u001b[A\n","Steps:  94% 1968/2092 [33:28<02:05,  1.01s/it]\u001b[A\n","Steps:  94% 1969/2092 [33:29<02:05,  1.02s/it]\u001b[A\n","Steps:  94% 1970/2092 [33:30<02:04,  1.02s/it]\u001b[A\n","Steps:  94% 1971/2092 [33:31<02:03,  1.02s/it]\u001b[A\n","Steps:  94% 1972/2092 [33:32<02:02,  1.02s/it]\u001b[A\n","Steps:  94% 1973/2092 [33:33<02:00,  1.01s/it]\u001b[A\n","Steps:  94% 1974/2092 [33:34<02:00,  1.02s/it]\u001b[A\n","Steps:  94% 1975/2092 [33:35<01:59,  1.02s/it]\u001b[A\n","Steps:  94% 1976/2092 [33:36<01:58,  1.02s/it]\u001b[A\n","Steps:  95% 1977/2092 [33:37<01:57,  1.02s/it]\u001b[A\n","Steps:  95% 1978/2092 [33:38<01:56,  1.02s/it]\u001b[A\n","Steps:  95% 1979/2092 [33:39<01:55,  1.02s/it]\u001b[A\n","Steps:  95% 1980/2092 [33:40<01:54,  1.03s/it]\u001b[A\n","Steps:  95% 1981/2092 [33:41<01:53,  1.02s/it]\u001b[A\n","Steps:  95% 1982/2092 [33:42<01:52,  1.02s/it]\u001b[A\n","Steps:  95% 1983/2092 [33:43<01:51,  1.02s/it]\u001b[A\n","Steps:  95% 1984/2092 [33:44<01:50,  1.02s/it]\u001b[A\n","Steps:  95% 1985/2092 [33:45<01:49,  1.02s/it]\u001b[A\n","Steps:  95% 1986/2092 [33:46<01:48,  1.02s/it]\u001b[A\n","Steps:  95% 1987/2092 [33:47<01:46,  1.02s/it]\u001b[A\n","Steps:  95% 1988/2092 [33:48<01:45,  1.01s/it]\u001b[A\n","Steps:  95% 1989/2092 [33:49<01:44,  1.02s/it]\u001b[A\n","Steps:  95% 1990/2092 [33:50<01:44,  1.02s/it]\u001b[A\n","Steps:  95% 1991/2092 [33:51<01:42,  1.02s/it]\u001b[A\n","Steps:  95% 1992/2092 [33:52<01:41,  1.02s/it]\u001b[A\n","Steps:  95% 1993/2092 [33:53<01:41,  1.02s/it]\u001b[A\n","Steps:  95% 1994/2092 [33:54<01:40,  1.02s/it]\u001b[A\n","Steps:  95% 1995/2092 [33:55<01:39,  1.02s/it]\u001b[A\n","Steps:  95% 1996/2092 [33:56<01:37,  1.02s/it]\u001b[A\n","Steps:  95% 1997/2092 [33:57<01:37,  1.02s/it]\u001b[A\n","Steps:  96% 1998/2092 [33:58<01:36,  1.02s/it]\u001b[A\n","Steps:  96% 1999/2092 [33:59<01:34,  1.02s/it]\u001b[A\n","Steps:  96% 2000/2092 [34:00<01:34,  1.02s/it]\u001b[A\n","Steps:  96% 2001/2092 [34:01<01:33,  1.02s/it]\u001b[A\n","Steps:  96% 2002/2092 [34:02<01:31,  1.02s/it]\u001b[A\n","Steps:  96% 2003/2092 [34:03<01:30,  1.02s/it]\u001b[A\n","Steps:  96% 2004/2092 [34:04<01:29,  1.02s/it]\u001b[A\n","Steps:  96% 2005/2092 [34:05<01:29,  1.02s/it]\u001b[A\n","Steps:  96% 2006/2092 [34:06<01:28,  1.02s/it]\u001b[A\n","Steps:  96% 2007/2092 [34:07<01:27,  1.02s/it]\u001b[A\n","Steps:  96% 2008/2092 [34:08<01:25,  1.02s/it]\u001b[A\n","Steps:  96% 2009/2092 [34:09<01:24,  1.02s/it]\u001b[A\n","Steps:  96% 2010/2092 [34:11<01:23,  1.02s/it]\u001b[A\n","Steps:  96% 2011/2092 [34:12<01:22,  1.02s/it]\u001b[A\n","Steps:  96% 2012/2092 [34:13<01:21,  1.02s/it]\u001b[A\n","Steps:  96% 2013/2092 [34:14<01:20,  1.02s/it]\u001b[A\n","Steps:  96% 2014/2092 [34:15<01:19,  1.02s/it]\u001b[A\n","Steps:  96% 2015/2092 [34:16<01:18,  1.02s/it]\u001b[A\n","Steps:  96% 2016/2092 [34:17<01:17,  1.02s/it]\u001b[A\n","Steps:  96% 2017/2092 [34:18<01:16,  1.02s/it]\u001b[A\n","Steps:  96% 2018/2092 [34:19<01:15,  1.02s/it]\u001b[A\n","Steps:  97% 2019/2092 [34:20<01:14,  1.02s/it]\u001b[A\n","Steps:  97% 2020/2092 [34:21<01:13,  1.02s/it]\u001b[A\n","Steps:  97% 2021/2092 [34:22<01:12,  1.02s/it]\u001b[A\n","Steps:  97% 2022/2092 [34:23<01:11,  1.02s/it]\u001b[A\n","Steps:  97% 2023/2092 [34:24<01:10,  1.02s/it]\u001b[A\n","Steps:  97% 2024/2092 [34:25<01:09,  1.02s/it]\u001b[A\n","Steps:  97% 2025/2092 [34:26<01:08,  1.02s/it]\u001b[A\n","Steps:  97% 2026/2092 [34:27<01:07,  1.02s/it]\u001b[A\n","Steps:  97% 2027/2092 [34:28<01:06,  1.02s/it]\u001b[A\n","Steps:  97% 2028/2092 [34:29<01:05,  1.02s/it]\u001b[A\n","Steps:  97% 2029/2092 [34:30<01:04,  1.02s/it]\u001b[A\n","Steps:  97% 2030/2092 [34:31<01:03,  1.02s/it]\u001b[A\n","Steps:  97% 2031/2092 [34:32<01:02,  1.02s/it]\u001b[A\n","Steps:  97% 2032/2092 [34:33<01:01,  1.02s/it]\u001b[A\n","Steps:  97% 2033/2092 [34:34<01:00,  1.02s/it]\u001b[A\n","Steps:  97% 2034/2092 [34:35<00:59,  1.02s/it]\u001b[A\n","Steps:  97% 2035/2092 [34:36<00:58,  1.02s/it]\u001b[A\n","Steps:  97% 2036/2092 [34:37<00:57,  1.02s/it]\u001b[A\n","Steps:  97% 2037/2092 [34:38<00:55,  1.02s/it]\u001b[A\n","Steps:  97% 2038/2092 [34:39<00:54,  1.02s/it]\u001b[A\n","Steps:  97% 2039/2092 [34:40<00:54,  1.02s/it]\u001b[A\n","Steps:  98% 2040/2092 [34:41<00:53,  1.02s/it]\u001b[A\n","Steps:  98% 2041/2092 [34:42<00:52,  1.02s/it]\u001b[A\n","Steps:  98% 2042/2092 [34:43<00:51,  1.02s/it]\u001b[A\n","Steps:  98% 2043/2092 [34:44<00:50,  1.02s/it]\u001b[A\n","Steps:  98% 2044/2092 [34:45<00:48,  1.02s/it]\u001b[A\n","Steps:  98% 2045/2092 [34:46<00:47,  1.02s/it]\u001b[A\n","Steps:  98% 2046/2092 [34:47<00:46,  1.02s/it]\u001b[A\n","Steps:  98% 2047/2092 [34:48<00:45,  1.02s/it]\u001b[A\n","Steps:  98% 2048/2092 [34:49<00:44,  1.02s/it]\u001b[A\n","Steps:  98% 2049/2092 [34:50<00:43,  1.02s/it]\u001b[A\n","Steps:  98% 2050/2092 [34:51<00:42,  1.02s/it]\u001b[A\n","Steps:  98% 2051/2092 [34:52<00:41,  1.02s/it]\u001b[A\n","Steps:  98% 2052/2092 [34:53<00:40,  1.02s/it]\u001b[A\n","Steps:  98% 2053/2092 [34:54<00:39,  1.02s/it]\u001b[A\n","Steps:  98% 2054/2092 [34:55<00:38,  1.02s/it]\u001b[A\n","Steps:  98% 2055/2092 [34:56<00:37,  1.02s/it]\u001b[A\n","Steps:  98% 2056/2092 [34:57<00:36,  1.02s/it]\u001b[A\n","Steps:  98% 2057/2092 [34:58<00:35,  1.02s/it]\u001b[A\n","Steps:  98% 2058/2092 [35:00<00:34,  1.02s/it]\u001b[A\n","Steps:  98% 2059/2092 [35:01<00:33,  1.02s/it]\u001b[A\n","Steps:  98% 2060/2092 [35:02<00:32,  1.02s/it]\u001b[A\n","Steps:  99% 2061/2092 [35:03<00:31,  1.02s/it]\u001b[A\n","Steps:  99% 2062/2092 [35:04<00:30,  1.02s/it]\u001b[A\n","Steps:  99% 2063/2092 [35:05<00:29,  1.02s/it]\u001b[A\n","Steps:  99% 2064/2092 [35:06<00:28,  1.02s/it]\u001b[A\n","Steps:  99% 2065/2092 [35:07<00:27,  1.02s/it]\u001b[A\n","Steps:  99% 2066/2092 [35:08<00:26,  1.02s/it]\u001b[A\n","Steps:  99% 2067/2092 [35:09<00:25,  1.02s/it]\u001b[A\n","Steps:  99% 2068/2092 [35:10<00:24,  1.02s/it]\u001b[A\n","Steps:  99% 2069/2092 [35:11<00:23,  1.02s/it]\u001b[A\n","Steps:  99% 2070/2092 [35:12<00:22,  1.02s/it]\u001b[A\n","Steps:  99% 2071/2092 [35:13<00:21,  1.02s/it]\u001b[A\n","Steps:  99% 2072/2092 [35:14<00:20,  1.02s/it]\u001b[A\n","Steps:  99% 2073/2092 [35:15<00:19,  1.02s/it]\u001b[A\n","Steps:  99% 2074/2092 [35:16<00:18,  1.02s/it]\u001b[A\n","Steps:  99% 2075/2092 [35:17<00:17,  1.02s/it]\u001b[A\n","Steps:  99% 2076/2092 [35:18<00:16,  1.02s/it]\u001b[A\n","Steps:  99% 2077/2092 [35:19<00:15,  1.02s/it]\u001b[A\n","Steps:  99% 2078/2092 [35:20<00:14,  1.02s/it]\u001b[A\n","Steps:  99% 2079/2092 [35:21<00:13,  1.02s/it]\u001b[A\n","Steps:  99% 2080/2092 [35:22<00:12,  1.03s/it]\u001b[A\n","Steps:  99% 2081/2092 [35:23<00:11,  1.02s/it]\u001b[A\n","Steps: 100% 2082/2092 [35:24<00:10,  1.02s/it]\u001b[A\n","Steps: 100% 2083/2092 [35:25<00:09,  1.02s/it]\u001b[A\n","Steps: 100% 2084/2092 [35:26<00:08,  1.02s/it]\u001b[A\n","Steps: 100% 2085/2092 [35:27<00:07,  1.02s/it]\u001b[A\n","Steps: 100% 2086/2092 [35:28<00:06,  1.02s/it]\u001b[A\n","Steps: 100% 2087/2092 [35:29<00:05,  1.01s/it]\u001b[A\n","Steps: 100% 2088/2092 [35:30<00:04,  1.02s/it]\u001b[A\n","Steps: 100% 2089/2092 [35:31<00:03,  1.02s/it]\u001b[A\n","Steps: 100% 2090/2092 [35:32<00:02,  1.02s/it]\u001b[A\n","Steps: 100% 2091/2092 [35:33<00:01,  1.02s/it]\u001b[A\n","Steps: 100% 2092/2092 [35:34<00:00,  1.01s/it]\u001b[A\n","                                              \u001b[A11/24/2024 18:17:43 - INFO - train_eval -   Saving model checkpoint to ckpt/30_integrated_model_test/checkpoint-4185\n","Epoch 2 done\n","Epochs: 100% 2/2 [1:11:24<00:00, 2142.17s/it]\n","Figure(640x480)\n","Figure(640x480)\n","Figure(640x480)\n","Figure(640x480)\n","11/24/2024 18:17:44 - INFO - utils.utils -   Evaluate the following checkpoints: [(1, 'ckpt/30_integrated_model_test/checkpoint-4185')]\n","\n","\n","\ttarget is - sampled_answer_span_test.json \n","\n","11/24/2024 18:17:44 - INFO - utils.utils -   Loading features from cached file data/data_cache/cached_sampled_answer_span_test_512_inte\n","/content/drive/MyDrive/training/30-finlaw-src/utils/utils.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  features_and_dataset = torch.load(cached_features_file_path)\n","11/24/2024 18:17:45 - INFO - train_eval -   ***** Running evaluation  *****\n","11/24/2024 18:17:45 - INFO - train_eval -     Num examples = 1203\n","11/24/2024 18:17:45 - INFO - train_eval -     Batch size = 128\n","Evaluating:   0% 0/12 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","Evaluating: 100% 12/12 [00:09<00:00,  1.22it/s]\n","11/24/2024 18:17:55 - INFO - train_eval -     Evaluation done in total 9.845403 secs (0.006818 sec per example)\n","11/24/2024 18:17:55 - INFO - utils.compute -   Writing predictions to: result/30_integrated_model_test/predictions_sampled_answer_span_test.json\n","1203it [00:04, 292.61it/s]\n","11/24/2024 18:17:59 - INFO - train_eval -   ***** Official Eval results *****\n","11/24/2024 18:18:00 - INFO - train_eval -   ****** 30_integrated_model_test/sampled_answer_span_test ******\n","11/24/2024 18:18:00 - INFO - train_eval -     Count_HasAns_exact = 923\n","11/24/2024 18:18:00 - INFO - train_eval -     Count_HasAns_f1 = 225\n","11/24/2024 18:18:00 - INFO - train_eval -     Count_HasAns_none = 0\n","11/24/2024 18:18:00 - INFO - train_eval -     Count_HasAns_zero = 55\n","11/24/2024 18:18:00 - INFO - train_eval -     Count_NoAns_exact = 0\n","11/24/2024 18:18:00 - INFO - train_eval -     Count_NoAns_zero = 0\n","11/24/2024 18:18:00 - INFO - train_eval -     HasAns_exact = 76.72485453034082\n","11/24/2024 18:18:00 - INFO - train_eval -     HasAns_f1 = 87.75737510926531\n","11/24/2024 18:18:00 - INFO - train_eval -     HasAns_total = 1203\n","11/24/2024 18:18:00 - INFO - train_eval -     exact = 76.72485453034082\n","11/24/2024 18:18:00 - INFO - train_eval -     f1 = 87.75737510926531\n","11/24/2024 18:18:00 - INFO - train_eval -     total = 1203\n","\n","\n","\ttarget is - sampled_table_test.json \n","\n","11/24/2024 18:18:00 - INFO - utils.utils -   Creating features from dataset file at data sampled_table_test.json\n","100% 761/761 [00:01<00:00, 730.88it/s]\n","convert answer_span examples to features: 100% 1200/1200 [00:11<00:00, 101.72it/s]\n","add example index and unique id: 100% 1200/1200 [00:00<00:00, 167181.45it/s]\n","11/24/2024 18:18:14 - INFO - utils.utils -   Saving features into cached file data/data_cache/cached_sampled_table_test_512_inte\n","11/24/2024 18:18:28 - INFO - train_eval -   ***** Running evaluation  *****\n","11/24/2024 18:18:28 - INFO - train_eval -     Num examples = 1200\n","11/24/2024 18:18:28 - INFO - train_eval -     Batch size = 128\n","Evaluating: 100% 60/60 [00:51<00:00,  1.17it/s]\n","11/24/2024 18:19:19 - INFO - train_eval -     Evaluation done in total 51.308147 secs (0.006689 sec per example)\n","11/24/2024 18:19:19 - INFO - utils.compute -   Writing predictions to: result/30_integrated_model_test/predictions_sampled_table_test.json\n","1200it [00:03, 343.48it/s]\n","11/24/2024 18:19:23 - INFO - train_eval -   ***** Official Eval results *****\n","11/24/2024 18:19:23 - INFO - train_eval -   ****** 30_integrated_model_test/sampled_table_test ******\n","11/24/2024 18:19:23 - INFO - train_eval -     Count_HasAns_exact = 0\n","11/24/2024 18:19:23 - INFO - train_eval -     Count_HasAns_f1 = 3\n","11/24/2024 18:19:23 - INFO - train_eval -     Count_HasAns_none = 0\n","11/24/2024 18:19:23 - INFO - train_eval -     Count_HasAns_zero = 1197\n","11/24/2024 18:19:23 - INFO - train_eval -     Count_NoAns_exact = 0\n","11/24/2024 18:19:23 - INFO - train_eval -     Count_NoAns_zero = 0\n","11/24/2024 18:19:23 - INFO - train_eval -     HasAns_exact = 0.0\n","11/24/2024 18:19:23 - INFO - train_eval -     HasAns_f1 = 0.04792768959435627\n","11/24/2024 18:19:23 - INFO - train_eval -     HasAns_total = 1200\n","11/24/2024 18:19:23 - INFO - train_eval -     exact = 0.0\n","11/24/2024 18:19:23 - INFO - train_eval -     f1 = 0.04792768959435627\n","11/24/2024 18:19:23 - INFO - train_eval -     total = 1200\n","\n","\n","\ttarget is - sampled_list_test.json \n","\n","11/24/2024 18:19:23 - INFO - utils.utils -   Creating features from dataset file at data sampled_list_test.json\n","100% 751/751 [00:00<00:00, 3023.47it/s]\n","convert answer_span examples to features: 100% 803/803 [00:01<00:00, 534.60it/s]\n","add example index and unique id: 100% 803/803 [00:00<00:00, 624633.92it/s]\n","11/24/2024 18:19:26 - INFO - utils.utils -   Saving features into cached file data/data_cache/cached_sampled_list_test_512_inte\n","11/24/2024 18:19:28 - INFO - train_eval -   ***** Running evaluation  *****\n","11/24/2024 18:19:28 - INFO - train_eval -     Num examples = 803\n","11/24/2024 18:19:28 - INFO - train_eval -     Batch size = 128\n","Evaluating: 100% 9/9 [00:07<00:00,  1.28it/s]\n","11/24/2024 18:19:35 - INFO - train_eval -     Evaluation done in total 7.053925 secs (0.006750 sec per example)\n","11/24/2024 18:19:35 - INFO - utils.compute -   Writing predictions to: result/30_integrated_model_test/predictions_sampled_list_test.json\n","803it [00:02, 313.98it/s]\n","11/24/2024 18:19:37 - INFO - train_eval -   ***** Official Eval results *****\n","11/24/2024 18:19:37 - INFO - train_eval -   ****** 30_integrated_model_test/sampled_list_test ******\n","11/24/2024 18:19:37 - INFO - train_eval -     Count_HasAns_exact = 492\n","11/24/2024 18:19:37 - INFO - train_eval -     Count_HasAns_f1 = 294\n","11/24/2024 18:19:37 - INFO - train_eval -     Count_HasAns_none = 0\n","11/24/2024 18:19:37 - INFO - train_eval -     Count_HasAns_zero = 17\n","11/24/2024 18:19:37 - INFO - train_eval -     Count_NoAns_exact = 0\n","11/24/2024 18:19:37 - INFO - train_eval -     Count_NoAns_zero = 0\n","11/24/2024 18:19:37 - INFO - train_eval -     HasAns_exact = 61.27023661270237\n","11/24/2024 18:19:37 - INFO - train_eval -     HasAns_f1 = 86.78110397160494\n","11/24/2024 18:19:37 - INFO - train_eval -     HasAns_total = 803\n","11/24/2024 18:19:37 - INFO - train_eval -     exact = 61.27023661270237\n","11/24/2024 18:19:37 - INFO - train_eval -     f1 = 86.78110397160494\n","11/24/2024 18:19:37 - INFO - train_eval -     total = 803\n","\n","\n","\ttarget is - sampled_yesno_test.json \n","\n","11/24/2024 18:19:37 - INFO - utils.utils -   Creating features from dataset file at data sampled_yesno_test.json\n","100% 366/366 [00:00<00:00, 3469.05it/s]\n","convert squad examples to features: 100% 401/401 [00:00<00:00, 653.49it/s]\n","add example index and unique id: 100% 401/401 [00:00<00:00, 682872.88it/s]\n","11/24/2024 18:19:39 - INFO - utils.utils -   Saving features into cached file data/data_cache/cached_sampled_yesno_test_512_inte\n","11/24/2024 18:19:39 - INFO - train_eval -   ***** Running evaluation  *****\n","11/24/2024 18:19:39 - INFO - train_eval -     Num examples = 401\n","11/24/2024 18:19:39 - INFO - train_eval -     Batch size = 128\n","Evaluating: 100% 4/4 [00:03<00:00,  1.33it/s]\n","11/24/2024 18:19:42 - INFO - utils.compute -   Writing predictions to: result/30_integrated_model_test/predictions_sampled_yesno_test.json\n","401it [00:00, 322886.52it/s]\n","11/24/2024 18:19:42 - INFO - train_eval -   ****** 30_integrated_model_test/sampled_yesno_test ******\n","11/24/2024 18:19:42 - INFO - train_eval -   accuracy: 86.78303956985474\n","\n","\n","\ttarget is - shuffled_sampled_multi_choice_test.json \n","\n","11/24/2024 18:19:42 - INFO - utils.utils -   Creating features from dataset file at data shuffled_sampled_multi_choice_test.json\n","100% 374/374 [00:00<00:00, 895.12it/s]\n","convert squad examples to features: 100% 1616/1616 [00:02<00:00, 683.22it/s]\n","add example index and unique id: 100% 1616/1616 [00:00<00:00, 652923.15it/s]\n","11/24/2024 18:19:46 - INFO - utils.utils -   Saving features into cached file data/data_cache/cached_shuffled_sampled_multi_choice_test_512_inte\n","11/24/2024 18:19:55 - INFO - train_eval -   ***** Running evaluation  *****\n","11/24/2024 18:19:55 - INFO - train_eval -     Num examples = 1616\n","11/24/2024 18:19:55 - INFO - train_eval -     Batch size = 128\n","Evaluating: 100% 15/15 [00:11<00:00,  1.27it/s]\n","11/24/2024 18:20:07 - INFO - utils.compute -   Writing predictions to: result/30_integrated_model_test/predictions_shuffled_sampled_multi_choice_test.json\n","1616it [00:00, 284598.39it/s]\n","11/24/2024 18:20:07 - INFO - train_eval -   ****** 30_integrated_model_test/shuffled_sampled_multi_choice_test ******\n","11/24/2024 18:20:07 - INFO - train_eval -   accuracy: 84.65346693992615\n","\n","11/24/2024 18:20:07 - INFO - __main__ -   ========================= SCORES =========================\n","11/24/2024 18:20:07 - INFO - __main__ -   filename: sampled_answer_span_test.json\n","11/24/2024 18:20:07 - INFO - __main__ -   # of samples: 1203\n","11/24/2024 18:20:07 - INFO - __main__ -   F1 score: 87.76\n","11/24/2024 18:20:07 - INFO - __main__ -   ------------------------------------------------------------\n","11/24/2024 18:20:07 - INFO - __main__ -   filename: sampled_table_test.json\n","11/24/2024 18:20:07 - INFO - __main__ -   # of samples: 1200\n","11/24/2024 18:20:07 - INFO - __main__ -   F1 score: 0.05\n","11/24/2024 18:20:07 - INFO - __main__ -   ------------------------------------------------------------\n","11/24/2024 18:20:07 - INFO - __main__ -   filename: sampled_list_test.json\n","11/24/2024 18:20:07 - INFO - __main__ -   # of samples: 803\n","11/24/2024 18:20:07 - INFO - __main__ -   F1 score: 86.78\n","11/24/2024 18:20:07 - INFO - __main__ -   ------------------------------------------------------------\n","11/24/2024 18:20:07 - INFO - __main__ -   filename: sampled_yesno_test.json\n","11/24/2024 18:20:07 - INFO - __main__ -   # of samples: 401\n","11/24/2024 18:20:07 - INFO - __main__ -   Accuracy: 86.78\n","11/24/2024 18:20:07 - INFO - __main__ -   ------------------------------------------------------------\n","11/24/2024 18:20:07 - INFO - __main__ -   filename: shuffled_sampled_multi_choice_test.json\n","11/24/2024 18:20:07 - INFO - __main__ -   # of samples: 404\n","11/24/2024 18:20:07 - INFO - __main__ -   Accuracy: 84.65\n","11/24/2024 18:20:07 - INFO - __main__ -   ------------------------------------------------------------\n","\n","11/24/2024 18:20:07 - INFO - __main__ -   ========================\n","11/24/2024 18:20:07 - INFO - __main__ -   || final score: 60.91 ||\n","11/24/2024 18:20:07 - INFO - __main__ -   ========================\n"]}],"source":["#학습 수행\n","!python run_qa.py --config_file koelectra_base.json"]},{"cell_type":"code","source":["!pip uninstall transformers\n","!pip install transformers==4.20.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":672},"id":"tDNwYoFV5T7c","executionInfo":{"status":"ok","timestamp":1732497493892,"user_tz":-540,"elapsed":11279,"user":{"displayName":"Min","userId":"00121443274832754975"}},"outputId":"823e7e65-8534-4454-ecb0-a8efe30a1e45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: transformers 4.20.1\n","Uninstalling transformers-4.20.1:\n","  Would remove:\n","    /usr/local/bin/transformers-cli\n","    /usr/local/lib/python3.10/dist-packages/transformers-4.20.1.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/transformers/*\n","Proceed (Y/n)? y\n","  Successfully uninstalled transformers-4.20.1\n","Collecting transformers==4.20.1\n","  Using cached transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (3.7.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (0.26.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (1.23.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (2022.7.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (2.28.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (0.12.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (4.64.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (4.3.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging>=20.0->transformers==4.20.1) (3.0.9)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (2.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (3.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (1.26.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (2024.8.30)\n","Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","Installing collected packages: transformers\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.20.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed transformers-4.20.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["transformers"]},"id":"f2122a3cff6240ceb691d9e0f07379eb"}},"metadata":{}}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":498},"executionInfo":{"elapsed":27565,"status":"error","timestamp":1732543867095,"user":{"displayName":"Min","userId":"00121443274832754975"},"user_tz":-540},"id":"lWaN1X0SjiQI","outputId":"afef018e-181e-4191-9abe-bb6c2c2c8431"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185 were not used when initializing ElectraModel: ['embedding_multi_choice.token_type_embeddings.weight', 'embedding_answer_span.position_ids', 'classifier_yesno.out_proj.bias', 'embedding_answer_span.token_type_embeddings.weight', 'embedding_yesno.position_ids', 'classifier_yesno.dense.weight', 'classifier_multi_choice.out_proj.bias', 'embedding_multi_choice.LayerNorm.weight', 'embedding_multi_choice.word_embeddings.weight', 'embedding_yesno.word_embeddings.weight', 'embedding_yesno.token_type_embeddings.weight', 'embedding_multi_choice.position_ids', 'classifier_multi_choice.dense.bias', 'embedding_yesno.LayerNorm.bias', 'embedding_answer_span.LayerNorm.weight', 'embedding_yesno.position_embeddings.weight', 'embedding_multi_choice.position_embeddings.weight', 'embedding_answer_span.LayerNorm.bias', 'classifier_yesno.dense.bias', 'classifier_multi_choice.dense.weight', 'embedding_answer_span.word_embeddings.weight', 'classifier_multi_choice.out_proj.weight', 'classifier_yesno.out_proj.weight', 'qa_outputs.weight', 'embedding_yesno.LayerNorm.weight', 'qa_outputs.bias', 'embedding_answer_span.position_embeddings.weight', 'embedding_multi_choice.LayerNorm.bias']\n","- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraModel were not initialized from the model checkpoint at /content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'embeddings.LayerNorm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Token indices sequence length is longer than the specified maximum sequence length for this model (2204 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (528) must match the size of tensor b (512) at non-singleton dimension 1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-2390c312a63a>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# 문서에 대해 답변 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_answer_from_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# 결과 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-2390c312a63a>\u001b[0m in \u001b[0;36mget_answer_from_document\u001b[0;34m(question, document)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mstart_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mend_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         hidden_states = self.embeddings(\n\u001b[0m\u001b[1;32m    907\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (528) must match the size of tensor b (512) at non-singleton dimension 1"]}],"source":["# 필요한 라이브러리 import\n","#from transformers import ElectraTokenizer, ElectraForQuestionAnswering\n","from docx import Document\n","#from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","#from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n","from transformers import ElectraModel, ElectraTokenizer\n","\n","\n","# KoELECTRA 모델과 토크나이저 로드\n","# KoELECTRA 모델과 토크나이저 로드\n","model_dir = '/content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185'\n","\n","# 명시적으로 from_pretrained를 사용하여 토크나이저 로드\n","#tokenizer = ElectraTokenizer.from_pretrained(model_dir)\n","#model = ElectraForQuestionAnswering.from_pretrained(model_dir)\n","model = ElectraModel.from_pretrained(model_dir)\n","tokenizer = ElectraTokenizer.from_pretrained(model_dir)\n","# 학습된 모델과 토크나이저 로드\n","#tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","#model = AutoModelForQuestionAnswering.from_pretrained(model_dir)\n","# docx 파일 읽기\n","file_path = '/content/drive/MyDrive/training/30-finlaw-src/매매1-1_단독주택_간이.docx'\n","def read_docx(file_path):\n","    doc = Document(file_path)\n","    text = \"\"\n","    for para in doc.paragraphs:\n","        text += para.text + \"\\n\"\n","    return text\n","\n","# 문서 분할 함수 (512 토큰으로 나누기)\n","def split_document(doc, max_tokens=512):\n","    tokens = tokenizer(doc)['input_ids']\n","    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]\n","    return chunks\n","\n","# 문서와 질문을 받아서 답변을 추출하는 함수\n","def get_answer_from_document(question, document):\n","    # 문서 분할\n","    document_chunks = split_document(document, max_tokens=512)\n","    answers = []\n","\n","    # 각 문서 분할에 대해 질문-답변 수행\n","    for chunk in document_chunks:\n","        chunk_text = tokenizer.decode(chunk)  # 토큰을 텍스트로 복원\n","        inputs = tokenizer(question, chunk_text, return_tensors='pt')\n","\n","\n","        outputs = model(**inputs)\n","        start_scores = outputs.start_logits\n","        end_scores = outputs.end_logits\n","\n","        # 시작과 끝 토큰에서 가장 높은 점수의 인덱스를 찾음\n","        start_index = start_scores.argmax()\n","        end_index = end_scores.argmax()\n","\n","        # 답변 추출\n","        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index+1]))\n","        answers.append(answer)\n","\n","    # 여러 답을 합쳐서 반환\n","    return \" \".join(answers)\n","\n","# 예시 문서와 질문\n","document_path = '/content/drive/MyDrive/training/30-finlaw-src/매매1-1_단독주택_간이.docx'\n","document = read_docx(document_path)\n","question = \"이 법률문서에서 중요한 내용은 무엇인가?\"\n","\n","# 문서에 대해 답변 추출\n","answer = get_answer_from_document(question, document)\n","\n","# 결과 출력\n","print(\"Answer:\", answer)\n"]},{"cell_type":"code","source":["from transformers import ElectraModel, ElectraTokenizer\n","import torch\n","from docx import Document\n","import torch.nn as nn\n","\n","# KoELECTRA 모델과 토크나이저 로드\n","model_dir = '/content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185'\n","model = ElectraModel.from_pretrained(model_dir)  # ElectraModel 로드 (QA 레이어는 직접 추가해야 함)\n","tokenizer = ElectraTokenizer.from_pretrained(model_dir)\n","\n","# QA를 위한 커스텀 헤드 정의\n","class ElectraForQuestionAnswering(nn.Module):\n","    def __init__(self, electra_model):\n","        super(ElectraForQuestionAnswering, self).__init__()\n","        self.electra = electra_model  # KoELECTRA 모델\n","        self.start_classifier = nn.Linear(self.electra.config.hidden_size, 1)\n","        self.end_classifier = nn.Linear(self.electra.config.hidden_size, 1)\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","        # Electra 모델의 출력\n","        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        sequence_output = outputs.last_hidden_state  # (batch_size, sequence_length, hidden_size)\n","\n","        # 시작과 끝 위치에 대한 점수 계산\n","        start_scores = self.start_classifier(sequence_output).squeeze(-1)\n","        end_scores = self.end_classifier(sequence_output).squeeze(-1)\n","\n","        return start_scores, end_scores\n","\n","# 모델을 QA 모델로 초기화\n","qa_model = ElectraForQuestionAnswering(model)\n","\n","# 문서 읽기 함수\n","def read_docx(file_path):\n","    doc = Document(file_path)\n","    text = \"\"\n","    for para in doc.paragraphs:\n","        text += para.text + \"\\n\"\n","    return text\n","\n","# 문서 분할 함수 (512 토큰으로 나누기)\n","def split_document(doc, max_tokens=512):\n","    tokens = tokenizer(doc, truncation=True, padding=True, max_length=max_tokens)['input_ids']\n","    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n","    return chunks\n","\n","# 문서와 질문을 받아서 답변을 추출하는 함수\n","def get_answer_from_document(question, document):\n","    # 문서 분할\n","    document_chunks = split_document(document, max_tokens=512)\n","    answers = []\n","\n","    # 각 문서 분할에 대해 질문-답변 수행\n","    for chunk in document_chunks:\n","        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)  # 토큰을 텍스트로 복원\n","        inputs = tokenizer(question, chunk_text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n","\n","        # 모델을 사용하여 예측 (질문-답변 추론)\n","        with torch.no_grad():\n","            start_scores, end_scores = qa_model(**inputs)\n","\n","        # 시작과 끝 토큰에서 가장 높은 점수의 인덱스를 찾음\n","        start_index = torch.argmax(start_scores)\n","        end_index = torch.argmax(end_scores)\n","\n","        # 답변 추출\n","        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index+1]))\n","        answers.append(answer)\n","\n","    # 여러 답을 합쳐서 반환\n","    return \" \".join(answers)\n","\n","# 예시 문서와 질문\n","document_path = '/content/drive/MyDrive/training/30-finlaw-src/매매1-1_단독주택_간이.docx'\n","document = read_docx(document_path)\n","question = \"이 법률문서에서 중요한 내용은 무엇인가?\"\n","\n","# 문서에 대해 답변 추출\n","answer = get_answer_from_document(question, document)\n","\n","# 결과 출력\n","print(\"Answer:\", answer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"REdcEHmK5irt","executionInfo":{"status":"ok","timestamp":1732543985267,"user_tz":-540,"elapsed":3148,"user":{"displayName":"Min","userId":"00121443274832754975"}},"outputId":"223c2fa9-a605-42ea-838b-53de31e8de39"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185 were not used when initializing ElectraModel: ['embedding_multi_choice.token_type_embeddings.weight', 'embedding_answer_span.position_ids', 'classifier_yesno.out_proj.bias', 'embedding_answer_span.token_type_embeddings.weight', 'embedding_yesno.position_ids', 'classifier_yesno.dense.weight', 'classifier_multi_choice.out_proj.bias', 'embedding_multi_choice.LayerNorm.weight', 'embedding_multi_choice.word_embeddings.weight', 'embedding_yesno.word_embeddings.weight', 'embedding_yesno.token_type_embeddings.weight', 'embedding_multi_choice.position_ids', 'classifier_multi_choice.dense.bias', 'embedding_yesno.LayerNorm.bias', 'embedding_answer_span.LayerNorm.weight', 'embedding_yesno.position_embeddings.weight', 'embedding_multi_choice.position_embeddings.weight', 'embedding_answer_span.LayerNorm.bias', 'classifier_yesno.dense.bias', 'classifier_multi_choice.dense.weight', 'embedding_answer_span.word_embeddings.weight', 'classifier_multi_choice.out_proj.weight', 'classifier_yesno.out_proj.weight', 'qa_outputs.weight', 'embedding_yesno.LayerNorm.weight', 'qa_outputs.bias', 'embedding_answer_span.position_embeddings.weight', 'embedding_multi_choice.LayerNorm.bias']\n","- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraModel were not initialized from the model checkpoint at /content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'embeddings.LayerNorm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Answer: \n"]}]},{"cell_type":"code","source":["from transformers import ElectraForQuestionAnswering, ElectraTokenizer\n","import torch\n","from docx import Document\n","\n","# KoELECTRA 모델과 토크나이저 로드 (QA 모델로 로드)\n","model_dir = '/content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185'\n","tokenizer = ElectraTokenizer.from_pretrained(model_dir)\n","model = ElectraForQuestionAnswering.from_pretrained(model_dir)\n","\n","\n","# 문서 읽기 함수\n","def read_docx(file_path):\n","    doc = Document(file_path)\n","    text = \"\"\n","    for para in doc.paragraphs:\n","        text += para.text + \"\\n\"\n","    return text\n","\n","# 문서 분할 함수 (512 토큰으로 나누기)\n","def split_document(doc, max_tokens=512):\n","    tokens = tokenizer(doc, return_tensors='pt', truncation=True)['input_ids'][0]\n","    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n","    return chunks\n","\n","# 문서와 질문을 받아서 답변을 추출하는 함수\n","def get_answer_from_document(question, document):\n","    # 문서 분할\n","    document_chunks = split_document(document, max_tokens=512)\n","    answers = []\n","\n","    # 각 문서 분할에 대해 질문-답변 수행\n","    for chunk in document_chunks:\n","        inputs = tokenizer(question, chunk, return_tensors='pt', truncation=True, padding=True, max_length=512)\n","\n","        # 모델을 사용하여 예측 (질문-답변 추론)\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            start_scores = outputs.start_logits\n","            end_scores = outputs.end_logits\n","\n","        # 시작과 끝 토큰에서 가장 높은 점수의 인덱스를 찾음\n","        start_index = torch.argmax(start_scores)\n","        end_index = torch.argmax(end_scores)\n","\n","        # 답변 추출\n","        answer = tokenizer.decode(inputs['input_ids'][0][start_index:end_index + 1])\n","        answers.append(answer)\n","\n","    # 여러 답을 합쳐서 반환\n","    return \" \".join(answers)\n","\n","# 예시 문서와 질문\n","document_path = '/content/drive/MyDrive/training/30-finlaw-src/매매1-1_단독주택_간이.docx'\n","document = read_docx(document_path)\n","question = \"이 법률문서에서 중요한 내용은 무엇인가?\"\n","\n","# 문서에 대해 답변 추출\n","answer = get_answer_from_document(question, document)\n","\n","# 결과 출력\n","print(\"Answer:\", answer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"zinaQbJf1hNg","executionInfo":{"status":"error","timestamp":1732544260337,"user_tz":-540,"elapsed":2938,"user":{"displayName":"Min","userId":"00121443274832754975"}},"outputId":"453109ac-aa2d-4346-88eb-3a808f15c462"},"execution_count":12,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-561e7624668a>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElectraTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElectraForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2223\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2225\u001b[0;31m             model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(\n\u001b[0m\u001b[1;32m   2226\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2227\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype)\u001b[0m\n\u001b[1;32m   2354\u001b[0m             \u001b[0mmodel_to_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpected_keys_not_prefixed\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   2357\u001b[0m                     \u001b[0;34m\"The state dictionary of the model you are trying to load is corrupted. Are you sure it was \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                     \u001b[0;34m\"properly saved?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?"]}]},{"cell_type":"code","source":["from transformers import ElectraForQuestionAnswering, ElectraTokenizer\n","from docx import Document\n","\n","# 모델 경로\n","model_dir = '/content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185'\n","\n","# KoELECTRA 모델과 토크나이저 로드 (QA 모델로 변경)\n","model = ElectraForQuestionAnswering.from_pretrained(model_dir)\n","tokenizer = ElectraTokenizer.from_pretrained(model_dir)\n","\n","\n","\n","# docx 파일 읽기\n","def read_docx(file_path):\n","    doc = Document(file_path)\n","    text = \"\"\n","    for para in doc.paragraphs:\n","        text += para.text + \"\\n\"\n","    return text\n","\n","# 문서 분할 함수 (512 토큰으로 나누기)\n","def split_document(doc, max_tokens=512):\n","    tokens = tokenizer(doc)['input_ids']\n","    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]\n","    return chunks\n","\n","# 문서와 질문을 받아서 답변을 추출하는 함수\n","def get_answer_from_document(question, document):\n","    # 문서 분할\n","    document_chunks = split_document(document, max_tokens=512)\n","    answers = []\n","\n","    # 각 문서 분할에 대해 질문-답변 수행\n","    for chunk in document_chunks:\n","        chunk_text = tokenizer.decode(chunk)  # 토큰을 텍스트로 복원\n","        inputs = tokenizer(question, chunk_text, return_tensors='pt')\n","\n","        # 모델 추론\n","        outputs = model(**inputs)\n","        start_scores = outputs.start_logits\n","        end_scores = outputs.end_logits\n","\n","        # 시작과 끝 토큰에서 가장 높은 점수의 인덱스를 찾음\n","        start_index = start_scores.argmax()\n","        end_index = end_scores.argmax()\n","\n","        # 답변 추출\n","        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index+1]))\n","        answers.append(answer)\n","\n","    # 여러 답을 합쳐서 반환\n","    return \" \".join(answers)\n","\n","# 예시 문서와 질문\n","document_path = '/content/drive/MyDrive/training/30-finlaw-src/매매1-1_단독주택_간이.docx'\n","document = read_docx(document_path)\n","question = \"이 법률문서에서 중요한 내용은 무엇인가?\"\n","\n","# 문서에 대해 답변 추출\n","answer = get_answer_from_document(question, document)\n","\n","# 결과 출력\n","print(\"Answer:\", answer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"ZE2yeeNI2R7z","executionInfo":{"status":"error","timestamp":1732543943106,"user_tz":-540,"elapsed":2065,"user":{"displayName":"Min","userId":"00121443274832754975"}},"outputId":"9e889c44-c3bd-4808-d104-42c07435dc2f"},"execution_count":6,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-9af71ddb70a5>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# KoELECTRA 모델과 토크나이저 로드 (QA 모델로 변경)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElectraForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElectraTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2223\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2225\u001b[0;31m             model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(\n\u001b[0m\u001b[1;32m   2226\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2227\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype)\u001b[0m\n\u001b[1;32m   2354\u001b[0m             \u001b[0mmodel_to_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpected_keys_not_prefixed\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   2357\u001b[0m                     \u001b[0;34m\"The state dictionary of the model you are trying to load is corrupted. Are you sure it was \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                     \u001b[0;34m\"properly saved?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import ElectraModel, ElectraTokenizer\n","\n","# 다중 태스크를 위한 Electra 모델\n","class ElectraForMultiTaskQuestionAnswering(nn.Module):\n","    def __init__(self, electra_model):\n","        super(ElectraForMultiTaskQuestionAnswering, self).__init__()\n","        self.electra = electra_model  # KoELECTRA 모델\n","\n","        # Answer Span Task\n","        self.start_classifier = nn.Linear(self.electra.config.hidden_size, 1)\n","        self.end_classifier = nn.Linear(self.electra.config.hidden_size, 1)\n","\n","        # Yes/No Task\n","        self.yesno_classifier = nn.Linear(self.electra.config.hidden_size, 2)\n","\n","        # Multi-choice Task\n","        self.multichoice_classifier = nn.Linear(self.electra.config.hidden_size, 4)  # 4개의 선택지라 가정\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None, task_type=\"answer_span\"):\n","        # Electra 모델의 출력\n","        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        sequence_output = outputs.last_hidden_state  # (batch_size, sequence_length, hidden_size)\n","\n","        if task_type == \"answer_span\":\n","            # Answer Span Task 처리\n","            start_scores = self.start_classifier(sequence_output).squeeze(-1)\n","            end_scores = self.end_classifier(sequence_output).squeeze(-1)\n","            return start_scores, end_scores\n","\n","        elif task_type == \"yesno\":\n","            # Yes/No Task 처리\n","            yesno_scores = self.yesno_classifier(sequence_output[:, 0, :])  # [CLS] 토큰에 대한 출력 사용\n","            return yesno_scores\n","\n","        elif task_type == \"multichoice\":\n","            # Multi-choice Task 처리\n","            multichoice_scores = self.multichoice_classifier(sequence_output[:, 0, :])  # [CLS] 토큰에 대한 출력 사용\n","            return multichoice_scores\n","\n","# 모델 및 토크나이저 로드\n","model_dir = '/content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185'\n","electra_model = ElectraModel.from_pretrained(model_dir)\n","tokenizer = ElectraTokenizer.from_pretrained(model_dir)\n","\n","# 다중 태스크 모델 초기화\n","multi_task_model = ElectraForMultiTaskQuestionAnswering(electra_model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIEampSjupZ8","executionInfo":{"status":"ok","timestamp":1732544999225,"user_tz":-540,"elapsed":3508,"user":{"displayName":"Min","userId":"00121443274832754975"}},"outputId":"30674809-f19f-46f7-8bbe-166a24f062e6"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185 were not used when initializing ElectraModel: ['embedding_multi_choice.token_type_embeddings.weight', 'embedding_answer_span.position_ids', 'classifier_yesno.out_proj.bias', 'embedding_answer_span.token_type_embeddings.weight', 'embedding_yesno.position_ids', 'classifier_yesno.dense.weight', 'classifier_multi_choice.out_proj.bias', 'embedding_multi_choice.LayerNorm.weight', 'embedding_multi_choice.word_embeddings.weight', 'embedding_yesno.word_embeddings.weight', 'embedding_yesno.token_type_embeddings.weight', 'embedding_multi_choice.position_ids', 'classifier_multi_choice.dense.bias', 'embedding_yesno.LayerNorm.bias', 'embedding_answer_span.LayerNorm.weight', 'embedding_yesno.position_embeddings.weight', 'embedding_multi_choice.position_embeddings.weight', 'embedding_answer_span.LayerNorm.bias', 'classifier_yesno.dense.bias', 'classifier_multi_choice.dense.weight', 'embedding_answer_span.word_embeddings.weight', 'classifier_multi_choice.out_proj.weight', 'classifier_yesno.out_proj.weight', 'qa_outputs.weight', 'embedding_yesno.LayerNorm.weight', 'qa_outputs.bias', 'embedding_answer_span.position_embeddings.weight', 'embedding_multi_choice.LayerNorm.bias']\n","- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraModel were not initialized from the model checkpoint at /content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'embeddings.LayerNorm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["import torch\n","\n","# 문서와 질문을 받아서 다중 태스크 기반으로 답변을 추출하는 함수\n","def get_answer_from_document(question, document, task_type=\"answer_span\"):\n","    # 문서 토큰화\n","    inputs = tokenizer(question, document, return_tensors='pt', truncation=True, padding=True, max_length=512)\n","\n","    # 모델을 사용하여 예측 (질문-답변 추론)\n","    with torch.no_grad():\n","        if task_type == \"answer_span\":\n","            start_scores, end_scores = multi_task_model(input_ids=inputs['input_ids'],\n","                                                        attention_mask=inputs['attention_mask'],\n","                                                        token_type_ids=inputs['token_type_ids'],\n","                                                        task_type=\"answer_span\")\n","\n","            # 시작과 끝 토큰에서 가장 높은 점수의 인덱스를 찾음\n","            start_index = torch.argmax(start_scores)\n","            end_index = torch.argmax(end_scores)\n","\n","            # 답변 추출\n","            answer = tokenizer.decode(inputs['input_ids'][0][start_index:end_index + 1])\n","\n","        elif task_type == \"yesno\":\n","            yesno_scores = multi_task_model(input_ids=inputs['input_ids'],\n","                                            attention_mask=inputs['attention_mask'],\n","                                            token_type_ids=inputs['token_type_ids'],\n","                                            task_type=\"yesno\")\n","            answer = \"Yes\" if torch.argmax(yesno_scores) == 0 else \"No\"\n","\n","        elif task_type == \"multichoice\":\n","            multichoice_scores = multi_task_model(input_ids=inputs['input_ids'],\n","                                                  attention_mask=inputs['attention_mask'],\n","                                                  token_type_ids=inputs['token_type_ids'],\n","                                                  task_type=\"multichoice\")\n","            answer = f\"Option {torch.argmax(multichoice_scores) + 1}\"\n","\n","    return answer\n"],"metadata":{"id":"aNfd3a1Vurl2","executionInfo":{"status":"ok","timestamp":1732545014670,"user_tz":-540,"elapsed":307,"user":{"displayName":"Min","userId":"00121443274832754975"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# 예시 문서와 질문\n","document = \"이 문서는 계약 조건에 대한 상세한 설명을 포함하고 있습니다.\"\n","question = \"이 문서에서 중요한 내용은 무엇입니까?\"\n","\n","# Answer Span 태스크 실행\n","answer = get_answer_from_document(question, document, task_type=\"answer_span\")\n","print(\"Answer Span Task:\", answer)\n","\n","# Yes/No 태스크 실행\n","answer = get_answer_from_document(question, document, task_type=\"yesno\")\n","print(\"Yes/No Task:\", answer)\n","\n","# Multi-choice 태스크 실행\n","answer = get_answer_from_document(question, document, task_type=\"multichoice\")\n","print(\"Multi-choice Task:\", answer)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"liDtTkMZuzsk","executionInfo":{"status":"ok","timestamp":1732545038936,"user_tz":-540,"elapsed":638,"user":{"displayName":"Min","userId":"00121443274832754975"}},"outputId":"72bc8289-d5a4-4e15-b172-43e775e3589e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer Span Task: \n","Yes/No Task: Yes\n","Multi-choice Task: Option 3\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import ElectraModel, ElectraTokenizer\n","\n","# 다중 태스크를 위한 Electra 모델\n","class ElectraForMultiTaskQuestionAnswering(nn.Module):\n","    def __init__(self, electra_model):\n","        super(ElectraForMultiTaskQuestionAnswering, self).__init__()\n","        self.electra = electra_model  # KoELECTRA 모델\n","\n","        # Answer Span Task\n","        self.start_classifier = nn.Linear(self.electra.config.hidden_size, 1)\n","        self.end_classifier = nn.Linear(self.electra.config.hidden_size, 1)\n","\n","        # Yes/No Task\n","        self.yesno_classifier = nn.Linear(self.electra.config.hidden_size, 2)\n","\n","        # Multi-choice Task\n","        self.multichoice_classifier = nn.Linear(self.electra.config.hidden_size, 4)  # 4개의 선택지라 가정\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None, task_type=\"answer_span\"):\n","        # Electra 모델의 출력\n","        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        sequence_output = outputs.last_hidden_state  # (batch_size, sequence_length, hidden_size)\n","\n","        if task_type == \"answer_span\":\n","            # Answer Span Task 처리\n","            start_scores = self.start_classifier(sequence_output).squeeze(-1)\n","            end_scores = self.end_classifier(sequence_output).squeeze(-1)\n","            return start_scores, end_scores\n","\n","        elif task_type == \"yesno\":\n","            # Yes/No Task 처리\n","            yesno_scores = self.yesno_classifier(sequence_output[:, 0, :])  # [CLS] 토큰에 대한 출력 사용\n","            return yesno_scores\n","\n","        elif task_type == \"multichoice\":\n","            # Multi-choice Task 처리\n","            multichoice_scores = self.multichoice_classifier(sequence_output[:, 0, :])  # [CLS] 토큰에 대한 출력 사용\n","            return multichoice_scores\n","\n","# 모델 및 토크나이저 로드\n","model_dir = '/content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185'\n","electra_model = ElectraModel.from_pretrained(model_dir)\n","tokenizer = ElectraTokenizer.from_pretrained(model_dir)\n","\n","# 다중 태스크 모델 초기화\n","multi_task_model = ElectraForMultiTaskQuestionAnswering(electra_model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPcSCOovvm4Q","executionInfo":{"status":"ok","timestamp":1732545249244,"user_tz":-540,"elapsed":2198,"user":{"displayName":"Min","userId":"00121443274832754975"}},"outputId":"6db55b4f-d7ed-4e24-a37d-0e4ffafc57e6"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185 were not used when initializing ElectraModel: ['embedding_multi_choice.token_type_embeddings.weight', 'embedding_answer_span.position_ids', 'classifier_yesno.out_proj.bias', 'embedding_answer_span.token_type_embeddings.weight', 'embedding_yesno.position_ids', 'classifier_yesno.dense.weight', 'classifier_multi_choice.out_proj.bias', 'embedding_multi_choice.LayerNorm.weight', 'embedding_multi_choice.word_embeddings.weight', 'embedding_yesno.word_embeddings.weight', 'embedding_yesno.token_type_embeddings.weight', 'embedding_multi_choice.position_ids', 'classifier_multi_choice.dense.bias', 'embedding_yesno.LayerNorm.bias', 'embedding_answer_span.LayerNorm.weight', 'embedding_yesno.position_embeddings.weight', 'embedding_multi_choice.position_embeddings.weight', 'embedding_answer_span.LayerNorm.bias', 'classifier_yesno.dense.bias', 'classifier_multi_choice.dense.weight', 'embedding_answer_span.word_embeddings.weight', 'classifier_multi_choice.out_proj.weight', 'classifier_yesno.out_proj.weight', 'qa_outputs.weight', 'embedding_yesno.LayerNorm.weight', 'qa_outputs.bias', 'embedding_answer_span.position_embeddings.weight', 'embedding_multi_choice.LayerNorm.bias']\n","- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraModel were not initialized from the model checkpoint at /content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'embeddings.LayerNorm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["import torch\n","\n","# 문서와 질문을 받아서 다중 태스크 기반으로 답변을 추출하는 함수\n","def get_answer_from_document(question, document, task_type=\"answer_span\"):\n","    # 문서 토큰화\n","    inputs = tokenizer(question, document, return_tensors='pt', truncation=True, padding=True, max_length=512)\n","\n","    # 모델을 사용하여 예측 (질문-답변 추론)\n","    with torch.no_grad():\n","        if task_type == \"answer_span\":\n","            # Answer Span Task 처리\n","            start_scores, end_scores = multi_task_model(input_ids=inputs['input_ids'],\n","                                                        attention_mask=inputs['attention_mask'],\n","                                                        token_type_ids=inputs['token_type_ids'],\n","                                                        task_type=\"answer_span\")\n","\n","            # 시작과 끝 토큰에서 가장 높은 점수의 인덱스를 찾음\n","            start_index = torch.argmax(start_scores)\n","            end_index = torch.argmax(end_scores)\n","\n","            # 유효한 범위 확인 (시작 인덱스가 끝 인덱스보다 작아야 함)\n","            if start_index <= end_index:\n","                answer = tokenizer.decode(inputs['input_ids'][0][start_index:end_index + 1], skip_special_tokens=True)\n","            else:\n","                answer = \"답을 찾을 수 없습니다.\"\n","\n","        elif task_type == \"yesno\":\n","            # Yes/No Task 처리\n","            yesno_scores = multi_task_model(input_ids=inputs['input_ids'],\n","                                            attention_mask=inputs['attention_mask'],\n","                                            token_type_ids=inputs['token_type_ids'],\n","                                            task_type=\"yesno\")\n","            answer = \"Yes\" if torch.argmax(yesno_scores) == 0 else \"No\"\n","\n","        elif task_type == \"multichoice\":\n","            # Multi-choice Task 처리\n","            multichoice_scores = multi_task_model(input_ids=inputs['input_ids'],\n","                                                  attention_mask=inputs['attention_mask'],\n","                                                  token_type_ids=inputs['token_type_ids'],\n","                                                  task_type=\"multichoice\")\n","            answer = f\"Option {torch.argmax(multichoice_scores) + 1}\"\n","\n","    return answer\n"],"metadata":{"id":"jmbgKwUlvo7a","executionInfo":{"status":"ok","timestamp":1732545326963,"user_tz":-540,"elapsed":313,"user":{"displayName":"Min","userId":"00121443274832754975"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# 예시 문서와 질문\n","document = \"이 문서는 계약 조건에 대한 상세한 설명을 포함하고 있습니다.\"\n","question = \"이 문서에서 중요한 내용은 무엇입니까?\"\n","\n","# Answer Span 태스크 실행\n","answer = get_answer_from_document(question, document, task_type=\"answer_span\")\n","print(\"Answer Span Task:\", answer)\n","\n","# Yes/No 태스크 실행\n","answer = get_answer_from_document(question, document, task_type=\"yesno\")\n","print(\"Yes/No Task:\", answer)\n","\n","# Multi-choice 태스크 실행\n","answer = get_answer_from_document(question, document, task_type=\"multichoice\")\n","print(\"Multi-choice Task:\", answer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vNgZI_jBvsk8","executionInfo":{"status":"ok","timestamp":1732545331011,"user_tz":-540,"elapsed":686,"user":{"displayName":"Min","userId":"00121443274832754975"}},"outputId":"46d3b5b0-f3a2-4b23-9748-e9f1f1358bf5"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer Span Task: 답을 찾을 수 없습니다.\n","Yes/No Task: No\n","Multi-choice Task: Option 2\n"]}]},{"cell_type":"code","source":["from docx import Document\n","\n","# docx 파일을 읽는 함수\n","def read_docx(file_path):\n","    doc = Document(file_path)\n","    text = \"\"\n","    for para in doc.paragraphs:\n","        text += para.text + \"\\n\"\n","    return text\n","\n","# 예시 docx 문서 경로\n","document_path = \"/content/drive/MyDrive/training/30-finlaw-src/매매1-1_단독주택_간이.docx\"\n","\n","# docx 파일을 읽어서 document 변수에 할당\n","document = read_docx(document_path)\n","\n","# 질문 정의\n","question = \"이 문서에서 중요한 내용은 무엇입니까?\"\n","\n","# Answer Span 태스크 실행\n","answer = get_answer_from_document(question, document, task_type=\"answer_span\")\n","print(\"Answer Span Task:\", answer)\n","\n","# Yes/No 태스크 실행\n","answer = get_answer_from_document(question, document, task_type=\"yesno\")\n","print(\"Yes/No Task:\", answer)\n","\n","# Multi-choice 태스크 실행\n","answer = get_answer_from_document(question, document, task_type=\"multichoice\")\n","print(\"Multi-choice Task:\", answer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yD4rp-YjwFHx","executionInfo":{"status":"ok","timestamp":1732545404932,"user_tz":-540,"elapsed":4147,"user":{"displayName":"Min","userId":"00121443274832754975"}},"outputId":"d1fb2a27-81c9-44ca-d975-6996a28659ba"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Answer Span Task: 답을 찾을 수 없습니다.\n"]},{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Yes/No Task: No\n","Multi-choice Task: Option 2\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import ElectraModel, ElectraTokenizer\n","\n","class ElectraForMultiTask(nn.Module):\n","    def __init__(self, electra_model):\n","        super(ElectraForMultiTask, self).__init__()\n","        self.electra = electra_model  # KoELECTRA 모델\n","\n","        # Answer Span Task\n","        self.start_classifier = nn.Linear(self.electra.config.hidden_size, 1)\n","        self.end_classifier = nn.Linear(self.electra.config.hidden_size, 1)\n","\n","        # Yes/No Task\n","        self.yesno_classifier = nn.Linear(self.electra.config.hidden_size, 2)\n","\n","        # Multi-choice Task\n","        self.multichoice_classifier = nn.Linear(self.electra.config.hidden_size, 4)  # 예시: 4개의 선택지\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None, task_type=\"answer_span\"):\n","        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        sequence_output = outputs.last_hidden_state  # (batch_size, sequence_length, hidden_size)\n","\n","        if task_type == \"answer_span\":\n","            start_scores = self.start_classifier(sequence_output).squeeze(-1)\n","            end_scores = self.end_classifier(sequence_output).squeeze(-1)\n","            return start_scores, end_scores\n","\n","        elif task_type == \"yesno\":\n","            yesno_scores = self.yesno_classifier(sequence_output[:, 0, :])  # [CLS] 토큰에 대한 출력 사용\n","            return yesno_scores\n","\n","        elif task_type == \"multichoice\":\n","            multichoice_scores = self.multichoice_classifier(sequence_output[:, 0, :])  # [CLS] 토큰에 대한 출력 사용\n","            return multichoice_scores\n","\n","# 모델 및 토크나이저 로드\n","model_dir = '/content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185'\n","electra_model = ElectraModel.from_pretrained(model_dir)\n","tokenizer = ElectraTokenizer.from_pretrained(model_dir)\n","\n","# 다중 태스크 모델 초기화\n","multi_task_model = ElectraForMultiTask(electra_model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GRlc-fWwxgbM","executionInfo":{"status":"ok","timestamp":1732545751958,"user_tz":-540,"elapsed":2404,"user":{"displayName":"Min","userId":"00121443274832754975"}},"outputId":"3618c7c6-8158-4a7c-94d0-922da9dfe2db"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185 were not used when initializing ElectraModel: ['embedding_multi_choice.token_type_embeddings.weight', 'embedding_answer_span.position_ids', 'classifier_yesno.out_proj.bias', 'embedding_answer_span.token_type_embeddings.weight', 'embedding_yesno.position_ids', 'classifier_yesno.dense.weight', 'classifier_multi_choice.out_proj.bias', 'embedding_multi_choice.LayerNorm.weight', 'embedding_multi_choice.word_embeddings.weight', 'embedding_yesno.word_embeddings.weight', 'embedding_yesno.token_type_embeddings.weight', 'embedding_multi_choice.position_ids', 'classifier_multi_choice.dense.bias', 'embedding_yesno.LayerNorm.bias', 'embedding_answer_span.LayerNorm.weight', 'embedding_yesno.position_embeddings.weight', 'embedding_multi_choice.position_embeddings.weight', 'embedding_answer_span.LayerNorm.bias', 'classifier_yesno.dense.bias', 'classifier_multi_choice.dense.weight', 'embedding_answer_span.word_embeddings.weight', 'classifier_multi_choice.out_proj.weight', 'classifier_yesno.out_proj.weight', 'qa_outputs.weight', 'embedding_yesno.LayerNorm.weight', 'qa_outputs.bias', 'embedding_answer_span.position_embeddings.weight', 'embedding_multi_choice.LayerNorm.bias']\n","- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraModel were not initialized from the model checkpoint at /content/drive/MyDrive/training/30-finlaw-src/ckpt/30_integrated_model_test/checkpoint-4185 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'embeddings.LayerNorm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# 이미 생성된 모델에서 가중치를 가져오기\n","state_dict = model.state_dict()\n","\n","# 가중치 이름과 해당 값 출력\n","for name, param in state_dict.items():\n","    print(f\"Layer: {name}, Shape: {param.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eBCZw7j7yJLD","executionInfo":{"status":"ok","timestamp":1732546090274,"user_tz":-540,"elapsed":283,"user":{"displayName":"Min","userId":"00121443274832754975"}},"outputId":"da099e19-870b-49ab-d4e7-b2276a9beff9"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer: embeddings.position_ids, Shape: torch.Size([1, 512])\n","Layer: embeddings.word_embeddings.weight, Shape: torch.Size([35000, 768])\n","Layer: embeddings.position_embeddings.weight, Shape: torch.Size([512, 768])\n","Layer: embeddings.token_type_embeddings.weight, Shape: torch.Size([2, 768])\n","Layer: embeddings.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: embeddings.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.0.attention.self.query.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.0.attention.self.query.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.0.attention.self.key.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.0.attention.self.key.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.0.attention.self.value.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.0.attention.self.value.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.0.attention.output.dense.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.0.attention.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.0.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.0.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.0.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n","Layer: encoder.layer.0.intermediate.dense.bias, Shape: torch.Size([3072])\n","Layer: encoder.layer.0.output.dense.weight, Shape: torch.Size([768, 3072])\n","Layer: encoder.layer.0.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.0.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.0.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.1.attention.self.query.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.1.attention.self.query.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.1.attention.self.key.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.1.attention.self.key.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.1.attention.self.value.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.1.attention.self.value.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.1.attention.output.dense.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.1.attention.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.1.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.1.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.1.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n","Layer: encoder.layer.1.intermediate.dense.bias, Shape: torch.Size([3072])\n","Layer: encoder.layer.1.output.dense.weight, Shape: torch.Size([768, 3072])\n","Layer: encoder.layer.1.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.1.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.1.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.2.attention.self.query.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.2.attention.self.query.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.2.attention.self.key.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.2.attention.self.key.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.2.attention.self.value.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.2.attention.self.value.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.2.attention.output.dense.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.2.attention.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.2.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.2.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.2.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n","Layer: encoder.layer.2.intermediate.dense.bias, Shape: torch.Size([3072])\n","Layer: encoder.layer.2.output.dense.weight, Shape: torch.Size([768, 3072])\n","Layer: encoder.layer.2.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.2.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.2.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.3.attention.self.query.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.3.attention.self.query.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.3.attention.self.key.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.3.attention.self.key.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.3.attention.self.value.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.3.attention.self.value.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.3.attention.output.dense.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.3.attention.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.3.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.3.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.3.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n","Layer: encoder.layer.3.intermediate.dense.bias, Shape: torch.Size([3072])\n","Layer: encoder.layer.3.output.dense.weight, Shape: torch.Size([768, 3072])\n","Layer: encoder.layer.3.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.3.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.3.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.4.attention.self.query.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.4.attention.self.query.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.4.attention.self.key.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.4.attention.self.key.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.4.attention.self.value.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.4.attention.self.value.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.4.attention.output.dense.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.4.attention.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.4.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.4.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.4.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n","Layer: encoder.layer.4.intermediate.dense.bias, Shape: torch.Size([3072])\n","Layer: encoder.layer.4.output.dense.weight, Shape: torch.Size([768, 3072])\n","Layer: encoder.layer.4.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.4.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.4.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.5.attention.self.query.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.5.attention.self.query.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.5.attention.self.key.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.5.attention.self.key.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.5.attention.self.value.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.5.attention.self.value.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.5.attention.output.dense.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.5.attention.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.5.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.5.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.5.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n","Layer: encoder.layer.5.intermediate.dense.bias, Shape: torch.Size([3072])\n","Layer: encoder.layer.5.output.dense.weight, Shape: torch.Size([768, 3072])\n","Layer: encoder.layer.5.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.5.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.5.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.6.attention.self.query.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.6.attention.self.query.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.6.attention.self.key.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.6.attention.self.key.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.6.attention.self.value.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.6.attention.self.value.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.6.attention.output.dense.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.6.attention.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.6.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.6.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.6.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n","Layer: encoder.layer.6.intermediate.dense.bias, Shape: torch.Size([3072])\n","Layer: encoder.layer.6.output.dense.weight, Shape: torch.Size([768, 3072])\n","Layer: encoder.layer.6.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.6.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.6.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.7.attention.self.query.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.7.attention.self.query.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.7.attention.self.key.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.7.attention.self.key.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.7.attention.self.value.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.7.attention.self.value.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.7.attention.output.dense.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.7.attention.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.7.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.7.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.7.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n","Layer: encoder.layer.7.intermediate.dense.bias, Shape: torch.Size([3072])\n","Layer: encoder.layer.7.output.dense.weight, Shape: torch.Size([768, 3072])\n","Layer: encoder.layer.7.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.7.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.7.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.8.attention.self.query.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.8.attention.self.query.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.8.attention.self.key.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.8.attention.self.key.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.8.attention.self.value.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.8.attention.self.value.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.8.attention.output.dense.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.8.attention.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.8.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.8.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.8.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n","Layer: encoder.layer.8.intermediate.dense.bias, Shape: torch.Size([3072])\n","Layer: encoder.layer.8.output.dense.weight, Shape: torch.Size([768, 3072])\n","Layer: encoder.layer.8.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.8.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.8.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.9.attention.self.query.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.9.attention.self.query.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.9.attention.self.key.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.9.attention.self.key.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.9.attention.self.value.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.9.attention.self.value.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.9.attention.output.dense.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.9.attention.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.9.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.9.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.9.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n","Layer: encoder.layer.9.intermediate.dense.bias, Shape: torch.Size([3072])\n","Layer: encoder.layer.9.output.dense.weight, Shape: torch.Size([768, 3072])\n","Layer: encoder.layer.9.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.9.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.9.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.10.attention.self.query.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.10.attention.self.query.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.10.attention.self.key.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.10.attention.self.key.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.10.attention.self.value.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.10.attention.self.value.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.10.attention.output.dense.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.10.attention.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.10.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.10.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.10.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n","Layer: encoder.layer.10.intermediate.dense.bias, Shape: torch.Size([3072])\n","Layer: encoder.layer.10.output.dense.weight, Shape: torch.Size([768, 3072])\n","Layer: encoder.layer.10.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.10.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.10.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.11.attention.self.query.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.11.attention.self.query.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.11.attention.self.key.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.11.attention.self.key.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.11.attention.self.value.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.11.attention.self.value.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.11.attention.output.dense.weight, Shape: torch.Size([768, 768])\n","Layer: encoder.layer.11.attention.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.11.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.11.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.11.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n","Layer: encoder.layer.11.intermediate.dense.bias, Shape: torch.Size([3072])\n","Layer: encoder.layer.11.output.dense.weight, Shape: torch.Size([768, 3072])\n","Layer: encoder.layer.11.output.dense.bias, Shape: torch.Size([768])\n","Layer: encoder.layer.11.output.LayerNorm.weight, Shape: torch.Size([768])\n","Layer: encoder.layer.11.output.LayerNorm.bias, Shape: torch.Size([768])\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/training/30-finlaw-src/model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NdqeR-6Wy4px","executionInfo":{"status":"ok","timestamp":1732546137546,"user_tz":-540,"elapsed":294,"user":{"displayName":"Min","userId":"00121443274832754975"}},"outputId":"aec02662-e1b9-4e3b-e9cd-6d0c79e3d6a7"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/training/30-finlaw-src/model\n"]}]},{"cell_type":"code","source":["!python electra_integrated_qa_model.py"],"metadata":{"id":"pd-coQDQzEQm","executionInfo":{"status":"ok","timestamp":1732546176501,"user_tz":-540,"elapsed":6819,"user":{"displayName":"Min","userId":"00121443274832754975"}}},"execution_count":32,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1zM0N2VWc5OquFWlZ7EOrTLYXy_hY8MOg","authorship_tag":"ABX9TyN+sS6MYPOURsZjGBLyrYuB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}